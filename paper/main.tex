\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{svg}
\usepackage{caption, booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{todonotes}
\usepackage{units}
\usepackage[
	colorlinks=true,
	urlcolor=blue,
	linkcolor=green
]{hyperref}
\newcommand{\scidatalogo}{\includegraphics[height=36pt]{SciData_logo.jpg}}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{40pt}
\lhead{\textsc{\scidatalogo}}
\usepackage{natbib}

\begin{document}
\input{descr-stats-ao-events.tex}
\input{descr-stats-av-events.tex}
\input{descr-stats-anno.tex}

% Titel-Vorgabe < 110 chars incl. spaces
%  jetzt 109 Chars
\title{Hemodynamic responses of the Parahippocampal Place Area to spatial cues in a movie and it's audio-description}

\author{Christian~O.~Häusler\textsuperscript{1,2{*}}, Michael Hanke\textsuperscript{1,2}}
% https://www.nature.com/sdata/publish/for-authors#other-formats

\maketitle
\thispagestyle{fancy}

1. Psychoinformatics Lab, Institute of Neuroscience and Medicine, Brain \&
Behaviour (INM-7), Research Centre Jülich, Jülich, Germany,
2. Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University,  Düsseldorf, Germany
{*}corresponding author: Christian Olaf Häusler (der.haeusler@gmx.net)

\begin{abstract}
% < 170 words new analysis of existing data of interest to a broad section of
% our audience highlighting innovative examples of data reuse may be used to
% present compelling new findings & conclusions derived from published.
Intro: PPA as classic example for an visual area; Methods: fMRI data
studyforrest data set, general linear model; Results: significant clusters in
PPA, RSC, individual results in x of 15 subjects; Conclusions: PPA nicht
exclusively \end{abstract}

\todo[inline]{Vorgabe: Intro + Discussion = main text ca. 3,000 words; einige
offene einige Dinge sind folgend in Kommentaren und nicht als ToDo gemarkt}

\todo[inline]{include figure providing schematic overview of the study and assay
design -> is overkill: Steht in den entsprechenden Veröffentlichungen \&
Paradigma ist mit "Film ab" wenig kompliziert}

\todo[inline]{our system cannot accept BibTeX bibliography files; authors who
    wish to use BibTeX to prepare their references should therefore copy the
    refernce list from the .bbl file that BibTeX generates and paste it into the
    main manuscript .tex file (and delete the associated
    \textbackslash{}bibliography and \textbackslash{}bibliographystyle
commands)}

\todo[inline]{Ich arbeite erstmal wie gehabt mit der references.bib
    und habe oben usepackage{natbib}' eingefügt}

\section{Introduction} Cognitive neuroscientists use brain imaging methods like
blood oxygenation level-dependent functional magnetic resonance imaging (BOLD
fMRI) to map perceptual and cognitive brain processes (e.g. viewing of
landscapes, \citep{epstein1998ppa}; or theory of mind
\citep{spunt2014validating}) to functional brain areas or networks. Brain
mapping studies have traditionally employed small sets of carefully chosen,
simplified stimuli to strictly control experimental conditions and facilitate
functional isolation of brain areas. The disadvantage is that a limited number
of simplified stimuli does not resemble our real-life experience and lacks
external validity \citep{westfall2016fixing} as well as ecological validity
\citep{hasson2004intersubject}. Additionally, brain mapping studies usually
average results across at least 10-15 subjects to improve the
signal-to-noise-ratio (SNR). The disadvantage here is that the averaging
approach does not characterize brain functions at an individual level, a
prerequisite for the application of brain imaging methods in individual
diagnostics (cf. \citep{dubois2016building})

\todo[inline]{Retrosplenial Cortex kommt in Einleitung bisher  nicht vor; wird
aufgrund der Ergebnisse in AV und AO in Discussion diskutiert werden; cluster
sind in den Abbildungen nicht zu sehen; kommen aber in (den Tabellen) der
Results Section }

% Was ist PPA?
A classic example of a functional, higher-level visual area is the
``parahippocampal place area'' (PPA) \citep{epstein1998ppa,
epstein1999parahippocampal}. The PPA is located in the posterior parahippocampal
gyrus at the boundary to the anterior lingual gyrus and medial fusiform gyrus,
both being part of the ventral visual stream \citep{mishkin1982contribution,
goodale1992separate}. Increased hemodynamic activity in the PPA correlates with
the perception of static pictures of landscapes or landmarks compared to
pictures of tools or faces.  According to the spatial layout hypothesis, the PPA
is involved in processing the surface geometry of a visual stimulus and possibly
in identifying scenes based on their spatial layout \citep{epstein2010reliable}.

% Literature Review PPA: Einleitung
At a group avergae level, results generalize to mental imagination of landscapes
\citep{ocraven2000mental} and haptic exploration of scenes constructed from LEGO
blocks \citep{wolbers2011modality}. One study that used spoken, place-related
sentences showed significantly decreased activation in the left and no
modulation in the right-hemispheric PPA \citep{aziz2008modulation}.

% Studien im Einzelnen: O'Craven: Mental imagery
In the study conducted by \citep{ocraven2000mental} participants viewed
alternating blocks of pictures showing famous faces and familiar places during
an initial experimental paradigm. In a subsequent paradigm, participants were
instructed to ``form a vivid mental image'' of the previously viewed pictures.
The PPA showed increased activation during imagination of places compared to
faces but the imagination tasks showed a smaller activation level compared to
the perceptual task.

% Wolbers: haptic exploration
In the block design study conducted by \citep{wolbers2011modality} the PPA of
sighted participants showed increased activation during a delayed
matching-to-sample task of haptically explored scenes constructed from LEGO
bricks compared to haptically explored abstract geometric objects.

\todo[inline]{Er hat noch eine Connectivitäts-Analyse mit dem Occipital Cortex
gemacht. Im Vergleich zum visuellen Paradigma ist im haptischen Paradigma keine
Connectivität. Interpretation: Es kann keine mental imagery sein}

\todo[inline]{"The scene-related increase in coupling with the PPA was
significantly stronger in multiple clusters in occipital cortex under visual
than haptic stimulation (Figure 2B; Table S2)"}

% Aziz (2008): place related sentences
\todo[inline]{Aziz hat auf eine zweite Mail geantwortet: Sie modelt die HRF über
den jeweils gesamten Satz von ca. 2.8 Sekunden}

\todo[inline]{Ihre Method Section ist strange und es bei ihr überall eh nur
    uneinheitliche Grütze heraus; Ich blicke nicht durch; was folgend kommt,
    wird nicht vollständig korrekt sein. So wie ich es verstanden habe, hat sie
    den Unterschied zwischen PPA und FFA angeguckt, aber diesen Unterschied im
Kontrast zu object-related Sentences (dortige Fig 3; lass uns kurz im Gespräch
drübergucken, ich habe die relvanten Abschnitte in der PDF markiert)}

To our knowledge only one study by \citep{aziz2008modulation} correlated
hemodynamic activity of the PPA with spoken sentences.  Sentences described
generic or famous places (e.g. ``The Taj Mahal faces a long thin reflecting
pool''), faces  (e.g. ``Marylin Monroe has a large square jaw''), or objects
(e.g. ``The television has a long antenna')'. Participants were instructed to
press a button whenever the sentence described an inaccurate or improbable fact.
Surprisingly, activation in the left, but not right, PPA was significantly
reduced when participants listened to place-related sentences compared to
listening to face-related sentences. Moreover, this effect was only observed in
sentences involving famous places.

\todo[inline]{Review von Aminoff (2013) kommt nur kurz als "competing hypothesis"; muss man dann natürlich in der Discussion diskutieren; was aber ggf. gar nicht notwendig ist, weil's darum gar bei uns gar nicht geht}

% Summary if literature review
In summary, studies suggest that the PPA does not exclusively respond to
visual(ly) spatial information. This is consistent with a competing hypotheses
stating the PPA does not process the spatial layout of a scene specifically but
contextual associations in general (\citep{aminoff2006parahippocampal,
aminoff2013role}). It is still uncertain if auditory stimuli lead to inhibitory
or excitatory responses, or no change at all in the PPA.

\todo[inline]{aber Huth (2016) implizit; diskutiert PPA also solches jedoch
nicht und macht voxel-wise encoding, was - glaube ich - über
excitation/inhibition nichts aussagt -> Paper checken}

% Shortcomings of previous studies; lack of research
All revied studies used a just a small set of carefully designed, simplified
stimuli to strictly control experimental conditions. They further relied on
explicit judgment-based tasks to ensured that participants payed attention to
the stimuli. This raises the question how the PPA behaves under life-like conditions.

\todo[inline]{Data-driven vs. model-driven analyses ist komplett raus; in
Discussion bringen (vgl. Speech-Anno-Paper}

% Why naturalistic stimuli
Naturalistic stimuli like movies \citep{hasson2008neurocinematics,
sonkusare2019naturalistic} or narratives \citep{honey2012not,
lerner2011topographic, silbert2014coupled} offer complex, continuous stimuli
that better resemble our experience of a our dynamic environment.

% Literature review naturalistic stimuli: Einleitung
Two early studies using naturalistic stimuli suggest functional specialization is preserved during free viewing of a movie when many areas respond simultaneously and interact [+++ der Schlusssatz ist aus hasson und bartels plagiiert \citep{bartels2004mapping, hasson2004intersubject}+++]

% Bartels (2004): Film + GLM (jedoch ohne PPA) und Hasson (2004)
\citep{bartels2004mapping} manually annotated time points that depict faces and
human bodies in a 22 minute clip of the movie \textit{Tomorrow Never Dies}
[+++Ref+++].
Employing a general linear model (GLM) to model hemodynamic activity, these time
points correlated with increased activity in the ``fusiform face area''
\citep{kanwisher1997ffa} and ``extrastriate body area''
\citep{downing2001bodyarea} respectively.

% Hasson (2004): Film + (reverse) ISC
Employing a reverse intersubject correlation, \citep{hasson2004intersubject}
used a 30 minute clip of the movie \textit{The Good, the Bad, the Ugly}
[+++Ref+++]. fMRI
brain volumes showing the highest hemodynamic activity in the PPA [across (five)
participants] were matched to the temporally corresponding movie frames.
Results revealed that corresponding movie frames depicted indoor and outdoor
scenes including buildings and open fields.

% Summary naturalistic stimuli
Nevertheless, no study investigated how the PPA responds to spatial clues that are embedded in a complex, audio-only stimulus.
\todo[inline]{Huth (2016) checken}


\todo[inline]{The next paragraphs contain a bunch of methods already; reduce
this to talk about the general approach:

a) annotation of events

b) investigation of model quality,

c) cross-analysis of datasets

d) Possibly cite https://psyarxiv.com/sdbqv/ and declare that this study is
a concrete realization of that promise}

\todo[inline]{Präteritum oder Präsens? Ist momentan gemischt; find's ok so}

\todo[inline]{Inner join of AV \& AO: N=14}

% What we did: Einleitung
In this study, we analyze data of three publicly available datasets from the
same set of 14 participants as part of the studyforrest data set
(\href{http://www.studyforrest.org}{studyforrest.org}). We exploratorily
investigate how the parahippocampal gyrus [or more general ``human brain''?]
responds to ``spatially relevant'' events embedded in a 2h audio-visual movie
\citep{hanke2016simultaneous} and its audio-description (i.e. the audio-only
variant) of the same length \citep{hanke2014audiomovie}. We compare these
results to PPA localizations that were obtained from a dedicated visual
localizer experiment \citep{sengupta2016extension}.

Both natural stimuli were originally designed to entertain an audience and to capture attention free from any perceptual or behavioral task.
We annotated events in the otherwise unknown temporal structure of both stimuli.
In/for the movie, we annotated different kinds of cinematographic cuts \citep{haeusler2016cutanno} as presumably ``spatially relevant'' events.
Our rationale was that cuts re-orient the viewer in the depicted environment
indepenten from the exact visual content in the movie frame following the cut.
In/for the audio-description, we annotated verbal clues about the missing visual content given by the audio-description's additional narrator \citep{haeusler2020speechanno}.

In conformity to the analyses performed in the studies using not-naturalistic stimuli, we employed a standard multi-level, voxel-wise general linear model (GLM) % in order to???
\todo[inline]{in order to\dots}

Encouraged by \citep{aziz2008modulation} and \citep{aminoff2013role}, we focus on the audio-description as a possible life-like stimulus free from any task to localize a ``visual area'' in individual study participants.

\todo[inline]{Bzgl. Open Source der Ergebnisse/Skripte: Dazu habe ich hier nichts geschrieben, weil ich nur weiß, das der Code veröffentlicht werden soll. Du hast die Vision diesbezüglich im Kopf und entsprechend auch kanonisch-prägnante Formulierungen und wo die am besten wie hinkommen}


\section{Methods}

\todo[inline]{Specific data outputs should be explicitly referenced via data
citation (see Data Records and Data Citations, below)}

We used data from three publicly available datsets as part of the studyforrest
dataset (\href{http://www.studyforrest.org}{studyforrest.org}) that has already
been used by other research groups for independent research questions before.

We used fMRI data from 14 subjects who took part in three different studies. Namely, the same subjects were

a) listening to the audio-description (AO study; \citep{hanke2014audiomovie}) of
the movie ``Forrest Gump'', a dataset already used by \citep{hu2017decoding}.
b) watching the actual audio-visual movie (AV study;
\citep{hanke2016simultaneous}, a dataset already used by
\citep{ben2018hippocampal}. c) participating in a dedicated block-wise visual
localizer (LOC study) \citep{sengupta2016extension}, a dataset aready used by
\citep{jiahui2019predicting}.

An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications  \citep{hanke2014audiomovie, hanke2016simultaneous,
sengupta2016extension}. Following is a brief summary of most important aspects.

\todo[inline]{Alles um Sengupta ergänzen}


\subsection{Participants}

\todo[inline]{inner join von AO \& AV: N=14; AO: N=19; es fehlt in beiden VP10;
Angaben zu Alter (Durchschnitt; ggf. Range) und Geschlecht sind folglich falsch}

In the AO study, 19 participants (all right-handed, age 21–38 years, mean age
26.6 years, 12 male) listened to the German audio-description
\citep{ForrestGumpGermanAD} of the movie ``Forrest Gump''
\citep{ForrestGumpMovie} as an additional audio track for visually impaired
listeners on Swiss public television. In the AV study, the same 14 participants
(21–39 years, mean age 29.4, six female) watched the audio-visual movie with dubbed German audio track \citep{ForrestGumpDVD}. All participants reported to have normal hearing, normal or corrected-to-normal vision, and no known history of neurological disorders.

\todo[inline]{Sengupta (2016)}
In the LOC study\dots

In both studies, participants received monetary compensation and gave written informed consent for their participation and for public sharing of all obtained data in anonymized form. The studies had prior approval by the Ethics Committee of Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli}
% stimulus name & references
We used the German DVD release \citep{ForrestGumpDVD} of
the audio-visual movie ``Forrest Gump'' \citep{ForrestGumpMovie} and its
temporally aligned audio-description (i.e. the audio-only variant of the movie)
that was broadcast as an additional audio track for visually impaired listeners
on Swiss public televinsion \citep{ForrestGumpGermanAD}.

% difference in audio track AV vs. AO
The plot of the movie is already carried by an off-screen voice of the main
character Forrest Gump.  In the largely identical audio-description, a male
narrator additionally describes essential aspects of the visual scenery when
there is no off-screen voice, dialog, or other relevant auditory content.

% stimulus creation
The audio-description was temporally aligned to the audio track of the German
DVD release. A few scenes less relevant to the major plot were removed to create
the ``research cut'' lasting $\approx$2h
\citep{hanke2014audiomovie, hanke2016simultaneous}. Both stimuli were further
processed (filtering, volume adjustments) to improve audibility during MRI
scanning. Finally, each stimulus was split into eight segments of approximately
15 minutes.  Except for the first movie segment, each segment started with a
snippet of at least six seconds immediately preceding the movie scene boundary
used to split the segments (s. Figure 3a in \citep{hanke2014audiomovie}.

\todo[inline]{Sengupta (2016)}


\subsection{Naturalistic stimuli annotation}

\todo[inline]{aus Speech-Anno Paper 'descriptive-statistics-anno.py' übernehmen \& anpassen, um Statistiken und runcommand-file zu erstellen}

% annotation procedure
We extended a publicly available annotation of speech \citep{haeusler2020speechanno} by manually annotating the audio-description's narrator: Two persons performed a categorization of nouns embedded in sentences spoken by the narrator to describe the movie's missing visual content.

\todo[inline]{WARUM WAS WIE annotiert wurde; warum Kategorien so gewählt?}

% noun categorization
% Kategorie ``++'' enthält ausnahmesweise auch adverbiale Best. der Zeit.
The nouns were categorized by the clue they provide about the cinematographic
scene's environment (geo, geo-room; setting\_new, setting\_old), its inherent
persons (female, female name; male, male name, person), a person's appearance
(face, head; body, bodypart), and a scene's inherent objects (object,
furniture). See Table \ref{tab:descr-nouns} for a complete overview of all
cateogries [+++rules, counts, examples].

A preliminary annotation was performed by one person according to the rules (see
Table XYZ). Minor corrections due to incorrectly applied rules were by the
author.

% Tabelle der Descriptive Nouns;
\todo[inline]{decr-stats-anno.py; Schrägstriche vor Platzhalter}

\todo[inline]{Caption hübsch machen}

\begin{table*}[t]
    \caption{Narrator's descriptive nouns.
        All categories sorted alphapetically.
    Examples are given in English.}
\label{tab:descr-nouns}
\begin{tabular}{llllllllllll}
\toprule
\textbf{category} & \textbf{rule} & \textbf{examples} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & trunk of the body; overlaid clothes & back, hip, shoulder; jacket, dress shirt & aDescrBodyAll & aDescrBodyI & aDescrBodyII & aDescrBodyIII & aDescrBodyIV & aDescrBodyV & aDescrBodyVI & aDescrBodyVII & aDescrBodyVIII \tabularnewline
bodypart & limbs and trousers & arm, finger, leg, toe & aDescrBodypartAll & aDescrBodypartI & aDescrBodypartII & aDescrBodypartIII & aDescrBodypartIV & aDescrBodypartV & aDescrBodypartVI & aDescrBodypartVII & aDescrBodypartVIII \tabularnewline
face & face or parts of it & face, ear, nose, mouth & aDescrFaceAll & aDescrFaceI & aDescrFaceII & aDescrFaceIII & aDescrFaceIV & aDescrFaceV & aDescrFaceVI & aDescrFaceVII & aDescrFaceVIII \tabularnewline
female & female person & nurse, mother, woman & aDescrFemaleAll & aDescrFemaleI & aDescrFemaleII & aDescrFemaleIII & aDescrFemaleIV & aDescrFemaleV & aDescrFemaleVI & aDescrFemaleVII & aDescrFemaleVIII \tabularnewline
females & female persons & women & aDescrFemalesAll & aDescrFemalesI & aDescrFemalesII & aDescrFemalesIII & aDescrFemalesIV & aDescrFemalesV & aDescrFemalesVI & aDescrFemalesVII & aDescrFemalesVIII \tabularnewline
fname & female name & Jenny & aDescrFnameAll & aDescrFnameI & aDescrFnameII & aDescrFnameIII & aDescrFnameIV & aDescrFnameV & aDescrFnameVI & aDescrFnameVII & aDescrFnameVIII \tabularnewline
furniture & movable objects insides/outsides & bench, bed, table, chair & aDescrFurnitureAll & aDescrFurnitureI & aDescrFurnitureII & aDescrFurnitureIII & aDescrFurnitureIV & aDescrFurnitureV & aDescrFurnitureVI & aDescrFurnitureVII & aDescrFurnitureVIII \tabularnewline
geo & immobile landmarks & building, tree, street, alley, meadow, cornfield, river & aDescrGeoAll & aDescrGeoI & aDescrGeoII & aDescrGeoIII & aDescrGeoIV & aDescrGeoV & aDescrGeoVI & aDescrGeoVII & aDescrGeoVIII \tabularnewline
geo-room & rooms / locales; elements defining a locale's spatial layout & living room; wall, door, window, floor, turf & aDescrGeoroomAll & aDescrGeoroomI & aDescrGeoroomII & aDescrGeoroomIII & aDescrGeoroomIV & aDescrGeoroomV & aDescrGeoroomVI & aDescrGeoroomVII & aDescrGeoroomVIII \tabularnewline
head & non-face parts of the head; worn headgear & head, hair, ear, neck, helmet & aDescrHeadAll & aDescrHeadI & aDescrHeadII & aDescrHeadIII & aDescrHeadIV & aDescrHeadV & aDescrHeadVI & aDescrHeadVII & aDescrHeadVIII \tabularnewline
male & male person & man, father, soldier & aDescrMaleAll & aDescrMaleI & aDescrMaleII & aDescrMaleIII & aDescrMaleIV & aDescrMaleV & aDescrMaleVI & aDescrMaleVII & aDescrMaleVIII \tabularnewline
males & male persons & boys, opponents & aDescrMalesAll & aDescrMalesI & aDescrMalesII & aDescrMalesIII & aDescrMalesIV & aDescrMalesV & aDescrMalesVI & aDescrMalesVII & aDescrMalesVIII \tabularnewline
mname & male name & Bubba, Kennedy & aDescrMnameAll & aDescrMnameI & aDescrMnameII & aDescrMnameIII & aDescrMnameIV & aDescrMnameV & aDescrMnameVI & aDescrMnameVII & aDescrMnameVIII \tabularnewline
object & countable entity with firm boundarie & telephone, car & aDescrObjectAll & aDescrObjectI & aDescrObjectII & aDescrObjectIII & aDescrObjectIV & aDescrObjectV & aDescrObjectVI & aDescrObjectVII & aDescrObjectVIII \tabularnewline
objects & countable entities with firm boundaries &  wheels, photos & aDescrObjectsAll & aDescrObjectsI & aDescrObjectsII & aDescrObjectsIII & aDescrObjectsIV & aDescrObjectsV & aDescrObjectsVI & aDescrObjectsVII & aDescrObjectsVIII \tabularnewline
persons & concrete persons of unknown sex / gender & hippies, patients & aDescrPersonsAll & aDescrPersonsI & aDescrPersonsII & aDescrPersonsIII & aDescrPersonsIV & aDescrPersonsV & aDescrPersonsVI & aDescrPersonsVII & aDescrPersonsVIII \tabularnewline
setting\_new & setting mentioned for the first time &  on a ``bridge'', on an ``alley'', on ``campus'' & aDescrSettingnewAll & aDescrSettingnewI & aDescrSettingnewII & aDescrSettingnewIII & aDescrSettingnewIV & aDescrSettingnewV & aDescrSettingnewVI & aDescrSettingnewVII & aDescrSettingnewVIII \tabularnewline
setting\_rec & recurring setting & at the ``bus stop'' & aDescrSettingrecAll & aDescrSettingrec & aDescrSettingrecII & aDescrSettingrecIII & aDescrSettingrecIV & aDescrSettingrecV & aDescrSettingrecVI & aDescrSettingrecVII & aDescrSettingrecVIII \tabularnewline
++ & nouns, adverbial adjectives and adverbs & in the ``evening'', it's ``daytime'', ``later'' &  aDescrAll & aDescrI & aDescrII & aDescrIII & aDescrIV & aDescrV & aDescrVI & aDescrVII & aDescrVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsection{Procedure}

In both [all three] studies, participants filled out a questionnaire on their
basic demographic information and familiarity with the audio-description or the
movie. Participants were instructed to inhibit physical movements except for
eye-movements, and otherwise to simply ``enjoy the audiobook'' or ``enjoy the
movie'' respectively. Audio-description and movie segments were presented in
chronological order with four segments in two fMRI sessions respectively.
Between sessions, participants left the scanner for a break with a flexible
duration. Structural images were obtained during the first study on a day
different from the fMRI session.


\subsection{Stimulation setup}

\todo[inline]{irrelevantes kürzen}
In both studies, stimulation was implemented with \href{http://www.psychopy.org}{PsychoPy} \citep{peirce2007psychopy} running on a computer with the \href{http://neuro.debian.net}{(Neuro)Debian} operating system \citep{halchenko2012open}.

In the AO study, visual instructions were presented on a rear-projection screen
inside scanner bore using an LCD projector (DLA-G150CL, JVC Ltd.). During the
functional scans, the projector presented a medium gray screen with the primary
purpose to illuminate a participant's visual field in order to prevent premature
fatigue. [The screen contained a solid black ``fixation'' dot that faded in and
out at the beginning and end of a movie segment.]

In the AV study, visual instructions and stimuli were presented on a
rear-projection screen using an LCD projector (JVC DLA RS66E, JVC Ltd., light
transmission reduced to \unit[13.7]{\%} with a gray filter) connected to the
stimulus computer [via a DVI extender system (Gefen EXT-DVI-142DLN with
EXT-DVI-FM1000)]. The movie was shown at a viewing distance of \unit[63]{cm} in
\unit[720]{p} resolution at full width on a \unit[1280 $\times$ 1024]{pixel}
screen with \unit[60]{Hz} video refresh rate and screen dimension of \unit[26.5
$\times$ 21.2]{cm}, corresponding to \unit[23.75 $\times$ 13.5] or \unit[23.75
$times$ 10.25]{cm} when considering only the movie content and excluding the
horizontal gray bars.

In both studies [+++???+++] auditory stimulation was delivered through an MR confon mkII+ driving custom-built in-ear headphones (HP-M01, MR confon GmbH, Magdeburg, Germany; \citep{baumgart1998electrodynamic}) that reduced the scanner noise by at least \unit[20–30]{dB}.
Headphones were  fed from an Aureon 7.1 USB (Terratec) sound card through an optical connection.


\subsection{fMRI data acquisition}

\todo[inline]{irrelevantes kürzen}

In both studies, a total of 3599 volumes were recorded for each participant
(451, 441, 438, 488, 462, 439, 542, and 338 volumes for movie segment 1–8
respectively).

% AO (Hanke, 2014)
In the AO study, a whole-body \unit[7]{Tesla} Siemens MAGNETOM magnetic
resonance scanner equipped with a local circularly polarized head transmit and a
32 channel brain receive coil (Nova Medical, Inc., Wilmington, MA, USA) acquired
T2*-weighted echo-planar images (gradient-echo, \unit[2]{s} repetition time
(TR), \unit[22]{ms} echo time, \unit[0.78]{ms} echo spacing, \unit[1488]{Hz/Px}
bandwidth, generalized autocalibrating partially parallel acquisition,
acceleration factor 3, \unit[2]{Hz/Px} bandwidth in phase encoding direction).
% slices & FOV
36 axial slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) with a 10 percent inter-slice gap were recorded in ascending order.
Slices were oriented to include the ventral portions of frontal and order.
Slices were oriented to include the ventral the ventral portions of frontal and
occipital cortex while minimizing intersection with the eyeballs.  The field of
view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions [Oh, S. et
al. 2012; In, M. \& Speck, O., 2012; Chung, J. et al., 2011]. Auxiliary scans
for slice alignment and motion- and distortion-correction were performed at the
beginning of the first fMRI recording session and also after the break at the
start of the recording for the second half of the movie.

% AV (Hanke, 2016)
In the AV study, a whole-body \unit[3]{Tesla} Philips Achieva dStream MRI
scanner equipped with a 32 channel head coil acquired T2*-weighted echo-planar
images (gradient-echo, \unit[2]{s} repetition time (TR), \unit[30]{ms} echo
time, \unit[90]{$^{\circ}$} flip angle, \unit[1943]{Hz/Px} bandwidth, parallel
acquisition with sensitivity encoding (SENSE) reduction factor 2).
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction were recorded in ascending order. Philips' ``SmartExam'' was
used to automatically position slices in AC-PC orientation such that the topmost
slice was located at the superior edge of the brain.
\todo[inline]{In the LOC study\dots}


\subsection{Preprocessing}

\todo[inline]{bin wie immer bzgl. motion correction \& Templateerstellung unsicher}

\todo[inline]{cite sengupta2016 where this was first done and described, ie same image/data was used here}

\todo[inline]{downsampling der 7T Daten}

% data source
fMRI time series data were obtained from
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{GitHub}.
Data are already aligned to a subject-specific
\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms}{3
Tesla BOLD template} by a rigid body transformation in MCFLIRT
\citep{jenkinson2002registration} to correct for motion during scanning
\citep{sengupta2016extension}.

% preprocessing intro
All further analysis steps in the current study were carried out using tools of
FSL v5.0.9 (\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software
Library}; \citep{smith2004fsl} on a computer cluster running the
\href{https://www.debian.org}{Debian} GNU/Linux operating system. Software
packages were obtained from repositories of
\href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.

% actual preprocessing
Preprocessing was carried out using FEAT v6.00 (FMRI Expert Analysis Tool;
\citep{woolrich2001autocorr}) as part of FSL.
% temporal filtering
High-pass temporal filtering was applied to every segment using a
Gaussian-weighted least-squares straight line with a cutoff period of
\unit[150]{s} (sigma=\unit[75.0]{s}) to remove low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model; \citep{woolrich2001autocorr})
to improve estimation efficiency \citep{woolrich2001autocorr}.

\subsection{Statistical analysis}

For both naturalistic stimli, we conducted a standard two-level general linear
model (GLM) analysis in FEAT to create subject-specific results ($Z$-maps)
across the 8 segments for every subject. A subsequent, third-level analysis averaged contrast estimates over subjects.


\subsubsection{first-level}

\paragraph{Regressors}

At the first level analyzing each segment for every subject, we created
regressors for the AO data from the narrator's descriptive nouns (see Table
\ref{tab:descr-nouns}) added to the annotation of speech
\citep{haeusler2020speechanno}. For the AV stimulus, we created regressors from
events provided by an annotation of movie cuts and depicted locations
\citep{haeusler2016cutanno}.
% https://github.com/psychoinformatics-de/studyforrest-data-annotations/blob/master/code/researchcut2segments.py}
First, both annotations were split into eight parts corresponding to the eight
segments used as stimuli during fMRI scanning.

% AO events
To create regressors for the AO analysis, XY from the XY original categories
were selected [+++why?+++]. Some categories were pooled / joined / merged
resulting in 11 new categories to provide more events per category per run.
% categories taken from movie cut annotation
Five categories were created from the annotation of movie cuts to capture
variance of a change in soundscape after cuts and tu create contrasts of
cross-modal negative control.
% nuissance regressors
Two regressors reflecting auditory (low-level) features (left-right difference
in volume and root mean squared volume) aligned to the frame rate of
(\unit[40]{ms}) were modeled to reduce nuisance effects.
% siehe Tabelle
An overview and short explanation of the 11 speech-related and 5 movie
cut-related categories and the two low-level auditory features ca be found in
Table \ref{tab:ao-events}.


\begin{table*}[t]
\caption{Overview of events to build the 18 regressors for the analysis of the audio-only (AO) stimulus. Some of the original categories (female, females, fname; male, males, mname; face, head; object, objects; see Table \ref{tab:descr-nouns}) were pooled into new categories (sex\_f; sex\_m; fahead; object).
\texttt{fg\_ad\_lrdiff} (left-right volume difference) and
\texttt{fg\_ad\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
For description of control condition for movie cuts see Table \ref{tab:av-events}}
\label{tab:ao-events}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & trunk of the body or overlaid clothes & \aoBodyAll & \aoBodyI & \aoBodyII & \aoBodyIII & \aoBodyIV & \aoBodyV & \aoBodyVI & \aoBodyVII & \aoBodyVIII \tabularnewline
bpart & limbs and trousers & \aoBpartAll & \aoBpartI & \aoBpartII & \aoBpartIII & \aoBpartIV & \aoBpartV & \aoBpartVI & \aoBpartVII & \aoBpartVIII \tabularnewline
fahead & face(parts) or head(parts) & \aoFaheadAll & \aoFaheadI & \aoFaheadII & \aoFaheadIII & \aoFaheadIV & \aoFaheadV & \aoFaheadVI & \aoFaheadVII & \aoFaheadVIII \tabularnewline
furn & big inventory of rooms (insides/outsides) & \aoFurnAll & \aoFurnI & \aoFurnII & \aoFurnIII & \aoFurnIV & \aoFurnV & \aoFurnVI & \aoFurnVII & \aoFurnVIII \tabularnewline
geo & immobile landmarks & \aoGeoAll & \aoGeoI & \aoGeoII & \aoGeoIII & \aoGeoIV & \aoGeoV & \aoGeoVI & \aoGeoVII & \aoGeoVIII \tabularnewline
groom & elements defining a locale's spatial layout & \aoGroomAll & \aoGroomI & \aoGroomII & \aoGroomIII & \aoGroomIV & \aoGroomV & \aoGroomVI & \aoGroomVII & \aoGroomVIII \tabularnewline
object & inanimate entities with firm boundaries & \aoObjAll & \aoObjI & \aoObjII & \aoObjIII & \aoObjIV & \aoObjV & \aoObjVI & \aoObjVII & \aoObjVIII \tabularnewline
se\_new & first-time mentioned setting & \aoSenewAll & \aoSenewI & \aoSenewII & \aoSenewIII & \aoSenewIV & \aoSenewV & \aoSenewVI & \aoSenewVII & \aoSenewVIII \tabularnewline
se\_old & recurring setting & \aoSeoldAll & \aoSeoldI & \aoSeoldII & \aoSeoldIII & \aoSeoldIV & \aoSeoldV & \aoSeoldVI & \aoSeoldVII & \aoSeoldVIII \tabularnewline
sex\_f & female person(s), name & \aoSexfAll & \aoSexfI & \aoSexfII & \aoSexfIII & \aoSexfIV & \aoSexfV & \aoSexfVI & \aoSexfVII & \aoSexfVIII \tabularnewline
sex\_m & male person(s), name & \aoSexmAll & \aoSexmI & \aoSexmII & \aoSexmIII & \aoSexmIV & \aoSexmV & \aoSexmVI & \aoSexmVII & \aoSexmVIII \tabularnewline
% vlo_ch has no events in segment 5. So indices are shifted
vse\_new & control for movie cut & \aoVsenewAll & \aoVsenewI & \aoVsenewII & \aoVsenewIII & \aoVsenewIV & \aoVsenewV & \aoVsenewVI & \aoVsenewVII & \aoVsenewVIII \tabularnewline
vse\_old & control for movie cut & \aoVseoldAll & \aoVseoldI & \aoVseoldII & \aoVseoldIII & \aoVseoldIV & \aoVseoldV & \aoVseoldVI & \aoVseoldVII & \aoVseoldVIII \tabularnewline
vlo\_ch & control for movie cut & \aoVlochAll & \aoVlochI & \aoVlochII & \aoVlochIII & \aoVlochIV & 0 & \aoVlochV & \aoVlochVI & \aoVlochVII \tabularnewline
vpe\_new & control for movie cut & \aoVpenewAll & \aoVpenewI & \aoVpenewII & \aoVpenewIII & \aoVpenewIV & \aoVpenewV & \aoVpenewVI & \aoVpenewVII & \aoVpenewVIII \tabularnewline
% vpe_old has no events in 3. So the indices are shifted
vpe\_old & control for movie cut & \aoVpeoldAll & \aoVpeoldI & \aoVpeoldII & 0 & \aoVpeoldIII & \aoVpeoldIV & \aoVpeoldV & \aoVpeoldVI & \aoVpeoldVII \tabularnewline
fg\_ad\_lrdiff & left-right volume difference & \aoFgadlrdiffAll & \aoFgadlrdiffI & \aoFgadlrdiffII & \aoFgadlrdiffIII & \aoFgadlrdiffIV & \aoFgadlrdiffV & \aoFgadlrdiffVI & \aoFgadlrdiffVII & \aoFgadlrdiffVIII \tabularnewline
fg\_ad\_rms & root mean square loudness & \aoFgadrmsAll & \aoFgadrmsI & \aoFgadrmsII & \aoFgadrmsIII & \aoFgadrmsIV & \aoFgadrmsV & \aoFgadrmsVI & \aoFgadrmsVII & \aoFgadrmsVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% AV events
To create regressors for the AV analysis, we categorized the depicted location
in the frame following one of the 869 cuts in the movie into five categories:
1) unfamiliar setting (\texttt{vs\_new}),
2) familiar setting (\texttt{vs\_old}),
3) locale switch (\texttt{vlo\_ch}),
4) unfamiliar perspective (\texttt{vpe\_new}), and
5) familiar perspective (\texttt{vpe\_old}).
% no cut condition
A sixth regressors (\texttt{no\_cut}) was created to build negative control
contrasts.
\todo{how no\_cut condition was done; compare no\_speech oben und Masterarbeit}
% how it was done
Frames within a longer lasting were pseudo-randomly chosen by the following
rationale:
% siehe Tabelle
An overview and short explanation of these six regressors and six additional
event categories were of low-level auditory visual features of the movie can be
found in Table \ref{tab:av-events}.

\begin{table*}[t]
    \caption{Overview of events to build the 14 regressors for the analysis of the audio-only (AV) stimulus.
\texttt{fg\_av\_ger\_lr} (left-right difference in ??? of consecutive movie frame).
\texttt{fg\_av\_ger\_pd} (perceptual differences of consecutive frames).
\texttt{fg\_av\_ger\_ud} (up-down difference in ??? of consecutive movie frames).
\texttt{fg\_av\_ger\_lrdiff} (left-right volume difference) and \texttt{fg\_av\_ger\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
For description of control condition for movie cuts see Table \ref{tab:ao-events}}
\label{tab:av-events}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
vse\_new &  change of the camera position to a setting not depicted before & \aoVsenewAll & \aoVsenewI & \aoVsenewII & \aoVsenewIII & \aoVsenewIV & \aoVsenewV & \aoVsenewVI & \aoVsenewVII & \aoVsenewVIII \tabularnewline
vse\_old & change of the camera position to a recurring setting & \aoVseoldAll & \aoVseoldI & \aoVseoldII & \aoVseoldIII & \aoVseoldIV & \aoVseoldV & \aoVseoldVI & \aoVseoldVII & \aoVseoldVIII \tabularnewline
vlo\_ch & change of the camera position to another locale within the same setting & \aoVlochAll & \aoVlochI & \aoVlochII & \aoVlochIII & \aoVlochIV & 0 & \aoVlochV & \aoVlochVI & \aoVlochVII \tabularnewline
vpe\_new & change of the camera position within a locale not depicted before & \aoVpenewAll & \aoVpenewI & \aoVpenewII & \aoVpenewIII & \aoVpenewIV & \aoVpenewV & \aoVpenewVI & \aoVpenewVII & \aoVpenewVIII \tabularnewline
% vpe_old has no events in 3. So the indices are shifted
vpe\_old & change of the camera position within a recurring locale & \aoVpeoldAll & \aoVpeoldI & \aoVpeoldII & 0 & \aoVpeoldIII & \aoVpeoldIV & \aoVpeoldV & \aoVpeoldVI & \aoVpeoldVII \tabularnewline
vno\_cut & a continuous frame from the same camera position & \avVnocutAll & \avVnocutI & \avVnocutII & 0 & \avVnocutIII & \avVnocutIV & \avVnocutV & \avVnocutVI & \avVnocutVII \tabularnewline
se\_new & control for AO narrator & \aoSenewAll & \aoSenewI & \aoSenewII & \aoSenewIII & \aoSenewIV & \aoSenewV & \aoSenewVI & \aoSenewVII & \aoSenewVIII \tabularnewline
se\_old & control for AO narrator & \aoSeoldAll & \aoSeoldI & \aoSeoldII & \aoSeoldIII & \aoSeoldIV & \aoSeoldV & \aoSeoldVI & \aoSeoldVII & \aoSeoldVIII \tabularnewline
fg\_av\_ger\_lr & XYZ & \avFgavgerlrAll & \avFgavgerlrI & \avFgavgerlrII & \avFgavgerlrIII & \avFgavgerlrIV & \avFgavgerlrV & \avFgavgerlrVI & \avFgavgerlrVII & \avFgavgerlrVIII \tabularnewline
fg\_av\_ger\_lrdiff & XYZ & \avFgavgerlrdiffAll & \avFgavgerlrdiffI & \avFgavgerlrdiffII & \avFgavgerlrdiffIII & \avFgavgerlrdiffIV & \avFgavgerlrdiffV & \avFgavgerlrdiffVI & \avFgavgerlrdiffVII & \avFgavgerlrdiffVIII \tabularnewline
fg\_av\_ger\_ml & XYZ & \avFgavgermlAll & \avFgavgermlI & \avFgavgermlII & \avFgavgermlIII & \avFgavgermlIV & \avFgavgermlV & \avFgavgermlVI & \avFgavgermlVII & \avFgavgermlVIII \tabularnewline
fg\_av\_ger\_pd & XYZ & \avFgavgerpdAll & \avFgavgerpdI & \avFgavgerpdII & \avFgavgerpdIII & \avFgavgerpdIV & \avFgavgerpdV & \avFgavgerpdVI & \avFgavgerpdVII & \avFgavgerpdVIII \tabularnewline
fg\_av\_ger\_rms & XYZ & \avFgavgerrmsAll & \avFgavgerrmsI & \avFgavgerrmsII & \avFgavgerrmsIII & \avFgavgerrmsIV & \avFgavgerrmsV & \avFgavgerrmsVI & \avFgavgerrmsVII & \avFgavgerrmsVIII \tabularnewline
fg\_av\_ger\_ud & XYZ & \avFgavgerudAll & \avFgavgerudI & \avFgavgerudII & \avFgavgerudIII & \avFgavgerudIV & \avFgavgerudV & \avFgavgerudVI & \avFgavgerudVII & \avFgavgerudVIII \tabularnewline
\end{tabular}
\end{table*}


% modeling of regressors from events
For both stimuli, regressors created from nouns were modeled as boxcar functions
from the beginning to the end of noun.  Regressors created from movie cuts were
modeled as boxcar function lasting \unit[200]{ms} after a cut. Finally,
regressors created from low-level percetual features were modeled as box car
function from the beginning to the end of the corresponding movie frame lasting
\unit[40]{ms}. All regressors were convolved with a double gamma canonical
hemodynamic response function (HRF). Their temporal derivatives were added to
compensate for small timing differences \citep{friston1998event}.
% motion parameters
Six motion parameters were used as additional nuisance regressors.
% null regressors
If a segment did not contain a single event for a specific regressor, a null
regressor was chosen for that segment.
\todo{wenn ich "null regressor" nehme, darf ich den ganzen Run, glaube ich, in
der 2nd level Analyse gar nicht benutzen? Hatte, glaube ich, Jeanette Mumford in
einem ihren Videos mal gesagt} Finally, designs were filtered with the same
% high-pass filtering
high-pass filter (cutoff period of \unit[150]{s}) as the BOLD time series.
% siehe Tabelle für Correlation of regressors
\todo[inline]{hier kommt die Abbildung ``correlation of regressors''; ref to
fig:reg-corr}


\paragraph{audio stimulus: contrasts of parameter estimate}
\todo[inline]{Warum ist welche COPES so gemacht und warum COPE 1 die Nummer 1}
% wieso, weshalb, warum
See Table \ref{tab:ao-contrasts}

\begin{table*}[t]
\caption{Contrasts of parameter estimates for the analysis of the audio-only
    stimulus (AO).
\texttt{all non-geo} = all non-geo related regressors except furniture.}
\label{tab:ao-contrasts}
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{nr.} &  \textbf{contrasted} & \textbf{aim/goal} \\
\midrule
1 & geo, groom > all non-geo & PPA \tabularnewline
2 & geo, groom, se\_new > all non-geo & PPA \tabularnewline
3 & room, se\_new, se\_old > all non-geo & PPA \tabularnewline
4 & geo > all non-geo & PPA \tabularnewline
5 & groom > all non-geo & PPA \tabularnewline
6 & se\_new > all non-geo & PPA \tabularnewline
7 & se\_new, se\_old > all non-geo & PPA \tabularnewline
8 & se\_new > se\_old & PPA \tabularnewline
9 & vse\_new > vpe\_old & control at moments of cuts \tabularnewline
10 & vse\_new, vpe\_new > vse\_old, vpe\_old & control at moments of cuts \tabularnewline
11 & vse\_new > vse\_old & control at moments of cuts \tabularnewline
12 & vse\_new > vse\_old, vpe\_old & control at moments of cuts \tabularnewline
13 & vse\_new, vpe\_new > vpe\_old & control at moments of cuts \tabularnewline
\end{tabular}
\end{table*}


\paragraph{audio-visual stimulus: contrasts of parameter estimate}
% wieso, weshalb, warum
We created a contrast presumably correlating with increased hemodynamic activity
in the PPA by contrasting different kind of cuts, noteably without controlling
for the specific visual content depicted in the frame after each cut.
% cinematographic rationale
The cinematographic rationale was that - over the course of a movie - cuts shift
from establishing the setting and the spatial relationships within a setting at
the beginning of a movie to depicting persons and objects more relevant to the
subsequently evolved narrative later in the movie {brown2012cinematography;
mercado2011filmmakers}.
% neuroscientific rationale
The neurocientific rationale was that activity in the PPA is greater when
participants view novel versus repeated scenes resp. view-points
\citep{epstein1999parahippocampal}.
% im Ergebnis
Hence, we contrasted cuts to a setting occurring for the first time (vse\_new)
and cuts within a locale depicted for the first time (vpe\_new) with shots to a
recurring setting (vse\_old) and cuts within a recurring locale (vpe\_old).

\begin{table*}[t]
    \caption{Contrasts of parameter estimates for the analysis of the audio-visual movie (AV).}
\label{tab:av-contrasts}
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{nr.} &  \textbf{contrasted} & \textbf{aim/goal} \\
\midrule
1 & vse\_new > vpe\_old  & PPA \tabularnewline
2 & vse\_new, vpe\_new > vse\_old, vpe\_old & PPA \tabularnewline
3 & vse\_new > vse\_old & PPA \tabularnewline
4 & vse\_new > vse\_old, vpe\_old & PPA \tabularnewline
5 & vse\_new, vpe\_new > vpe\_old & PPA \tabularnewline
6 & vno\_cut > vse\_new & negative control \tabularnewline
7 & vno\_cut > vse\_old & negative control \tabularnewline
8 & vno\_cut > vse\_new, vse\_old & negative control \tabularnewline
9 & vno\_cut > vpe\_new, vpe\_old & negative control \tabularnewline
10 & se\_new > se\_old & control for narrator \tabularnewline
\end{tabular}
\end{table*}


\paragraph{Registration \& smoothing: individuals vs. group}

\todo[inline]{Wie immer bei Co-Registration: bitte prüfen, ob ich es richtig
geschrieben habe}

\todo[inline]{wie besprochen sind die individuelle Results im
subject space und werden erst später für die Bland-Altman-Plots nach MNI
transformiert}

% anatomical alignment of single subjects
The GLM analysis that fitted each voxel's time course separately for each
subject was performed in functional space preserving the orientation of the EPI
images. For subsequent subject-specific second level results, subject-specific
BOLD T2* time series were anatomically aligned via linear transformation to a
subject-specific, study-specific T2* template (s.
\href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/sub-01/bold3Tp2/brain.nii.gz"}{example})
that served as standard space.
% anatomical alignment for group results
For subsequent the third level results averaging contrasts of interest across
subjects, time series were anatomically aligned via non-linear transformation
[+++wie genau?+++] to a
\href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/templates/grpbold3Tp2/brain.nii.gz"}{study-specific
T2* group template}.
% MNI co-registration
To facilitate visualization and identification of cortical structures, this
reference images was later co-registered to the MNI 152 template with an affine
transformation (12 degrees of freedom)


\subsubsection{second level}
% model
The second-level analyses which averaged contrast estimates over the eight
stimulus segments were carried out using a fixed effects model by forcing the
random effects variance to zero in FLAME (FMRIB's Local Analysis of Mixed
Effects; \citep{beckmann2003general, woolrich2004multilevel}.
% thresholding
(Gaussianised T/F) statistic images were thresholded using clusters determined
by Z>2.3 and a (corrected) cluster significance threshold of p=.05
\citep{worsley2001statistical}.


\subsubsection{third level}
% model
Third level analyses which averaged contrast estimates over subjects were
carried out using FLAME stage 1 with automatic outlier detection
\citep{beckmann2003general, woolrich2004multilevel, woolrich2008robust}.
% thresholding
Z (Gaussianised T/F) statistic images were thresholded using clusters determined
by Z>3.4 and a (corrected) cluster significance threshold of p=.05
\citep{worsley2001statistical}.
% brain region identification
Brain regions associated with observed clusters were determined with the Juelich
Histological Atlas \citep{eickhoff2005toolbox, eickhoff2007assignment} and the
Harvard-Oxford Cortical Atlas \citep{desikan2006automated} provided by FSL.
% PPA masks
PPA ROI masks for individual subjects and a PPA group overlap mask were created
from data provided by \citep{sengupta2016extension}.

\subsection{Erstellung der Masken, spatial transformations}

\subsubsection{plot-group-and-individuals.py}

\subsubsection{plot-bland-altman.py}

% generate-masks.py; DO NOT TOUCH!
% FOR GROUP:
% 0. In FSLeyes, manually created probabilistic brain lobe maps from \href{"https://doi.org/10.5281/zenodo.1470761"}{the MNI Structural Atlas} (./data/atlases/mni_prob_*.nii.gz)
% 1. these brain lobe maps were aligned (via FSL's flirt -applyxf with 12 dof) to the \href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/templates/grpbold3Tp2/brain.nii.gz"}{study-specific T2* group template} (./data/atlases/grpbold3Tp2_mni_prob_*.nii.gz)
% 2. from \href{"https://github.com/psychoinformatics-de/studyforrest-data-visualrois"} {sengupta2016extension}: UNIlateral probabilistic group ROI masks (data/masks/*_overlap.nii.gz) -> BIlateral probabilistic & binary grp ROI masks in grp space (.data/masks/bilat_???_prob.nii & .data/masks/bilat_???_binary.nii )
% FOR SUBJECTS:
% 1. bilateral grp ROI masks in grp space -> bilateral grp ROI masks in subject space (applywarp; sub-??/masks/grp_*_bin.nii.gz)
% 2. probabilistic MNI mask -> subject space (via FSL's applywarp)
% 3. merge probabilistic occipital & temporal mask in subject space
%	(sub-??/masks/mni_prob_occip_tempo.nii.gz)
% 4. created subject-specific FOV from 4d 7t audio drama time series
%	(./sub-??/masks/ao_fov_mask.nii.gz)
% 5. individual, unilateral ROIs from {sengupta2016extension} were joined and co-registered to MNI 152 (1mm) space, and binarized
% 6. individual (un)thresholded zmaps (from the COPEs) were co-regi<wstered with MNI 152 (1mm) template.


\section{audio-only stimulus: results}
Primary contrast, probability over contrasts (individual clusters per contrasts,
individuelle Bland-Altman-Plots für primary contrast

\begin{table*}[t]
    \caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected) for the contrast \texttt{geo, groom > all non-geo} (audio-only stimulus).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value, followed by brain structures from posterior to anterior, and partially covered areas.}
\label{tab:res-ao-group1}
\begin{tabular}{rrrrrrrrrp{3cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
188 & 5.96E-08 & 4.48 & -17.5 & -65.5 & 25.5 & -14.7 & -59.1 & 15.2 & l. precuneus \\
164 & 2.38E-07 & 4.47 & 17.5 & -58 & 23 & 15.6 & -55.6 & 16 & r. precuneus; \\
83 & 0.000128 & 4.48 & 27.5 & -43 & -17 & 27.2 & -41.1 & -14 & r. temporal (occipital) fusiform c.; posterior parahippocampal g. \\
73 & 0.000318 & 3.93 & -22.5 & -43 & -12 & -23.9 & -43.6 & -11.2 & l. lingual g.; temporal (occipital) fusiform g., posterior parahippocampal c. \\
63 & 0.000824 & 4.1 & 40 & -75.5 & 30.5 & 40.9 & -76.3 & 28.6 & r. superior lateral occipital c. \\
37 & 0.0129 & 4.24 & -37.5 & -78 & 33 & -38.4 & -79.5 & 28.9 & l. superior lateral occipital c. \\
\bottomrule
\end{tabular}
\end{table*}


\section{audio-visual stimulus: results}
Primary contrast, probability over contrasts (individual clusters per contrasts,
individuelle Bland-Altman-Plots für primary contrast.


\begin{table*}[t]
    \caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected) for the contrast \texttt{vse\_new > vpe\_old} (audio-visual stimulus).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value, followed by brain structures from posterior to anterior, and partially covered areas.}
\label{tab:res-ao-group1}
\begin{tabular}{rrrrrrrrrp{3cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
3003 & 0 & 5.31 & 22.5 & -45.5 & -12 & 4.53 & -63.3 & -3.72 & r. lingual gyrus; r cuneal c., intracalarine c., bilaterally occipital fusiform g., temporal fusiform c., posterior parahippocampal c.  \\
154 & 6.56E-07 & 4.46 & -35 & -83 & 28 & -32.8 & -86.2 & 21.4 & l. superior lateral occipital c. \\
121 & 7.69E-06 & 4.65 & 25 & -80.5 & 25.5 & 23.7 & -83.8 & 25.4 & r. superior lateral occipital cortex \\
\bottomrule
\end{tabular}
\end{table*}

% MIH the discussion should primarily be anchored on the points the introduction concludes with. From my POV there is no need (and likely no space) to discuss results by stimulus type. Issue like different impact of attentional focus etc can be brought up in the joint paragraph that discussed how such a factor could have differentially impacted the results -- and if that would matter.
\section{Discussion (audio)}
% Hanke et al. (2014): Compared to an audio-visual movie, this is a stimulus that leaves a much larger margin for inter-individual differences in imagining scenery, as well as actors' character and personality traits, while still preserving the time-locked presentation of information to a listener. At the same time, an auditory stimulus limits the effect of an attentional focus on the selection of a subset of simultaneously occurring auditory events, in contrast to the selection of different parts of the visual field.
% mental imagery hypthesis (s. O'Craven \& Kanwisher, 2000)
% Lerner (2011), Honey (2012), Silbert (2014) finden keine ISC in PPA? vgl. DataPaper bzgl. Hörbuch??
% setting\_new ist häufig vermischt mit anderer Kategorie "ein Arzt", "ein alter Film", "auf einer Straße", "im Fernsehen", "die drei Jungs", "die zwei im Bett", "Jenny als Teenager", "Forrest vor einem Buffet", "Kennedy", "Jenny kommt aus dem Wohnwagen ihrer Großmutter"
% Soundscape macht es häufiger bereits vorher klar, dass Settingwechsel ist bzw. nicht erst das annotiere Substantiv
% object und (room)geo ist vermischt
% Retrospelinal Cortex ist in Probabilistic maps krasser als PPA.

% From Poster:
% 1. PPA shows increased activation during non-visual, complex auditory stimulation that provides incidental spatial information.
% 2. only minor temporal correlation of regressors of the audio drama & movie that were not designed for experimental research.
% 3. but still, alternative contrasts show significant clusters in auditory cortices.
% 4. inter-individual differences: for two subjects, an alternative audio-contrast shows bilaterally significant clusters whereas the primary contrast shows no significant clusters.
% 5. audio drama shows bilaterally increased activation in subject 4 in which the visual localizer shows an unilateral cluster.
% 6. nevertheless, a  complex auditory stimulus might allow individual localization of a functionally defined brain region.
% 7. results of other studies suggest7 the PPA does not exclusively process the ``spatial layout''" but ``contextual associations''.
% 8. an audio drama might be suitable as ecologically more valid ``localizer'' for a variety of brain functions.


\section{Discussion (movie)}
lore ipsum

\section*{General Discussion}
% Data-driven vs. Analysis-Driven
% "Abfall" einer Film-Messung
% how do results (PPA-like response localization) differ from a standard visual/block-design localizer for:
% feasibility of (multi-dimensional) localizer development with naturalistic stimuli
% Story für parietale Aktivierung im Film -> räuml. Orientierung / Aufmerksamkeit
% CONFOUNDS als klassisches Gegenargument gg. Filme (aber er ändert sich nicht über die Zeit, vgl. resting state)
% was sind für Film und Audiobook die Confounds?
% Occipital Place Area (Nakamura, 2000), RSC
% Natural stimuli like movies \citep{hasson2008neurocinematics, sonkusare2019naturalistic} or narratives \citep{honey2012not, lerner2011topographic, silbert2014coupled} offer an easy to implement, (continuous,) complex, immersive, task-free paradigm that more closely resembles our natural dynamic environment than traditional experimental designs. Compared to specifically designed paradigms, the complexity of a natural stimulus goes hand in hand with an unknown temporal structure unless the stimulus is - often manually - annotated \citep{haeusler2016annotation, labs2015portrayed}.
% Due to their continuity, complexity, and length, they offer a vast amount of data on a variety of brain functions like low-level perception, attention, comprehension, memory, emotions, or social interactions. This suggests the data can later be reused for variety of totally different, independent research questions, possibly on an individual level.
% aminoffr2013; jedoch zu non-spatial stuff: Although the PHC is commonly thought of as responding to visual stimuli, it has also been found to elicit activity for auditory stimuli [25,26,30,31] and to respond to odor stimuli [32–34].

\subsection*{Code availability}
% wo kommt dieser Abschnitt hin?
% how to write Code availability statement
% https://www.springernature.com/gp/authors/research-data-policy/data-availability-statements/12330880
\todo[inline]{provide supporting source code, and explaining how and where others may access all data underlying the analysis}
\emph{for all studies using custom code in the generation or processing of datasets,
a statement must be included here, indicating whether and how the code can be
accessed, including any restrictions to access; include information on the versions
of any software used, if relevant, and any specific variables or parameters used to
generate, test, or process the current dataset. }

\section*{Acknowledgements}
% Text acknowledging non-author contributors. Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.
% Author contributions Please describe briefly the contributions of each author to this work on a separate line.
\emph{COH did this and that.
MH did this and that.
We are grateful to \href{www.florianschurz.de}{Florian Schurz} who initiated doing the annotation of the descriptive nouns, and performed the preliminary annotation of nouns.}

\section*{Competing financial interests}
\emph{A competing financial interests statement is required for all accepted
papers published in \emph{Scientific Data}. If none exist simply write,
``The author(s) declare no competing financial interests''.}

\section*{Figures Legends}
\emph{Figure should be referred to using a consistent numbering scheme through
the entire Data Descriptor. For initial submissions, authors may choose
to supply this document as a single PDF with embedded figures, but
separate figure image files must be provided for revisions and accepted
manuscripts. In most cases, a Data Descriptor should not contain more
than three figures, but more may be allowed when needed. We discourage
the inclusion of figures in the Supplementary Information \textendash{}
all key figures should be included here in the main Figure section.}

\emph{Figure legends begin with a brief title sentence for the whole figure
and continue with a short description of what is shown in each panel,
as well as explaining any symbols used. Legend must total no more
than 350 words, and may contain literature references.}

\section*{Tables}

\emph{Tables supporting the Data Descriptor. These can provide summary information
(sample numbers, demographics, etc.), but they should generally not
be used to present primary data (i.e. measurements). Tables containing
primary data should be submitted to an appropriate data repository.}

\emph{Tables may be provided within the \LaTeX{} document or as separate
files (tab-delimited text or Excel files). Legends, where needed,
should be included here. Generally, a Data Descriptor should have
fewer than ten Tables, but more may be allowed when needed. Tables
may be of any size, but only Tables which fit onto a single printed
page will be included in the PDF version of the article (up to a maximum
of three).}

{\small\bibliographystyle{unsrtnat}
\bibliography{references}}

%\begin{thebibliography}{1}
%\expandafter\ifx\csname url\endcsname\relax
%  \def\url#1{\texttt{#1}}\fi
%\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
%\providecommand{\bibinfo}[2]{#2}
%\providecommand{\eprint}[2][]{\url{#2}}
%
%\bibitem{cite1}
%\bibinfo{author}{Califano, A.}, \bibinfo{author}{Butte, A.~J.},
%  \bibinfo{author}{Friend, S.}, \bibinfo{author}{Ideker, T.} \&
%  \bibinfo{author}{Schadt, E.}
%\newblock \bibinfo{title}{{Leveraging models of cell regulation and GWAS data
%  in integrative network-based association studies}}.
%\newblock \emph{\bibinfo{journal}{Nature Genetics}}
%  \textbf{\bibinfo{volume}{44}}, \bibinfo{pages}{841--847}
%  (\bibinfo{year}{2012}).
%
%\bibitem{cite2}
%\bibinfo{author}{Wang, R.} \emph{et~al.}
%\newblock \bibinfo{title}{{PRIDE Inspector: a tool to visualize and validate MS
%  proteomics data.}}
%\newblock \emph{\bibinfo{journal}{Nature Biotechnology}}
%  \textbf{\bibinfo{volume}{30}}, \bibinfo{pages}{135--137}
%  (\bibinfo{year}{2012}).
%\end{thebibliography}

\section*{Data Citations}

Bibliographic information for the data records described in the manuscript.

1. Lastname1, Initial1., Lastname2, Initial2., ...\& LastnameN, InitialN. \emph{Repository name} Dataset accession number or DOI (YYYY).

\end{document}
\documentclass[english]{article}
