\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{svg}
\usepackage{caption, booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{todonotes}
\usepackage{units}
\usepackage[left=2cm,right=4cm,top=2cm,bottom=2cm]{geometry}
\usepackage[
	colorlinks=true,
	urlcolor=blue,
	linkcolor=green
]{hyperref}
\newcommand{\scidatalogo}{\includegraphics[height=36pt]{SciData_logo.jpg}}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{40pt}
\lhead{\textsc{\scidatalogo}}
\usepackage{natbib}

\begin{document}
\input{descr-stats-ao-events.tex}
\input{descr-stats-av-events.tex}
\input{descr-stats-anno.tex}

% title has to be < 110 chars incl. spaces
%  at the moment 109 Chars
\title{Hemodynamic responses of the anterior "parahippocampal place area" (PPA) to a Hollywood movie's audio-description}

\author{Christian~O.~Häusler\textsuperscript{1,2{*}}, Michael
Hanke\textsuperscript{1,2}}
% https://www.nature.com/sdata/publish/for-authors#other-formats

\maketitle
\thispagestyle{fancy}

1. Psychoinformatics Lab, Institute of Neuroscience and Medicine, Brain \&
Behaviour (INM-7), Research Centre Jülich, Jülich, Germany, 2. Institute of
Systems Neuroscience, Medical Faculty, Heinrich Heine University,  Düsseldorf,
Germany {*}corresponding author: Christian Olaf Häusler (der.haeusler@gmx.net)

\begin{abstract}
% < 170 words new analysis of existing data of interest to a broad section of
% our audience highlighting innovative examples of data reuse may be used to
% present compelling new findings & conclusions derived from published.  intro
Neuroimaging studies have parcellated the human brain into distinct functional
areas.
% PPA
The ``parahippocampal place area'' is classically considered to be a higher
visual area as increased hemodynamic activity in the PPA correlates/is
correlated with the perception of static pictures of landscapes (or landmarks)
compared to pictures of faces or objects.
% What we do
In the current study, we exploratorily tested if spatial information embedded in
the audio-description of a Hollywood movie (i.e. the movie's soundtrack with an
additional narrator) the audio-visual movie can be used to isolate the PPA on a
group average level and in individual subjects.
%
We use publicly available functional magnetic resonance imaging data
(studyforrest.org) to build canonical general linear model (GLM) $t$-contrasts
and compare current results to results that we gained from a dedicated visual
localizer experiment conducted with the same 14 participants.
% compare to visual localizer experiment group results
On a group average level, our results show significantly increased activation in
the PPA but restricted to its anterior part [as well as in the retrosplenial and
lateral occipital cortex] of both hemispheres.
% individual results
On an individual level, results show significant bilateral clusters in the
anterior PPA of nine subjects and significant unilateral clusters in two
subjects.
% conclusion: generalizability
Results suggest that increased activation in the PPA during the perception of
static pictures generalizes to the perception of auditorily presented spatial
information.
% conclusion: PPA has subregions
Lastly, our results provides further evidence that the PPA can be divided into
functional subregions.\todo{kinda bold statements; this depends on the data \&
discussion}\end{abstract}


\todo[inline]{our system cannot accept BibTeX bibliography files; authors who
    wish to use BibTeX to prepare their references should therefore copy the
    reference list from the .bbl file that BibTeX generates and paste it into
    the main manuscript .tex file (and delete the associated
    \textbackslash{}bibliography and \textbackslash{}bibliographystyle
commands)}


\section{Introduction}

\todo[inline]{Limit for main text (intro and discussion) = ca. 3k words;}

\todo[inline]{studies regarding naturalistic stimuli are dropped (Hasson, 2004:
reverse correlation in PPA; Bartels, 2004: GLM, FFA and EBA). That should be okay since we focus on the auditory part which should be clearer now; Hasson /
Bartels might shortly be mentioned in the discussion though}

\todo[inline]{mih: Possibly cite DuPre (2019) and declare that this study is a
    concrete realization; COH: I read it but don't know what/how to cite it for
    any other reason than just to cite it (here or in discussion)}

% brain mapping via fMRI
Humans can perceive and make sense of their daily environment casually and
seemingly without any effort. Cognitive neuroscientists use brain imaging
techniques to map perceptual and cognitive processes (e.g. seeing and
recognizing faces; \citep{kanwisher1997ffa}) to functional brain areas or
networks.
% What is the PPA?
A classic example of a higher-level visual area is the ``parahippocampal place
area'' (PPA) \citep{epstein1998ppa, epstein1999parahippocampal}.
% anatomical location != functional location
The PPA is located in the posterior parahippocampal gyrus including adjacent
regions of the fusiform gyrus and anterior lingual gyrus
\citep{epstein2008parahippocampal}, and thus not anatomically but functionally
defined:
% neural correlate of scene perception
increased hemodynamic activity is observed in the PPA when subjects view
pictures of landscapes, buildings or landmarks (compared to e.g. pictures of
tools or faces) during blood oxygenation level-dependent functional magnetic
resonance imaging (BOLD fMRI) \citep{aguirre1998area, epstein2014neural,
epstein1998ppa}.

\todo[inline]{The following review of O'Craven and Wolbers could be shortened.
Keeping the small paragraph regarding Aziz should make the focus on auditory
perception clearer. What do you think?}
% literature review overview 1: imagination & haptic exploration
Increased hemodynamic activity in the PPA generalizes to mental imagination of
landscapes \citep{ocraven2000mental} and haptic exploration of scenes
constructed from LEGO blocks \citep{wolbers2011modality}.
% O'Craven: watching pictures
In a study conducted by \cite{ocraven2000mental} participants viewed alternating
blocks of pictures showing famous faces and familiar places during an initial
experimental paradigm.
% O'Craven: mental imagery
In a subsequent paradigm, participants were instructed to ``form a vivid mental
image'' of the previously viewed pictures.
% O'Craven results
The PPA showed increased activation during imagination of places compared to
faces but the imagination tasks showed a smaller activation level compared to
the perceptual task.
% Wolbers: haptic exploration
In a block design study conducted by \cite{wolbers2011modality} the PPA of
[sighted] participants showed increased activation during a delayed
match-to-sample task of haptically explored scenes constructed from LEGO bricks
compared to haptically explored abstract geometric objects.
% Wolbers: connectivity analysis
\todo[inline]{Wolbers performed connectivity analysis between PPA and occipital
    cortex, and compares connectivity during visual and haptic paradigm: ``The
    scene-related increase in coupling with the PPA was significantly stronger
    in multiple clusters in occipital cortex under visual than haptic
stimulation (Figure 2B; Table S2)''}

\todo[inline]{following phrasing might be strange but necessary to delineate
from Huth (2016); they do voxel-wise semantic modeling regarding places in PPA
but imo that does not say anything about excitation/inhibition; interestingly
Huth mentions PPA during talks but it is not discussed in any paper and despite
being promised none of his group came up with a paper regarding that topic}

% Aziz (2008): place related sentences
To our knowledge only one study \citep{aziz2008modulation} investigated how
hemodynamic activity in the PPA differs between different semantic stimulus
categories occurring in spoken language.
% Aziz' stimuli
\cite{aziz2008modulation} used sentences that described generic or famous places
(e.g. ``The Taj Mahal faces a long thin reflecting pool''), faces (``Marilyn
Monroe has a large square jaw''), or objects (``The television has a long
antenna'').
% Aziz' tasks
Participants were instructed to press a button whenever the sentence described
an inaccurate or improbable fact.
% Aziz' results
Activation in the left, but not right, PPA was significantly reduced when
participants listened to place-related sentences compared to listening to
face-related sentences. Moreover, this effect was only observed in sentences
involving famous places.
% wtf did Aziz really do?
\todo[inline]{lass uns bitte kurz im Gespräch über Aziz Paper gucken; die
relvanten Abschnitte sind in der PDF markiert. Ihre Method Section ist
``strange'' und im Grunde kommt überall nur uneinheitliche Grütze heraus; ich
blicke nicht durch, weshalb geschriebenes nicht vollständig korrekt sein wird;
so wie ich es verstanden habe, hat sie vier 2x2 ANOVAs den Unterschied zwischen
PPA und FFA angeguckt, aber diesen Unterschied im Kontrast zu object-related
Sentences (dortige Fig 3)}

% summary if literature review
In summary, studies suggest that the PPA does preferably but not exclusively
respond to visually presented spatial information.
% three things in common
Nevertheless, the reviewed studies usually had three things in common.
% designed stimulus set
First, they employed a small sets of carefully chosen stimuli. Further, stimuli
were conceptualized to control confounding variables (e.g. color, luminance,
size, spatial frequencies, sentence length), a process that is often not
reported in detail.\todo{they usually just say ``we made the black \& white}
% block-design & task
Second, studies used block-designed paradigms to maximize detection power.
Further, they usually employed explicit perceptual judgment to keep participants
paying attention to the stimuli.
% averaging
Third, the reviewed studies averaged results across 7-11 subjects to improve
signal-to-noise ratio (SNR).
% disadvantages
Conceptualized stimuli, block-design experiments and the averaging approach have
their merits but also disadvantages.
% designed stimulus set
First, small sets of conceptualized stimuli presented on experimental blocks
poorly resembles our rich and multidimensional environment that our brains are
accommodated to [sonkusare2019naturalistic].
% block-design & task
Second, study participants can and will become aware about which perceptual
process is examined in the study by merely being exposed to the experimental
paradigm.
% inference: validity is poor
Thus, small sets of conceptualized stimuli and block-design experiment lack
external validity \citep{westfall2016fixing} as well as ecological validity
\citep{hasson2004intersubject}.
% disadvantage of averaging
Lastly, averaging data across subjects increases the SNR but does not
characterize brain functions at an individual level, a prerequisite for the
application of brain imaging methods in individual diagnostics (cf.
\cite{dubois2016building, eickhoff2020towards}).

% Open research question
In this study, we investigate if increased activation in the PPA correlates with
incidental ``spatial information'' in the audio-description of the Hollywood
movie ``Forrest Gump'' (i.e. the audio-only variant of the movie) and, for
cross-modal comparison, in the (actual) audio-visual movie (itself).
% we are different
Since we did not design the given naturalistic stimuli (see
\citep{hamilton2018revolution, hasson2008neurocinematics,
sonkusare2019naturalistic} for reviews) for optimal localization performance of
the PPA, we operationalize the perception of landscapes and landmarks (compared
to e.g. faces and objects) differently from the reviewed studies.
% operationalization via annotation
Here, we annotated words spoken by the narrator of the audio-description as well
as cuts and depicted locations in the movie.
% public data
We then analyze publicly available data
(\href{http://www.studyforrest.org}{studyforrest.org}) from the same subjects
that listened to the audio-description and watched the movie during fMRI
scanning.
% first: group level AO
First, general linear model (GLM) contrasts based on the audio-description's
annotation are used to test if we could isolate the PPA with an auditory
stimulus on a group level.
% second: group level AV
Second as conceptual/cross-modal control, we compare results of the
audio-description to results based on the annotation of the movie.
% individual results
Lastly, focussing on the audio-description, we compare results on the level of
individual subjects to results of a block-designed visual localizer experiment
done with the same individuals \citep{sengupta2016extension}.
% results
\todo[inline]{following paragraph will be similar to abstract, intro to
discussion and conclusion}
Our results demonstrate that (a), (b), (c).

\todo[inline]{Bzgl. Open Source der Ergebnisse/Skripte: Dazu habe ich hier
nichts geschrieben; probably something like ``all raw data, scripts, and
results are available under xy as datalad/BIDS dataset''}


\section{Methods}

\todo[inline]{Specific data outputs should be explicitly referenced via data
citation (see Data Records and Data Citations, below)}

\todo[inline]{parts from original studies probably too detailed}

\todo[inline]{current order of subsections might be ``non-canonical''}

% intro
We used data from three publicly available datasets
(\href{http://www.studyforrest.org}{studyforrest.org}) that have already been
used by other research groups for independent research questions.
% used studies
The same subjects were
% AO
a) listening to the audio-description (AO study; \citep{hanke2014audiomovie}) of
the movie ``Forrest Gump'', a dataset already used by \citep{hu2017decoding,
nguyen2016integration},
% AV
b) watching the actual audio-visual movie (AV study;
\citep{hanke2016simultaneous}), a dataset already used by
\citep{ben2018hippocampal},
% VIS
c) participating in a dedicated six-category block-design visual localizer (VIS
study; \citep{sengupta2016extension}), a dataset already used by
\citep{jiahui2019predicting}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications. Following is a summary of the most important aspects.


\subsection{Participants}
% AO study
In the AO study \citep{hanke2014audiomovie}, 20 German native speakers (all
right-handed, age 21–38 years, mean age 26.6 years, 12 male) listened to the
German audio-description \citep{ForrestGumpGermanAD} of the movie ``Forrest
Gump'' \citep{ForrestGumpMovie}.
% AV study
In the AV study \citep{hanke2016simultaneous}, 15 participants (21–39 years,
mean age 29.4, six female) of the prior AO study watched the audio-visual movie
with dubbed German audio track \citep{ForrestGumpDVD}.
% VIS study
In the VIS study \citep{sengupta2016extension}, the same 15 participants took
part in a six-category block-design visual localizer.

% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli}
% AO & AV stimulus name & references
We used the German DVD release \citep{ForrestGumpDVD} of the audio-visual movie
``Forrest Gump'' \citep{ForrestGumpMovie} and its audio-description that was
broadcast as an additional audio track for visually impaired listeners on Swiss
public television \citep{ForrestGumpGermanAD}.
% AV: voice-over
The plot of the movie is already carried by a voice-over of the main character
Forrest Gump.
% AO: additional narrator
In the largely identical audio-description, an additional male narrator
describes essential aspects of the visual scenery when there is no off-screen
voice, dialog, or other relevant auditory content.

% AO & AV: stimulus creation
The audio-description was temporally aligned to the audio track of the German
DVD release. A few scenes less relevant to the major plot were removed to create
the ``research cut'' lasting $\approx$ \unit[2]{h} \citep{hanke2014audiomovie,
hanke2016simultaneous}.
% further processing
Both stimuli were further processed (filtering, volume adjustments) to improve
audibility during MRI scanning. Black horizontal bars at the top and bottom of
the movie were replaced with medium-gray bars of the same size in order to
increase background illumination for a more pleasant experience.
% splitting
Each stimulus was split into eight segments of approximately 15 minutes. Except
for the first segment, each segment started with a snippet of at least six
seconds immediately preceding the movie scene boundary used to split the
segments (see Figure 3a in \citep{hanke2014audiomovie}).

% VIS study picture categories
All stimuli of the VIS study were already used in a previous study
\citep{haxby2011common}. There were 24 unique grayscale images (matched in
luminance, size of 400 $\times$\unit[400]{px}) for each of six stimulus
categories: faces, bodies without heads, small objects, houses and
outdoor scenes comprising of nature and street scenes, and phase scrambled
images. Mirrored views of these 24 $\times$ 6 images were also used as stimuli.


\subsection{Naturalistic stimuli annotation}
% todo stimuli were not designed for research purposes
Both natural stimuli were not design to isolate a specific brain area but to
entertain their audiences free from any perceptual or behavioral task.
% fixed stimuli and operationalization
We annotated the temporal structure of the fixed stimuls material to
operationalize the perception of different stimulus features (e.g. the
perception of landmarks and faces) and create GLM contrasts in the subsequent
data analysis.
% AO annotation
For the analysis of the AO data, we extended an publicly available annotation of
speech \citep{haeusler2020speechanno} by further annotating words spoken by the
audio-description's narrator.
% annotation procedure
Two persons performed a categorization of nouns that the narrator uses to
describe the movie's missing visual content.
% categories in detail
Nouns were categorized by the verbal clue they provide about the cinematographic
scene's environment (\texttt{geo}, \texttt{geo-room}; \texttt{setting\_new},
\texttt{setting\_old}), its inherent persons (e.g. \texttt{female},
\texttt{male}, \texttt{persons}), a person's body or worn clothes
(\texttt{face}, \texttt{head}, \texttt{body}, \texttt{bodypart}), and a scene's
inherent objects (\texttt{object}, \texttt{furniture}).
% why and how of categories 1
Some categories were created to reflect the six stimulus categories used in the
visual localizer experiment (e.g. \texttt{geo}, \texttt{object}).
% why and how of categories 2
Other categories were created to semantically cluster some remaining nouns into
additional categories (e.g. \texttt{fname}, \texttt{furniture})
% problem with hierarchical categories
Notably, the categories \texttt{setting\_new} and \texttt{setting\_rec} comprise
not just words that describe an abstract location (e.g. ) or a setting as whole
(e.g. ``in Greenbow'', ``in a disco'', ``the platoon wades through a rice
field''). They also comprise words that could count as a member of another
categories in case the narrator uses these words to indicate a switch from one
setting to another (e.g. ``Jenny as a teenager'').
% reference to table with rules and examples
A complete overview of all 19 categories, the rule for a word to belong to a
category, and examples can be seen in Table \ref{tab:descr-nouns-rules}.
% procedure
A preliminary annotation was performed by one person according to the rules.
Minor corrections due to incorrectly applied rules were done by the author.
% reference to table with counts
The resulting counts for the whole stimulus and the eight stimulus segments used
during fMRI scanning can be seen in Table \ref{tab:descr-nouns-counts}.

% AV anno
For the analysis of the AV data, we took advantage of a publicly available
annotation of movie cuts and depicted locations \citep{haeusler2016cutanno}.
% rationale behind using movie cuts
Our rationale was that cuts are ``spatially relevant'' events that re-orient the
viewer within the environment depicted in the movie.

\todo[inline]{currently table is to wide; smaller font? less text? rotate the
table?}

% table for descriptive nouns: categories, rules, examples counts
\begin{table*}[t]
    \caption{Descriptive nouns spoken by the audio-description's narrator:
    categories, annotation rules, and examples (given in English). The category
    \texttt{++} also contains adverbial of time.}
\label{tab:descr-nouns-rules}
\begin{tabular}{lll}
\toprule
\textbf{category} & \textbf{rule} & \textbf{examples} \\
\midrule
body & trunk of the body; overlaid clothes & back, hip, shoulder; jacket, dress, shirt \tabularnewline
bodypart & limbs and trousers & arm, finger, leg, toe \tabularnewline
face & face or parts of it & face, ear, nose, mouth \tabularnewline
female & female person & nurse, mother, woman \tabularnewline
females & female persons & women \tabularnewline
fname & female name & Jenny \tabularnewline
furniture & movable furniture insides \& outsides & bench, bed, table, chair
\tabularnewline
geo & immobile landmarks & building, tree, street, alley, meadow, cornfield \tabularnewline
geo-room & rooms (insides) or  locales (outsides) & living room; wall, door, window, floor, turf \tabularnewline
head & non-face parts of the head; worn headgear & head, hair, ear, neck,
helmet \tabularnewline
male & male person & man, father, soldier \tabularnewline
males & male persons & boys, opponents \tabularnewline
mname & male name & Bubba, Kennedy \tabularnewline
object & countable entity with firm boundaries & telephone, car \tabularnewline
objects & countable entities & wheels, plants \tabularnewline
persons & concrete persons of unknown sex & hippies, patients \tabularnewline
setting\_new & noun cueing an unfamiliar setting &  on a ``bridge'', on an ``alley'', on ``campus'' \tabularnewline
setting\_rec & noun cueing a recurring setting & at the ``bus stop'' \tabularnewline
++ & cue regarding time & in the ``evening'', it's ``daytime'', ``later'' \tabularnewline

\bottomrule
\end{tabular}
\end{table*}

% table of descriptive nouns counts
\begin{table*}[t]
    \caption{Descriptive nouns spoken by the audio-descriptions narrator:
        counts for the whole audio-only stimulus and its eight segments used
        during fMRI scanning.}
\label{tab:descr-nouns-counts}
\begin{tabular}{llllllllll}
\toprule
\textbf{category} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & \anBodyAll & \anBodyI & \anBodyII & \anBodyIII & \anBodyIV & \anBodyV & \anBodyVI & \anBodyVII & \anBodyVIII \tabularnewline
bodypart &  \anBodypartAll & \anBodypartI & \anBodypartII & \anBodypartIII & \anBodypartIV & \anBodypartV & \anBodypartVI & \anBodypartVII & \anBodypartVIII \tabularnewline
face & \anFaceAll & \anFaceI & \anFaceII & \anFaceIII & \anFaceIV & \anFaceV & \anFaceVI & \anFaceVII & \anFaceVIII \tabularnewline
female & \anFemaleAll & \anFemaleI & \anFemaleII & \anFemaleIII & \anFemaleIV & \anFemaleV & \anFemaleVI & \anFemaleVII & \anFemaleVIII \tabularnewline
females & \anFemalesAll & \anFemalesI & \anFemalesII & \anFemalesIII & \anFemalesIV & \anFemalesV & \anFemalesVI & \anFemalesVII & \anFemalesVIII \tabularnewline
fname & \anFnameAll & \anFnameI & \anFnameII & \anFnameIII & \anFnameIV & \anFnameV & \anFnameVI & \anFnameVII & \anFnameVIII \tabularnewline
furniture & \anFurnitureAll & \anFurnitureI & \anFurnitureII & \anFurnitureIII & \anFurnitureIV & \anFurnitureV & \anFurnitureVI & \anFurnitureVII & \anFurnitureVIII \tabularnewline
geo & \anGeoAll & \anGeoI & \anGeoII & \anGeoIII & \anGeoIV & \anGeoV & \anGeoVI & \anGeoVII & \anGeoVIII \tabularnewline
geo-room & \anGeoroomAll & \anGeoroomI & \anGeoroomII & \anGeoroomIII & \anGeoroomIV & \anGeoroomV & \anGeoroomVI & \anGeoroomVII & \anGeoroomVIII \tabularnewline
head & \anHeadAll & \anHeadI & \anHeadII & \anHeadIII & \anHeadIV & \anHeadV & \anHeadVI & \anHeadVII & \anHeadVIII \tabularnewline
male & \anMaleAll & \anMaleI & \anMaleII & \anMaleIII & \anMaleIV & \anMaleV & \anMaleVI & \anMaleVII & \anMaleVIII \tabularnewline
males & \anMalesAll & \anMalesI & \anMalesII & \anMalesIII & \anMalesIV & \anMalesV & \anMalesVI & \anMalesVII & \anMalesVIII \tabularnewline
mname & \anMnameAll & \anMnameI & \anMnameII & \anMnameIII & \anMnameIV & \anMnameV & \anMnameVI & \anMnameVII & \anMnameVIII \tabularnewline
object & \anObjectAll & \anObjectI & \anObjectII & \anObjectIII & \anObjectIV & \anObjectV & \anObjectVI & \anObjectVII & \anObjectVIII \tabularnewline
objects & \anObjectsAll & \anObjectsI & \anObjectsII & \anObjectsIII & \anObjectsIV & \anObjectsV & \anObjectsVI & \anObjectsVII & \anObjectsVIII \tabularnewline
persons & \anPersonsAll & \anPersonsI & \anPersonsII & \anPersonsIII & \anPersonsIV & \anPersonsV & \anPersonsVI & \anPersonsVII & \anPersonsVIII \tabularnewline

setting\_new & \anSettingnewAll & \anSettingnewI & \anSettingnewII & \anSettingnewIII & \anSettingnewIV & \anSettingnewV & \anSettingnewVI & \anSettingnewVII & \anSettingnewVIII \tabularnewline

setting\_rec & \anSettingrecAll &
\anSettingrecI & \anSettingrecII & \anSettingrecIII & \anSettingrecIV & \anSettingrecV & \anSettingrecVI & \anSettingrecVII & \anSettingrecVIII \tabularnewline
++ & \anAll & \anI & \anII & \anIII & \anIV & \anV & \anVI & \anVII & \anVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsection{Procedure}
% questionnaire
Participants filled out a questionnaire on their basic demographic information
and familiarity with the movie.
% AO & AV: instructions
Participants were instructed to inhibit physical movements except for
eye-movements, and otherwise to simply ``enjoy the audiobook'' or ``enjoy the
movie'' respectively.
% AO & AV: presentation & instructions
Audio-description and movie segments were presented in chronological order with
four segments in two fMRI sessions. Between sessions, participants left the
scanner for a break with a flexible duration. Structural images were obtained
during the first study on a day different from the fMRI session.

% VIS: presentation & instructions
In the VIS study, participants were presented with four block-design runs, with
two \unit[16]{s} blocks per stimulus category in each run, while they also
performed a one-back matching task to keep them attentive.

\subsection{Stimulation setup}

% paradigm implementation
In all three studies, stimulation was implemented with
\href{http://www.psychopy.org}{PsychoPy} \citep{peirce2007psychopy} running on a
computer with the NeuroDebian operating system \citep{halchenko2012open}.

% AO
In the AO study, visual instructions were presented on a rear-projection screen
inside scanner bore using an LCD projector (DLA-G150CL, JVC Ltd.). During the
functional scans, the projector presented a medium gray screen with the primary
purpose to illuminate a participant's visual field in order to prevent premature
fatigue.

% AV & VIS
In the AV and VIS study, visual instructions and stimuli were presented on a
rear-projection screen using an LCD projector (JVC DLA RS66E, JVC Ltd., light
transmission reduced to \unit[13.7]{\%} with a gray filter) connected to the
stimulus computer.
% screen size
The movie was shown at a viewing distance of \unit[63]{cm} in \unit[720]{p}
resolution at full width on a \unit[1280 $\times$ 1024]{pixel} screen with
\unit[60]{Hz} video refresh rate and screen dimension of \unit[26.5 $\times$
21.2]{cm}.
% angle of view: AV
In the AV study, this corresponded to \unit[23.75 $\times$ 13.5] or \unit[23.75
$\times$ 10.25]{cm} when considering only the movie content and excluding the
horizontal gray bars.
% angle of view: VIS
In the VIS study, stimulus images were displayed at a size of approximately
\unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.

\todo[inline]{auditory stimulation was the same in AO and AV?}

% AO & AV: auditory stimulation
In the AO and AV study, auditory stimulation was delivered through an MR confon
mkII+ driving custom-built in-ear headphones (HP-M01, MR confon GmbH, Magdeburg,
Germany; \citep{baumgart1998electrodynamic}) that reduced the scanner noise by
at least \unit[20–30]{dB}. Headphones were fed from an Aureon 7.1 USB
(Terratec) sound card through an optical connection.


\subsection{fMRI data acquisition}

% AO
In the AO study, a whole-body \unit[7]{Tesla} Siemens MAGNETOM magnetic
resonance scanner equipped with a local circularly polarized head transmit and a
32 channel brain receive coil (Nova Medical, Inc., Wilmington, MA, USA) acquired
T2*-weighted echo-planar images (gradient-echo, \unit[2]{s} repetition time
(TR), \unit[22]{ms} echo time, \unit[0.78]{ms} echo spacing, \unit[1488]{Hz/Px}
bandwidth, generalized autocalibrating partially parallel acquisition,
acceleration factor 3, \unit[2]{Hz/Px} bandwidth in phase encoding direction).
% slices
36 axial slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) with a \unit[10]{\%} inter-slice gap were recorded in ascending
order.
% slice orientation
Slices were oriented to include the ventral portions of frontal and occipital
cortex while minimizing intersection with the eyeballs.
% FOV
The field of view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions [+++Oh, S.
et al. 2012; In, M. \& Speck, O., 2012; Chung, J. et al., 2011+++]
% auxiliary scans
Auxiliary scans for slice alignment and motion- and distortion-correction were
performed at the beginning of the first fMRI recording session and also after
the break at the start of the recording for the second half of the movie.
% AV & VIS
In the AV and VIS study, a whole-body \unit[3]{Tesla} Philips Achieva dStream
MRI scanner equipped with a 32 channel head coil acquired T2*-weighted
echo-planar images (gradient-echo, \unit[2]{s} repetition time, \unit[30]{ms}
echo time, \unit[90]{$^{\circ}$} flip angle, \unit[1943]{Hz/Px} bandwidth,
parallel acquisition with sensitivity encoding (SENSE) reduction factor 2).
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction were recorded in ascending order.
% slice positioning
Philips' ``SmartExam'' was used to automatically position slices in AC-PC
orientation such that the topmost slice was located at the superior edge of the
brain. This automatic slice positioning procedure was identical in the AV and
VIS study, and yielded a congruent geometry across paradigms.

% no. of volumes
A total of 3599 volumes were recorded for each participant in each of the
naturalistic stimulus paradigms (451, 441, 438, 488, 462, 439, 542, and 338
volumes for segment 1–8).


\subsection{Preprocessing}

\todo[inline]{part regarding motion correction/co-registration might be
incorrect}

\todo[inline]{what is missing: As I see it, AO must have been downsampled
to 2.5x2.5x2.5 mm and AV must have been upsampled from 3.0 to 2.5mm??}

% data sources
% https://github.com/psychoinformatics-de/studyforrest-data-aligned/tree/master/code
% https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms}
fMRI time series of those 15 participants in the studyforrest dataset that took
part in all three experiments were obtained from
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{GitHub}.
Data were already corrected for motion and aligned by non-linear warping to a
participant-specific BOLD template image \citep{sengupta2016extension}.
% exclusion of VP 10
Data of one participant were dropped to due to invalid distortion correction
during scanning of the AO stimulus.

% preprocessing intro
All further analysis steps of the current study were carried out using tools of
FSL v5.0.9 (\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library};
\citep{smith2004fsl}) on a computer cluster running the Debian GNU/Linux
operating system. Software packages were obtained from repositories of
\href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.

% my preprocessing
Preprocessing was carried out using FEAT v6.00 (FMRI Expert Analysis Tool;
\citep{woolrich2001autocorr}) as part of FSL.
% temporal filtering
High-pass temporal filtering was applied to every stimulus segment using a
Gaussian-weighted least-squares straight line with a cutoff period of
\unit[150]{s} (sigma=\unit[75.0]{s}) to remove low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model; \citep{woolrich2001autocorr})
to improve estimation efficiency.


\subsection{Statistical analysis}

Similarly to the reviewed studies that employed simplified stimuli, we conducted
a standard two-level general linear model (GLM) analysis to create
subject-specific results ($Z$-maps) across the 8 segments for every subject. A
subsequent, third-level analysis averaged contrast estimates over subjects.

\todo[inline]{better speak of conditions than categories?}

% first level analysis
% regressors
% AO: events
For the analysis of the AO stimulus, we created regressors correlating with the
occurrence of descriptive nouns spoken by the narrator that were added to the
original annotation of speech \citep{haeusler2020speechanno}.
% AV events
For the AV stimulus, we created regressors correlating with movie cuts provided
by another previously published annotation \citep{haeusler2016cutanno}.
% https://github.com/psychoinformatics-de/studyforrest-data-annotations/blob/master/code/researchcut2segments.py}

% procedure
Both annotations were split into eight parts corresponding to the eight stimulus
segments.
% AO events
To create regressors for the AO analysis, we used all of the original 19
categories of descriptive nouns except ``\texttt{++}'' (i.e. cues regarding
time).
% pooling
Some categories that were similar but offered only a small amount of counts were
pooled resulting in eleven new categories that served as experimental conditions
(see Table \ref{tab:ao-events}).
% setting beats other category
Events that fell into one of the two setting-related categories
(\texttt{se\_new} or \texttt{se\_old}) but also into another category were
treated as belonging to the setting-related category (e.g. ``Jenny as a
teenager'').
% modeling as boxcar function
Events were were modeled as boxcar function from onset to offset of each word.
% null regressors
If a segment did not contain an event of a category, a null regressor was chosen
for that category in that segment.\todo{wenn ich "null regressor" nehme, darf
ich den ganzen Run, glaube ich, in der 2nd level Analyse gar nicht benutzen?
Hatte, glaube ich, Jeanette Mumford in einem ihren Videos mal gesagt}
% categories taken from movie cut annotation
Five additional categories were created from the annotation of movie cuts and
modeled as impulse events to capture variance of a possibly confounding change
of the soundscape after a cut, and to create contrasts of cross-modal negative
control (see details below).
% nuisance regressors
Lastly, we used continuous bins of information about two auditory features
(left-right difference in volume and root mean square energy) that was averaged
across the length of every movie frame (\unit[40]{ms}) to capture variance
correlating with low-level perceptual processes.
% Reference to table
An explanation and counts of the eleven speech-related event categories, the
five movie cut-related categories, and the two low-level confounds can be found
in Table \ref{tab:ao-events}.


\begin{table*}[t]
\caption{Overview of events to build the 18 regressors for the analysis of the
audio-only (AO) stimulus. Some of the annotation's original categories
    (female, females, fname; male, males, mname; face, head; object, objects;
    see Table\ref{tab:descr-nouns-rules})
    were pooled into 11 new categories (\texttt{sex\_f}; \texttt{sex\_m};
    \texttt{fahead}; \texttt{object}).
\texttt{fg\_ad\_lrdiff} (left-right volume difference) and
\texttt{fg\_ad\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
For a description of the five control conditions for movie cuts see Table \ref{tab:av-events}.}
\label{tab:ao-events}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & trunk of the body or overlaid clothes & \aoBodyAll & \aoBodyI & \aoBodyII & \aoBodyIII & \aoBodyIV & \aoBodyV & \aoBodyVI & \aoBodyVII & \aoBodyVIII \tabularnewline
bpart & limbs and trousers & \aoBpartAll & \aoBpartI & \aoBpartII & \aoBpartIII & \aoBpartIV & \aoBpartV & \aoBpartVI & \aoBpartVII & \aoBpartVIII \tabularnewline
fahead & face(parts) or head(parts) & \aoFaheadAll & \aoFaheadI & \aoFaheadII & \aoFaheadIII & \aoFaheadIV & \aoFaheadV & \aoFaheadVI & \aoFaheadVII & \aoFaheadVIII \tabularnewline
furn & moveable furniture insides \& outsides & \aoFurnAll & \aoFurnI & \aoFurnII & \aoFurnIII & \aoFurnIV & \aoFurnV & \aoFurnVI & \aoFurnVII & \aoFurnVIII \tabularnewline
geo & immobile landmarks & \aoGeoAll & \aoGeoI & \aoGeoII & \aoGeoIII & \aoGeoIV & \aoGeoV & \aoGeoVI & \aoGeoVII & \aoGeoVIII \tabularnewline
groom & elements defining geometry of room & \aoGroomAll & \aoGroomI & \aoGroomII & \aoGroomIII & \aoGroomIV & \aoGroomV & \aoGroomVI & \aoGroomVII & \aoGroomVIII \tabularnewline
object & inanimate entities with firm boundaries & \aoObjAll & \aoObjI & \aoObjII & \aoObjIII & \aoObjIV & \aoObjV & \aoObjVI & \aoObjVII & \aoObjVIII \tabularnewline
se\_new & noun cueing an unfamiliar setting & \aoSenewAll & \aoSenewI & \aoSenewII & \aoSenewIII & \aoSenewIV & \aoSenewV & \aoSenewVI & \aoSenewVII & \aoSenewVIII \tabularnewline
se\_old & noun cueing a recurring setting & \aoSeoldAll & \aoSeoldI & \aoSeoldII & \aoSeoldIII & \aoSeoldIV & \aoSeoldV & \aoSeoldVI & \aoSeoldVII & \aoSeoldVIII \tabularnewline
sex\_f & female person(s), name & \aoSexfAll & \aoSexfI & \aoSexfII & \aoSexfIII & \aoSexfIV & \aoSexfV & \aoSexfVI & \aoSexfVII & \aoSexfVIII \tabularnewline
sex\_m & male person(s), name & \aoSexmAll & \aoSexmI & \aoSexmII & \aoSexmIII & \aoSexmIV & \aoSexmV & \aoSexmVI & \aoSexmVII & \aoSexmVIII \tabularnewline
% vlo_ch has no events in segment 5. So indices are shifted
vse\_new & control for movie cut & \aoVsenewAll & \aoVsenewI & \aoVsenewII & \aoVsenewIII & \aoVsenewIV & \aoVsenewV & \aoVsenewVI & \aoVsenewVII & \aoVsenewVIII \tabularnewline
vse\_old & control for movie cut & \aoVseoldAll & \aoVseoldI & \aoVseoldII & \aoVseoldIII & \aoVseoldIV & \aoVseoldV & \aoVseoldVI & \aoVseoldVII & \aoVseoldVIII \tabularnewline
vlo\_ch & control for movie cut & \aoVlochAll & \aoVlochI & \aoVlochII & \aoVlochIII & \aoVlochIV & 0 & \aoVlochV & \aoVlochVI & \aoVlochVII \tabularnewline
vpe\_new & control for movie cut & \aoVpenewAll & \aoVpenewI & \aoVpenewII & \aoVpenewIII & \aoVpenewIV & \aoVpenewV & \aoVpenewVI & \aoVpenewVII & \aoVpenewVIII \tabularnewline
% vpe_old has no events in 3. So the indices are shifted
vpe\_old & control for movie cut & \aoVpeoldAll & \aoVpeoldI & \aoVpeoldII & 0 & \aoVpeoldIII & \aoVpeoldIV & \aoVpeoldV & \aoVpeoldVI & \aoVpeoldVII \tabularnewline
fg\_ad\_lrdiff & left-right volume difference & \aoFgadlrdiffAll & \aoFgadlrdiffI & \aoFgadlrdiffII & \aoFgadlrdiffIII & \aoFgadlrdiffIV & \aoFgadlrdiffV & \aoFgadlrdiffVI & \aoFgadlrdiffVII & \aoFgadlrdiffVIII \tabularnewline
fg\_ad\_rms & root mean square (loudness) & \aoFgadrmsAll & \aoFgadrmsI & \aoFgadrmsII & \aoFgadrmsIII & \aoFgadrmsIV & \aoFgadrmsV & \aoFgadrmsVI & \aoFgadrmsVII & \aoFgadrmsVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% AV regressors/events
To create regressors for the AV analysis, we categorized the depicted location
in the frame following one of the 869 cuts into five categories (see Table
\ref{tab:av-events}): 1) a cut into an unfamiliar setting (\texttt{vse\_new}),
2) a cut into a familiar setting (\texttt{vse\_old}), 3) a switch of room/locale
within a setting (\texttt{vlo\_ch}), 4) a cut to an unfamiliar perspective
(\texttt{vpe\_new}), and 5) a cut to a familiar perspective (\texttt{vpe\_old}).
% null regressor
Here again, a null regressor was chosen for segment if it did not contain an
event of specific category.
% no cut condition: intro
A sixth regressors (\texttt{no\_cut}) serving a control condition and to create
negative control contrasts. Frames within movie shots lasting at least
\unit[20]{s} were pseudo-randomly chosen by the following rationale:
% how they were fitted
a \texttt{no\_cut} event had to have a minimum distance of \unit[10]{s} to a
movie cut and to another \texttt{no\_cut} event.
% reference to table
A short explanation and counts of these six categories modeled as impulse
events, and six additional event categories for low-level auditory and visual
features of the movie (here again continuous bins lasting \unit[40]{ms}) can be
found in Table \ref{tab:av-events}.
% modeling of regressors from events
Events of both stimuli were convolved with a double gamma hemodynamic response
function (HRF).

\begin{table*}[t]
    \caption{Overview of events to build the 14 regressors for the analysis of the audio-only (AV) stimulus.
\texttt{fg\_av\_ger\_lr} (left-right difference in [+++???+++] of
consecutive movie frames).\texttt{fg\_av\_ger\_pd} (perceptual differences of
consecutive frames).
\texttt{fg\_av\_ger\_ud} (up-down difference in [+++???+++] of consecutive movie frames). \texttt{fg\_av\_ger\_lrdiff} (left-right volume difference) and \texttt{fg\_av\_ger\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
For description of the two event categories of the audio-description's
narrator (\texttt{se\_new} and \texttt{se\_old}) to build contrast
of negative control see Table \ref{tab:ao-events}.}
\label{tab:av-events}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
vse\_new &  change of the camera position to a setting not depicted before & \aoVsenewAll & \aoVsenewI & \aoVsenewII & \aoVsenewIII & \aoVsenewIV & \aoVsenewV & \aoVsenewVI & \aoVsenewVII & \aoVsenewVIII \tabularnewline
vse\_old & change of the camera position to a recurring setting & \aoVseoldAll & \aoVseoldI & \aoVseoldII & \aoVseoldIII & \aoVseoldIV & \aoVseoldV & \aoVseoldVI & \aoVseoldVII & \aoVseoldVIII \tabularnewline
vlo\_ch & change of the camera position to another locale within the same setting & \aoVlochAll & \aoVlochI & \aoVlochII & \aoVlochIII & \aoVlochIV & 0 & \aoVlochV & \aoVlochVI & \aoVlochVII \tabularnewline
vpe\_new & change of the camera position within a locale not depicted before & \aoVpenewAll & \aoVpenewI & \aoVpenewII & \aoVpenewIII & \aoVpenewIV & \aoVpenewV & \aoVpenewVI & \aoVpenewVII & \aoVpenewVIII \tabularnewline
% vpe_old has no events in 3. So the indices are shifted
vpe\_old & change of the camera position within a recurring locale & \aoVpeoldAll & \aoVpeoldI & \aoVpeoldII & 0 & \aoVpeoldIII & \aoVpeoldIV & \aoVpeoldV & \aoVpeoldVI & \aoVpeoldVII \tabularnewline
vno\_cut & a continuous frame from the same camera position & \avVnocutAll & \avVnocutI & \avVnocutII & 0 & \avVnocutIII & \avVnocutIV & \avVnocutV & \avVnocutVI & \avVnocutVII \tabularnewline
se\_new & control for AO narrator & \aoSenewAll & \aoSenewI & \aoSenewII & \aoSenewIII & \aoSenewIV & \aoSenewV & \aoSenewVI & \aoSenewVII & \aoSenewVIII \tabularnewline
se\_old & control for AO narrator & \aoSeoldAll & \aoSeoldI & \aoSeoldII & \aoSeoldIII & \aoSeoldIV & \aoSeoldV & \aoSeoldVI & \aoSeoldVII & \aoSeoldVIII \tabularnewline
fg\_av\_ger\_lr & XYZ & \avFgavgerlrAll & \avFgavgerlrI & \avFgavgerlrII & \avFgavgerlrIII & \avFgavgerlrIV & \avFgavgerlrV & \avFgavgerlrVI & \avFgavgerlrVII & \avFgavgerlrVIII \tabularnewline
fg\_av\_ger\_lrdiff & left-right volume difference & \avFgavgerlrdiffAll & \avFgavgerlrdiffI & \avFgavgerlrdiffII & \avFgavgerlrdiffIII & \avFgavgerlrdiffIV & \avFgavgerlrdiffV & \avFgavgerlrdiffVI & \avFgavgerlrdiffVII & \avFgavgerlrdiffVIII \tabularnewline
fg\_av\_ger\_ml & +++++???+++++ & \avFgavgermlAll & \avFgavgermlI & \avFgavgermlII & \avFgavgermlIII & \avFgavgermlIV & \avFgavgermlV & \avFgavgermlVI & \avFgavgermlVII & \avFgavgermlVIII \tabularnewline
fg\_av\_ger\_pd & +++++???+++++ & \avFgavgerpdAll & \avFgavgerpdI & \avFgavgerpdII & \avFgavgerpdIII & \avFgavgerpdIV & \avFgavgerpdV & \avFgavgerpdVI & \avFgavgerpdVII & \avFgavgerpdVIII \tabularnewline
fg\_av\_ger\_rms & root mean square (loudness) & \avFgavgerrmsAll & \avFgavgerrmsI & \avFgavgerrmsII & \avFgavgerrmsIII & \avFgavgerrmsIV & \avFgavgerrmsV & \avFgavgerrmsVI & \avFgavgerrmsVII & \avFgavgerrmsVIII \tabularnewline
fg\_av\_ger\_ud & +++++???+++++ & \avFgavgerudAll & \avFgavgerudI & \avFgavgerudII & \avFgavgerudIII & \avFgavgerudIV & \avFgavgerudV & \avFgavgerudVI & \avFgavgerudVII & \avFgavgerudVIII \tabularnewline
\end{tabular}
\end{table*}


% reference to figure ``correlation of regressors''
We computed the Pearson coefficients of the regressors across the eight segments
of both naturalistic stimuli. Results demonstrate no to minor correlations (s.
Figure \ref{fig:reg-corr}) which suggest the models should not suffer from low
detection power due to shared variance among regressors.
%
\todo[inline]{sounds meh but don't know how do write it better; results of
    correlations are given here already; maybe the short discussion (cf. very
    beginning of discussion) of these correlation could be put here too which
    will saves space in the main text; but mixing methods/results/discussion is
not absolutely "clean"}

% temporal derivatives
Temporal derivatives were included in the design matrix to compensate for
regional differences between modeled and actual HRF \citep{friston1998event}.
% motion parameters
Six motion parameters were also used as additional nuisance regressors.
% high-pass filtering
Finally, designs were filtered with the same high-pass filter (cutoff period of
\unit[150]{s}) as the BOLD time series.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/regressor-corr}
    \caption{Pearson
        correlation coefficients of the regressors used in the analysis of the
        audio-description (blue; see Table \ref{tab:ao-events} for a
        description) and audio-visual movie (red; see Table
        \ref{tab:av-events}).}
    \label{fig:reg-corr}
\end{figure*}

\todo[inline]{MH: construction of contrasts of interests feels "messy";
     consequence of the inability to filter the stimulus material. That "messy"
     part is usually not obvious from a manuscript. Here we have a fixed,
     complex stimulus, and we have to operationalize our questions with whatever
     we have at hand. This is not necessarily exact, and looking at the
 robustness is a feature, not a problem; COH: did not change a lot of text tbh
 but should be clearer now.}

% we have fixed stimuli
Compared to previous studies we did not design the stimulus material and
experimentally controlled conditions for the
purpose of optimal localization performance since we used fixed stimuli.
% 8 + 5 contrasts
We created eight $t$-contrasts for the AO stimulus (see Table
\ref{tab:ao-contrasts}) and five $t$-contrasts for the AV stimulus (see Table
\ref{tab:av-contrasts}) to operationalize ``spatial perception'' compared to
``non-spatial'' perception.\todo{big ouff\dots}
% test for robustness
Creating more than just one contrast for the two
fixed stimuli also let us test for/the robustness of our results across varying
ways operationalization.
% contrasts are of different quality
We assumed all contrasts could in principle isolate the PPA. Nevertheless, we
also assumed contrasts would differ in their isolation performance because they
differ in how well the included event categories reflect on average the
preferred stimulus type processed in the PPA.
% selection of the primary contrast
Thus for each stimulus, we chose one ``primary contrast'' for a detailed report
and discussion of the group-level and subject-specific results. Primary
contrasts were chosen based on how well the used categories on average were
assumed to match the preferred stimulus type in balance with the amount of
available data / events.

% AO COPEs: intro
In context of the AO stimulus, the categories \texttt{se\_new} and
\texttt{se\_old} were problematic for three reasons and thus are not included in
the primary contrast:
% sparse auditory, rich visually
(a) they often only vaguely describe a scene (e.g. ``in Greenbow'', ``in a
disco'', ''black and white film recordings'') whereas a picture of the verbally
described scene would be perceptually richer,
% hierarchical categories
(b) they also comprise words that could count as a member of another category in
case the narrator uses these words to indicate a switch from one setting to
another (e.g. ``Jenny as a teenager'')
% soundscapes forestalls narrator
(c) the narrator is often lacking behind a short dialogue or a change of the
soundscape that already cue a change of settings.  cuts but in AO stimulus
% conclusion
Hence, we assumed contrasts that (exclusively) used these two categories would
induce noise and would be less successful in isolating the PPA.
% furniture
We did not use the category ``furniture'' (\texttt{furn}) as a
regressor of interest in any contrasts because nouns in this category were
usually big and heavy. In a serial stream of auditory information, participants
could perceive furniture-related nouns as either a geometry-defining local scene
element or as an isolated object. Hence we did not use this category as
regressor of interest to reduce noise.
% negative control contrasts
Five $t$-contrasts (see Table \ref{tab:ao-contrasts}) we created as negative
controls to test if increased activation in the PPA was correlated with
moments of cuts and therefore possibly correlated with a accompanying change in
the soundscape.


\begin{table*}[t]
\caption{Contrasts of parameter estimates for the analysis of the audio-only
    stimulus (AO). Contrast 1-8 aim to isolate the PPA (\texttt{all non-geo} =
    body, bodypart, fahead, object, sex\_f, sex\_m).
    Contrasts 9-13 serve as negative control at the moments of movie cuts that
    could correlate with changes of the audio tracks soundscape (see Table
    \ref{tab:av-contrasts}).}
\label{tab:ao-contrasts}
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{nr.} &  \textbf{contrast} & \textbf{purpose} \\
\midrule
1 & geo, groom > all non-geo & PPA \tabularnewline
2 & geo, groom, se\_new > all non-geo & PPA \tabularnewline
3 & groom, se\_new, se\_old > all non-geo & PPA \tabularnewline
4 & geo > all non-geo & PPA \tabularnewline
5 & groom > all non-geo & PPA \tabularnewline
6 & se\_new > all non-geo & PPA \tabularnewline
7 & se\_new, se\_old > all non-geo & PPA \tabularnewline
8 & se\_new > se\_old & PPA \tabularnewline
9 & vse\_new > vpe\_old & control at moments of cuts \tabularnewline
10 & vse\_new, vpe\_new > vse\_old, vpe\_old & control at moments of cuts \tabularnewline
11 & vse\_new > vse\_old & control at moments of cuts \tabularnewline
12 & vse\_new > vse\_old, vpe\_old & control at moments of cuts \tabularnewline
13 & vse\_new, vpe\_new > vpe\_old & control at moments of cuts \tabularnewline
\end{tabular}
\end{table*}


\todo[inline]{MH: I think it's better to explain the rationale of the regressors
here in the context of the contrasts and not already above in the list of
created regressors: COH: yes, I checked that. Above is just a short
list/explanation except for ``no cut'' condition. imo it makes more sense to
explain it there but maybe I cannot get the reason why you think it fits here
better}

% AV COPEs: intro
In context of the AV stimulus, we categorized the frames following a movie cut
irrespective of their concrete visual content and thus without controlling
confounding variables (e.g. presence of faces). The rationale that this
procedure would still be able to isolate the PPA was twofold.
% cinematographic rationale cut to new scenes: establishing shots
The cinematographic rationale was that movies tend to establish the setting
(\texttt{vse\_new}) and the spatial relationships within a setting
(\texttt{vpe\_new}) at the beginning of a movie.
% cut to recurrent scene
Later in the movie, the field sizes of movie shots tend to decrease when
switching back to already established settings (\texttt{vse\_old}).
% similarly for shots within a scene
Moreover, shots within a recurring locale or setting tend to shift to depicting
persons and objects (\texttt{vpe\_old}) more relevant to the evolved plot
\citep{brown2012cinematography, mercado2011filmmakers}.
% neuroscientific rationale
The neuroscientific rationale was simply that activity in the PPA is greater
when participants view novel versus repeated scenes or view-points.
\citep{epstein1999parahippocampal}.
% hence, the following contrasts
Hence, we created varying contrasts comparing a setting occurring for the first
time and cuts within a locale depicted for the first time to cuts into recurring
settings and cuts within a recurring locale.
% negative control
Five additional $t$-contrasts we created for negative control purposes
(see Table \ref{tab:av-contrasts}) by using regressors based on regressors of
the AO stimulus.


\begin{table*}[t]
    \caption{Contrasts of parameter estimates for the analysis of the audio-visual movie (AV).
Contrast 1-5 aim to isolate the PPA.
    Contrast 6-9 serve as negative control.
    Contrast 10 serves as control for moments when the narrator cues the
    listener about a switch of settings, events that could be correlated with a
    switch of the soundscape.}
\label{tab:av-contrasts}
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{nr.} &  \textbf{contrast} & \textbf{purpose} \\
\midrule
1 & vse\_new > vpe\_old & PPA \tabularnewline
2 & vse\_new, vpe\_new > vse\_old, vpe\_old & PPA \tabularnewline
3 & vse\_new > vse\_old & PPA \tabularnewline
4 & vse\_new > vse\_old, vpe\_old & PPA \tabularnewline
5 & vse\_new, vpe\_new > vpe\_old & PPA \tabularnewline
6 & vno\_cut > vse\_new & negative control \tabularnewline
7 & vno\_cut > vse\_old & negative control \tabularnewline
8 & vno\_cut > vse\_new, vse\_old & negative control \tabularnewline
9 & vno\_cut > vpe\_new, vpe\_old & negative control \tabularnewline
10 & se\_new > se\_old & control for narrator \tabularnewline
\end{tabular}
\end{table*}

% alignments
% single subjects (for Bland-Altman-Plots) 1st level
The GLM analysis that fitted each voxel's time course separately for each
subject and segment was performed in functional space preserving the orientation
of the EPI images.
% two ways of co-registration
We co-registered the subjects in two ways.
% subject template (for Bland-Altman-Plot of unthresholded maps)
% \href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/sub-01/bold3Tp2/brain.nii.gz"}{example})
On the one hand, subject-specific BOLD T2* time series were anatomically aligned
via linear transformation to a subject-specific, study-specific T2* template
(cf. \citep{sengupta2016extension}).
% group template (for group and individual brain slices)
% https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/templates/grpbold3Tp2/brain.nii.gz"
On the other hand, time series were aligned via non-linear transformation to a
study-specific T2* group template (cf. \citep{hanke2014audiomovie}).
% MNI152
This group template was co-registered to the MNI 152 template with an affine
transformation (12 degrees of freedom) to facilitate visualization and
identification of cortical structures.

% second level
% model
The second level analyses which averaged contrast estimates over the eight
stimulus segments were carried out using a fixed effects model by forcing the
random effects variance to zero in FLAME (FMRIB's Local Analysis of Mixed
Effects; \citep{beckmann2003general, woolrich2004multilevel}.
% thresholding (Z>2.3 in subject-space; Z>3.4 in group space)
(Gaussianised T/F) statistic images were thresholded using clusters determined
by Z>3.4 and a (corrected) cluster significance threshold of p=.05
\citep{worsley2001statistical}.

% third level model
The third level analysis which averaged contrast estimates over subjects was
carried out using FLAME stage 1 with automatic outlier detection
\citep{beckmann2003general, woolrich2004multilevel, woolrich2008robust}.
% thresholding
Here again, Z (Gaussianised T/F) statistic images were thresholded using
clusters determined by Z>3.4 and a (corrected) cluster significance threshold of
p=.05 \citep{worsley2001statistical}.
% brain region identification
Brain regions associated with observed clusters were determined with the Jülich
Histological Atlas \citep{eickhoff2005toolbox, eickhoff2007assignment} and the
Harvard-Oxford Cortical Atlas \citep{desikan2006automated} provided by FSL.
% PPA masks
PPA region of interest (ROI) masks for individual subjects and a PPA group mask
of individual PPA overlaps were created from data provided by
\citep{sengupta2016extension}.


\section{Results}

% study goal
The goal of this study was to investigate if increased hemodynamic activity in
the PPA correlates with spatial information presented under more ecological
valid conditions.\todo{cautious phrasing but still problematic: looking for
confirmation}
% what we did
We annotated a \unit[2]{h} movie and its temporally aligned audio-description to
create GLM contrasts similar to previous studies that used simplified stimuli.
% group level
On a group average level, we compare results gained from both naturalistic
stimuli to results gained from a visual localizer experiment.
% individual level
On the level single subjects, we also assess localization performance of a
``visual area'' using the auditory stimulus.

% all AO contrasts
% 1 PPA (l/r); RSC (l/r), LOC (l/r)
% 2 PPA (l/r); RSC (l/r), LOC (l/r), r. sup.temp.C., r.fr.pole; putamen; etc
% 3 PPA (l/r); RSC (l/r), LOC (l/r),l&r sup.temp.C., anterior cing., paracing.
% 4 PPA (l/r); RSC (l/r), LOC (l/r)
% 5 PPA (l/r); RSC (l/r), LOC (-/-)
% 6 PPA (l/r); RSC (l/r), LOC (l/-), l&r sup.temp.C.; medial pref. c., precun
% 7 PPA (-/-); RSC (l/r), LOC (l/-), l&r sup.temp.C
% 8 PPA (-/-); RSC (-/-), LOC (-/-)
% ---------- control contrasts ----------
% 9 PPA (-/-); RSC (-/r), LOC (-/-), left pars triangularis
%10 PPA (-/-); RSC (-/-), LOC (-/-)
%11 PPA (-/-); RSC (-/-), LOC (-/-),
%12 PPA (-/-); RSC (-/-), LOC (-/-), right posterior hippocampus
%13 PPA (-/-); RSC (-/-), LOC (-/-) results: PPA

% all AV contrasts
% 1 PPA (l/r); RSC (l/r), LOC (l/r), VisC (l/r),
% intracalcarine, cuneal, ling.c
% 2 PPA (l/r); RSC (-/-), LOC (l/r), VisC (-/-)
% 3 PPA (l/r); RSC (-/-), LOC (-/-), VisC (-/r), occ. fusisf. g.
% 4 PPA (l/r); RSC (l/r), LOC (l/r), VisC (-/-), lingual g., occ. fusif. g.
% 5 PPA (l/r); RSC (l/r), LOC (l/r), VisC (-/-), occ. fusif. g.
% ---------- control contrasts ----------
% 6 nothing
% 7 nothing
% 8 nothing
% 9 nothing
%10 se_new > se_old (nouns but in AV stimulus):
% LOC (l/r) right PPA, bilateral sup. lat. occ. c., sup. parietal lobe (right)

\subsection{Group results (all AO + AV PPA contrasts)}

\todo[inline]{subheadings are still in for better readability; structure could
be changed; at the moment its AO (PPA in primary contrast, robustness of PPA;
other clusters across contrasts), then the same for AV}

\todo[inline]{paragraphs start with 1-2 sentences that set up the question
that the paragraph answers; e.g. ``To verify that\dots''}

\todo[inline]{middle of paragraph presents data \& logic that pertain the
question}

\todo[inline]{paragraph ends with a concluding / logical statement}


\subsubsection{PPA in AO: primary contrasts}

% intro
First on a group average level, we tested if we could isolate the PPA using GLM
contrasts based on the audio-description's annotation of speech.
% primary AO contrast
The primary $t$-contrast for the audio-description (\texttt{geo, groom > all
non-geo}) yielded six significant clusters (see Table \ref{tab:res-ao-group1},
Figure \ref{fig:group-slices}).
% overlap with ROI
Two homologous clusters are located in the anterior part of the PPA group
overlap that we gained from the visual localizer experiment.
% in detail
specifically, the clusters of the current analysis are at the borders of the
posterior parahippocampal cortex, the occipital and temporal fusiform gyrus and
lingual gyrus.
% precuneus
Two additional clusters are located in the ventral precuneus extending into the
retrosplenial cortex.
% LOC
Finally, two homologous clusters are located in the superior lateral occipital
cortex.
% concluding statement
\todo[inline]{any concluding statement here or merely report the clusters?}

% table group results primary AO contrast
\begin{table*}[t]
    \caption{Significant clusters (z-threshold Z>3.4; p<.05 cluster-corrected)
        of the primary $t$-contrast for the audio-description comparing
        geometry-related nouns to non geometry-related nouns spoken by the
        audio-description's narrator (\texttt{geo, groom > all non-geo}).
        Clusters sorted by voxel size.
    The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
    \label{tab:res-ao-group1}
\begin{tabular}{rrrrrrrrrp{3cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
188 & 5.96E-08 & 4.48 & -17.5 & -65.5 & 25.5 & -14.7 & -59.1 & 15.2 & l. precuneus \\
164 & 2.38E-07 & 4.47 & 17.5 & -58 & 23 & 15.6 & -55.6 & 16 & r. precuneus; \\
83 & 0.000128 & 4.48 & 27.5 & -43 & -17 & 27.2 & -41.1 & -14 & r. temporal (occipital) fusiform c.; posterior parahippocampal g. \\
73 & 0.000318 & 3.93 & -22.5 & -43 & -12 & -23.9 & -43.6 & -11.2 & l. lingual g.; temporal (occipital) fusiform g., posterior parahippocampal c. \\
63 & 0.000824 & 4.1 & 40 & -75.5 & 30.5 & 40.9 & -76.3 & 28.6 & r. superior lateral occipital c. \\
37 & 0.0129 & 4.24 & -37.5 & -78 & 33 & -38.4 & -79.5 & 28.9 & l. superior lateral occipital c. \\
\bottomrule
\end{tabular}
\end{table*}

% figure group results primary AO & AV contrasts
\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/group-slices}
    \caption{Mixed-effects group-level (N=14) GLM results. Significant clusters
        (Z>3.4, p<.05 cluster-corrected) are overlaid on the MNI152 T1-weighted
        head template (gray).
        Light gray: The audio-description's field-of-view
        (cf. \citep{hanke2014audiomovie}).
        Black: outline of overlapping individual PPA ROIs.
        The results of audio-description's primary $t$-contrast (blue) that
        compares geometry-related nouns to non geometry-related nouns spoken by
        the audio-description's narrator (\texttt{geo, groom > all non-geo})
        are overlaid over the movie's primary $t$-contrast (blue) that compares
        cuts to a new setting with cuts to a familiar perspective
        (\texttt{vse\_new > vpe\_old})
   \citep{sengupta2016extension}.}
    \label{fig:group-slices}
\end{figure*}

\subsubsection{PPA in AO: robustness across contrasts}

\todo[inline]{I consider tables of significant clusters in PPA for all contrasts
as overkill. Maybe in ``supporting material'' or reference to dataset? all sign.
clusters of all contrasts are in the comments}

% all eight AO PPA contrasts
In order to test the robustness of our findings, we created seven additional
$t$-contrasts for the AO stimulus (see Table \ref{tab:ao-contrasts}).
% results
Six of the overall eight AO contrasts show significant bilateral clusters in
anterior regions of the group PPA overlap (see Figure
% statement/answer
Results suggest that our findings do not depend on one specific contrasts but
are robust across differently designed
contrasts.\ref{fig:stability-slices}).\todo[inline]{Discussion: discuss that
contrasts make ``less and less'' sense}
% question
\todo[inline]{concluding statement often feels like interpretation/discussion}
% question 2



% figure group results of robustness across all AO & AV contrasts
\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/stability-slices}
    \caption{Stability of contrasts. Overlap of significant clusters (Z>3.4;
        p<.05, cluster corrected) of contrasts created to isolate the PPA.
        The audio-description's contrasts 1-8 (blue; \ref{tab:ao-contrasts})
        are overlaid over the audio-visual movie's contrasts 1-5 (red;
        \ref{tab:av-contrasts}), both on top of the MNI152 T1-weighted head
        template (gray).
        Black: outline of overlapping individual PPA ROIs.
        Light gray: The audio-description's field-of-view (cf.
        \citep{hanke2014audiomovie}).}
    \label{fig:stability-slices}
    \end{figure*}


\subsubsection{other sign. clusters across AO contrasts}
% precuneus = retrosplenial cortex ?
\todo[inline]{significant clusters in other regions than PPA could or should (?)
also be reported? Following is a short list; depends on the discussion I guess}
% ventral precuneus = retrosplenial cortex?
7 contrasts: bilateral clusters in RSC (and ventral precuneus);
% transverse occipital sulcus = occipital place area
6 contrasts (4 bilat.): transverse occipital sulcus (= OPA?);\todo{AO: clusters
are more lateral; AV: clusters are more medial}
% confounds
4 contrasts: right superior temporal gyrus; 3 contrasts: left superior temporal
gyrus; 3 contrasts: some (smallish) prefrontal clusters


\subsubsection{PPA in AV: primary contrasts}
% AV intro
Second as conceptual/cross-modal control of the group average results, we
analyzed data from the audio-visual movie by contrasting different kinds of
movie cuts independent(ly) from the frames' visual content following the cuts.
% AV primary contrast
The movie's primary $t$-contrast that compared cuts to an unfamiliar setting
with cuts within familiar setting (\texttt{vse\_new > vpe\_old}) yielded three
significant clusters (see Table \ref{tab:res-av-group1}, Figure
\ref{fig:group-slices}).
% PPA (others the clusters extend into).
One cluster spans across the midline and comprises (reported from posterior to
anterior), parts of the intracalcarine and cuneal cortex, the lingual gyrus and
retrosplenial cortex, the occipital and temporal fusiform gyrus and the
parahippocampal cortex in both hemispheres.
% LOC
Two additional homologous clusters are located in the superior lateral occipital
cortex (left bigger than right).


\begin{table*}[t]
    \caption{Significant clusters (z-threshold Z>3.4; p<.05 cluster-corrected)
        of the primary $t$-contrast for the movie comparing cuts to a new
        setting with cuts to a familiar perspective (\texttt{vse\_new >
        vpe\_old}).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:res-av-group1}
\begin{tabular}{rrrrrrrrrp{3cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
3003 & 0 & 5.31 & 22.5 & -45.5 & -12 & 4.53 & -63.3 & -3.72 & r. lingual g.; r. cuneal c., intracalcarine c., bilaterally occipital fusiform g., temporal fusiform c., posterior parahippocampal c.  \\
154 & 6.56E-07 & 4.46 & -35 & -83 & 28 & -32.8 & -86.2 & 21.4 & l. superior lateral occipital c. \\
121 & 7.69E-06 & 4.65 & 25 & -80.5 & 25.5 & 23.7 & -83.8 & 25.4 & r. superior lateral occipital cortex \\
\bottomrule
\end{tabular}
\end{table*}

\subsubsection{PPA in AV: robustness + other clusters across contrasts}

% all eight AV PPA contrasts
Here again in order to test the robustness of our findings, we created four
additional $t$-contrasts for the AV stimulus (see Table \ref{tab:av-contrasts}).
%% results: PPA
All of the overall five AV contrasts yielded significant bilateral clusters (in
that overlap with the PPA group mask/overlap [biggest overlap in posterior part
of group ROI = temporal occipital fusiform cortex] (see Figure
\ref{fig:stability-slices}).
% concluding statement
Here again, we can show that results can in principle be accomplished by varying
the design of the contrasts.
% question
\todo[inline]{concluding statement = interpretation/discussion?}


\subsection{negative controls in AO and AV}

\todo[inline]{contrasts are mentioned in the method section for the purpose of
    completeness. Shall results be reported (and later discussed)? Following is
a short overview}

% AO stimulus
AO stimulus (using annotation of movie cuts) all null results except: contrast 9
(vse\_new > pe\_old): right RSC.
% correlation of these nouns with cuts
Compare Figure \ref{fig:reg-corr}: nouns that cue change to a new setting are
correlated  with cuts to a new setting (vse\_new; r$\approx$0.3; second highest
correlation of a verbal cue category with a cut category.

% AV stimulus
AV stimulus. All contrasts using ``no cut condition'' > x yield null results.
The contrast that uses the narrator's nouns but in the AV stimulus (se\_new >
se\_old): right PPA, bilateral sup. lat. occ. cortex; LOC (l/r), right superior
parietal lobe. Compate Figure \ref{fig:reg-corr}: nouns that cue change to a new
setting are correlated  with cuts to a new setting (vse\_new; r$\approx$0.3;
second highest correlation of a verbal cue category with a cut category; AND:
nouns that cue change to a familiar setting are somewhat correlated with cuts to
a familiar setting (se\_old; r$\approx$0.4; highest correlation of a verbal
category with a cut category).


\subsection{individual results}

\todo[inline]{other reasons than localization performance?}

% why individual results in the first place.
We also investigated localization performance of both naturalistic paradigms on
the level of individual subjects.
% results in both subject and group space
Time series of all subjects were analyzed both in group space and subject space.
% individual z-maps -> dataset
Results of the analyses that were performed in each subject's subject-space
(unthresholded $z$-maps and thresholded $z$-maps with Z>2.3, p<.05 cluster
corrected) can be found in the accompanying dataset.
% report here in group space
For better comparison of results across participants, we here report results
($z$-maps; Z>3.4, p<.05 cluster-corrected) of the analyses that was performed in
group space.
% ref to figure
Figure \ref{fig:subs-thresh-ppa} depicts thresholded $z$-maps of the primary AO
and AV $t$-contrast, and the outline of the individual PPAs
\citep{sengupta2016extension} overlayed on the MNI152 template (transversal slice z=-11 for all subjects).

% results in detail voxels of significant clusters in PPA ROI (and  RSC
% liberally)
% sub-01 (m): PPA (l/r), AO (l/r), AV (-/r); RSC: AO (l/r), AV (-/-)
% sub-02 (m): PPA (l/r), AO (-/-), AV (l/-); RSC: AO (-/-), AV (-/-)
% sub-03 (f): PPA (l/r), AO (-/-), AV (-/r); RSC: AO (-/-), AV (-/-)
% sub-04 (f): PPA (-/r), AO (l/r), AV (-/r); RSC: AO (l/r), AV (-/-) !
% sub-05 (m): PPA (l/r), AO (-/-), AV (-/-); RSC: AO (-/-), AV (-/-)
% sub-06 (m): PPA (l/r), AO (l/r), AV (-/-); RSC: AO (l/-), AV (-/-)
% sub-09 (m): PPA (l/r), AO (l/-), AV (l/r); RSC: AO (l/r), AV (l/r)
% sub-14 (f): PPA (l/r), AO (l/r), AV (-/r); RSC: AO (l/r), AV (-/-)
% sub-15 (m): PPA (l/r), AO (l/r), AV (-/r); RSC: AO (l/r), AV (l/r)
% sub-16 (m): PPA (l/r), AO (l/r), AV (l/r); RSC: AO (l/r), AV (l/-)
% sub-17 (m): PPA (l/r), AO (l/r), AV (l/r); RSC: AO (l/r), AV (l/r)
% sub-18 (m): PPA (l/r), AO (l/r), AV (l/r); RSC: AO (l/r), AV (-/r)
% sub-19 (f): PPA (l/r), AO (l/r), AV (-/r); RSC: AO (l/r), AV (l/r)
% sub-20 (f): PPA (-/r), AO (-/-), AV (l/r); RSC: AO (-/-), AV (-/-)

% PPA ROI (Sengupta et al., 2016)
The dedicated localizer \citep{sengupta2016extension} yielded bilateral ROIs in
12 of 14 subjects and a unilateral right clusters in two subjects (sub-04,
sub-20).
% procedure in Sengupta
Individual ROIs were assessed by visually inspecting unthresholded $z$-maps of
three different contrasts, and setting a subjectively best fitting $z$-threshold
to isolate a ROI that matched the PPA.
% AO
In the current analysis of the AO stimulus, we use the same AO (and AV) contrast
for all subjects. We find statistically significant bilateral clusters in nine
subjects, and one cluster in the left hemisphere of sub-09 that are within or
overlapping with the PPA ROI.
% subj-04
In sub-04, two homologous clusters are apparent, whereas the dedicated localizer
(and AV stimulus) yielded only one cluster in the right hemisphere.
% AV
Results of the AV contrast yield homologous clusters in five subjects,
unilateral right clusters in six subjects (of which one subjects yielded a
unilateral cluster in the dedicated localizer), and a unilateral left cluster in
one subject.
% difference to dedicated localizer
We find homologous clusters in sub-20, whereas the dedicated localizer yielded
only one cluster in the right hemisphere.
% concluding statement
\todo[inline]{concluding statement that does not sound like
interpretation/discussion?}


\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/subs-thresh-ppa}
    \caption{Fixed-effects individual-level GLM results (Z>3.4, p<.05
        cluster-corrected).
        Individual brains are aligned via non-linear
        transformation to a study-specific T2* group template that is
        co-registered to the MNI152 template with an affine transformation (12
        degrees of freedom).
        The results of audio-description's primary
        $t$-contrast (blue) that compares geometry-related nouns to non
        geometry-related nouns spoken by the audio-description's narrator
        (\texttt{geo, groom > all non-geo}) are overlaid over the movie's
        primary $t$-contrast (blue) that compares cuts to new setting with cuts
        to a familiar perspective (\texttt{vse\_new > vpe\_old}).
        Black:
        outline of subject-specific PPA ROIs.
        Light gray: The
        audio-description's field-of-view (cf. \citep{hanke2014audiomovie}).
        To facilitate comparisons across subjects, we chose the same horizontal
        slice (x=-11) for all subjects. Thus, the figure does not depict the
        left cluster of the AV stimulus in sub-09 and sub-18, and the right
        cluster of the AV stimulus in sub-15.}
    \label{fig:subs-thresh-ppa}
\end{figure*}

\todo[inline]{anything else to report in the following paragraph? seems more
like just an explanation of the plots}
% Bland-Altman-Plots: why they were done
To better depict/convey the agreement and difference between individual results
of the dedicated visual localizer and the primary AO contrast, we created one
Bland-Altman-Plots for each of the 14 subjects (see Figure
\ref{fig:bland-altman}).
% x-axis
The x-axis of every subplot depicts the mean of $z$-values of two spatially
corresponding voxels taken from a subject's unthresholded $z$-map.
% y-axis
The y-axis of every subplot depicts the difference of $z$-values of two
spatially corresponding voxels (dedicated localizer minus AO stimulus).
% masking
Voxels were spatially constrained to a) voxels located in the occipital and
temporal lobes (gray; using the probabilistic Jülich Histological Atlas
\citep{eickhoff2005toolbox, eickhoff2007assignment}), b) voxels located in the
PPA overlap of all subjects (blue), and c) voxels located in the individual PPA
ROI(s)(red).
% top KDE plot
A shift of the distribution to voxels with a mean above zero can be seen the
further we constrain voxels to the individual PPA (see KDE plot for the x-axis).
% right KDE plot
Values above the horizontal line depict voxels that show a higher $z$-value in
the dedicated localizer, values below the horizontal line depict voxels that
show higher $z$-value in the AO stimulus.

\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/subjs_bland-altman.png}
    \caption{Bland-Altman-Plots for individual subjects. The x-axis depicts the
        mean of two spatially corresponding voxels in the unthresholded z-map of
        the visual localizer and in the unthresholded z-map of contrast 1 of the
        AO stimulus (KDE plot on the top). The y-axis depicts the difference of
        two voxels (KDE plot on the right). The overlays depict voxels spatially
        contrained to be located in the temporal and occipital cortex (gray),
    PPA overlap of all subjects (blue), and individual PPA (red).}
    \label{fig:bland-altman} \end{figure*}


\section{Discussion}

\todo[inline]{the structure is as follows at the moment: correlation of
regressors; AO group; AV group; robustness AO + AV; anterior vs posterior PPA;
other clusters significant across contrasts (= retrosplenial cortex and
occipital place area)}

\todo[inline]{discussion explains how results fill gap identified in
intro}

\todo[inline]{provides caveats to the interpretation describes how the paper
advances the field by providing new opportunities}

\todo[inline]{typically done by recapitulating the results, discussing
limitations, and then revealing how the central contribution may catalyze future
progress}

\todo[inline]{first paragraph summarizes the important findings from the results
section, focusing on their meaning}

% previous studies: visual
Previous studies reported bilaterally increased hemodynamic activity in the PPA
when participants were watching static pictures of landscape compared to
pictures of faces or objects (e.g. \citep{epstein1998ppa,
epstein1999parahippocampal}) whereas one study using spoken sentences reported
mixed results \citep{aziz2008modulation}.
% we use similar statistical models but with naturalistic stimuli
Here, we use statistical models that are similar to previous studies that used
small sets of experimentally controlled stimuli to investigate hemodynamic brain
responses during two naturalistic stimuli.
% more than just one contrast
We computed varying GLM contrasts that differed in the contrasted categories and
the amount of available data to test the robustness of our findings in both
stimuli.
% comparison to localizer experiment
We compare results of the primary contrast of each stimulus on the level of
individual subjects to results that we gained from a dedicated visual localizer
paradigm done with the same subjects \citep{sengupta2016extension}.
% summary of results main finding
Results of our whole-brain analyses suggest that the perception of verbal
``spatial information'' embedded in an auditory narrative contrasted with
``non-spatial information'' is correlated with increased activation in the
anterior PPA of both hemispheres.
% to do
\todo[inline]{this is just the main finding; other possible ``honourable''
mentions like robustness of results, AV stimulus, individual results?}

\subsection{correlation of regressors}

\todo[inline]{Following paragraph could pasted into method section to save space
here; a short interpretation is given there already; so methods/ results/
discussion are not absolutely separated there already}
% stimuli not designed for research purposes
Both naturalistic stimuli were not designed to be used in a BOLD fMRI experiment
but to entertain their audiences.
% why correlation matrix  was created
Thus, we first tested for correlation of regressors (s. \ref{fig:reg-corr}) that
are based on the annotated stimulus features because a high degree of shared
variance lowers detection power
% results of computing feature correlation
Results show no to minor correlations of regressors across all stimulus
segments.
% encouraged by that we build contrasts (statement)
Thus we felt encouraged to create traditional/canonical voxel-wise GLMs that
modeled and contrasted hemodynamic activity similarly to previous studies that
used simplified stimuli.


% The second through fourth paragraphs may deal with potential weaknesses
% linking it to the relevant literature or how future experiments can deal
% with these weaknesses.

% The fifth paragraph may then culminate in a description of how the paper moves
% the field forward.

% Discussion paragraphs often conclude by describing a clever, informal way of
% perceiving the contribution or by discussing future directions that can extend
% the contribution.

% step by step, the reader thus learns to put the paper’s conclusions into the
%right context.


\subsection{group level: AO \& AV (PPA only)}

% AO stimulus
Group results of the AO stimulus' primary contrast yielded spatially concise
clusters in the anterior part of the PPA group overlap taken from
\citep{sengupta2016extension}.
% diff to Aziz (2008)
Unlike \citep{aziz2008modulation} we did not use whole sentences as events to
model hemodynamic responses but modeled from on- to offset of single words that
cue the listener about the movie's missing visual content.
% conclusion
Our group results suggest that auditory spatial information does not just
modulate activity in the PPA (depending on other factors like familiarity; s.
\citep{aziz2008modulation}) but correlates with bilaterally increased
activation.\todo{``modulate'' suggests causation}

% Previous studies used tasks
Usually studies that use small sets of experimentally controlled stimuli also
employ a task to keep subjects attentive to the stimuli.
% but epstein1998
Nevertheless, one early block-design study \citep{epstein1998ppa} compared
results from a paradigm that employed a (perceptual) judgment task of static
pictures to the same paradigm but without that task.
% results during no task
Hemodynamic activity was less but still significantly increased when
participants had no task to keep them alert and attentive to the stimuli.
% we have no task neither
Our paradigm is similar in that sense that our participants had no behavioral or
cognitive task (e.g. forming a mental image of the stimuli; cf.
\citep{ocraven2000mental})  but just had to ``enjoy the audio-description''.
% but we are still different
Our paradigm still differs from \citep{epstein1998ppa} because the relevant
stimulus features were incidentally embedded in a continuous stream of auditory
information.
% conclusion
The embeddedness prevents participants from making assumptions about the
investigated perceptual process by just being exposed to the paradigm and thus
mimics experiencing the world outside the lab more closely.
% concluding statement: kinda automatic process
The lack of a task further suggests that verbal spatial information is processed
in the PPA in an automatic fashion and does not necessarily rely on deliberately
paying attention to spatial information.

\todo[inline]{following: how to frame it (better) that AV merely serves as
conceptual control?; imo should be a small section anyway}
% AV stimulus
Group results of the AV stimulus' primary contrast yielded bilateral clusters
that span the group overlap of individual PPA ROIs from anterior to posterior.
% difference in location/size to AO and VIS
Clusters also expand into more posterior regions (occipital fusiform gyrus,
lingual gyrus, and intracalcarine cortex).\todo{which of these regions (do not
overlap) with group ROI?}
% confound of higher perception
This larger extension, especially into the primary visual cortex, can be
attributed to the fact that we averaged the content of individual frames after a
cut and did not control for confounding features like depicted faces or
objects,and low-level visual features.
% cross-modal control purpose?
\todo[inline]{concluding statement regarding AV vs AO and cross-modality of
results?}


\subsection{stability of contrasts}

% why we did a couple of contrasts
For both stimuli we created additional contrasts that varied the amount of
events available for the analysis and how well the annotated events within
categories on average matched the stimulus type that we assumed to be
preferentially processed in the PPA.
% PPA AO
Regarding contrasts of the AO stimulus, six of eight contrasts show significant
bilateral clusters in the anterior part of the group PPA overlap.
% PPA AV
Regarding contrasts of the AV stimulus, all five contrasts show significant
bilateral clusters in the PPA but extending into more posterior brain regions.
% results indicate robustness
Thus, the results of the additional contrasts indicate that our findings are
robust across contrasts and do not depend on the design of one specific
contrasts.

% but
Nevertheless, results also suggest that localization performance depends on the
amount of available data and how well the feature space is modeled.
% example
For example, contrast 8 of the AO stimulus (\texttt{se\_new} > texttt{se\_old})
that used the most heterogeneous categories comprising the least amount of
events yielded neither a significant cluster in the right-hemispheric nor
left-hemispheric PPA.
% concluding statement
Hence, investigators using model-driven analyses have to consider how many
events a stimulus to be chosen may provide and how homogeneous the events to be
averaged might be.

% ``confounding'' clusters
Despite performing a whole brain analysis, we do not find many (random) clusters
in single contrasts or even systematically across contrasts that hamper the
interpretation of results.\todo{ouff\dots}
% AV: null results regarding attention and eye movements
For example in the AV results, we do not find significant clusters that could be
attributed to cognitive processes like attention, eye movements(, or
memory).\todo{which kind of memory}
% AO: auditory cortices
For example in the AO results, contrasts that compare (among other categories)
verbal cues about the change of a setting (\texttt{se\_new} > \texttt{se\_old})
to categories that do not cue a change of a setting (contrasts 2, 3, 6, 7) show
significant clusters in (primary and secondary) auditory cortices.\todo{``poor
job of nuisance regressors?}
% averaging vs. nuisance regressors
This suggests that variance correlating with lower level auditory processes was
not averaged out across trials and nuisance regressors did not do their fucking
job (still, it is kinda innate in the contrast).\todo{the word ''fuck`` is not
acceptable}
% possible reason
This could be attributed to a possible change of the soundscape and/or increased
volume when the narrative switches to a new setting.
% concluding statement
Thus, investigators that use model-driven methods to investigate brain functions
using naturalistic stimuli should (extensively) annotate the stimulus material
and test for correlations among variables.


\subsection{consistencies across contrasts}


\subsubsection{anterior vs. posterior PPA}

% intro
Results of the AO contrasts consistently yielded significant clusters
spatially constrained to the anterior part of the PPA group overlap (and
anterior part of significant clusters of the AV contrasts).

% intro to connectivity and object gradient
Kind of complete overview of (references): Previous studies suggest that
posterior and anterior part of the PPA show differences in
% connectivity
a) connectivity profiles \citep{baldassano2013differential, baldassano2016two}[+
Nasr et al., 2013; Silson et al., 2016a],
% scenes vs abstract objects
b) a gradient in sensitivity scene and abstract object stimuli
\citep{arcaro2009retinotopic, aminoff2007parahippocampal,
baldassano2013differential},
% low-level scene features
c) differences in response to low-level scene features [Nasr et al., 2014;
Silson et al., 2015; Baldassano et al., 2016a,b; Watson et al., 2016],
% high-level scene features
d) high-level scene features [Park et al., 2014; Aminoff and Tarr, 2015; Linsley
and Macevoy, 2015; Marchette et al., 2015],
% TMS
e) stimulation studies [Rafique et al., 2015] \citep{baldassano2016two}



\paragraph{posterior PPA (pPPA)}

% connectivity
The posterior PPA is more [strongly] functionally connected to the occipital
visual cortex, including lateral occipital cortex (LOC) and transverse occipital
sulcus (TOS) \citep{baldassano2013differential, baldassano2016two}.
% LOC and TOS
[These regions have well-defined retinotopic maps (Arcaro et al., 2009; Nasr et
al., 2011), and are associated with the perception of low-level visual features
and object shape \citep{baldassano2013differential}.]

%
Increased activity within the pPHC during viewing of scenes relative to objects
has been suggested to reflect regional specialization for visuospatial processes
related to the extraction of scene layout information [Epstein,
2008]\citep{baumann2016functional}.

% high-level visual features
Higher-level visual features also drive response patterns in these regions
[Bryan et al., 2016]. They are hypothesized to be involved in extracting visual
environmental features that can be used for navigation [Marchette et al., 2015;
Julian et al., 2016; Kamps et al., 2016]. However, neither OPA nor posterior PPA
show reliable familiarity effects [Epstein et al., 2007b; see further discussion
below]\citep{baldassano2016two}.
%
The idea that the pPHC serves a dedicated role in visuospatial processing is
further supported by studies showing that this brain region is sensitive to
changes in basic visuospatial properties such as the shape and spatial frequency
of abstract geometric objects and layouts [Rajimehr et al., 2011; Zeidman et
al., 2012]\citep{baumann2016functional}.


% object sensitivity
The pPPA is more responsive to both simple visual textures and (abstract)
objects \citep{arcaro2009retinotopic, baldassano2013differential}:
%
The pPPA responded about four times as strongly to a
flickering checker-board stimulus compared to an anterior portion
\citep{arcaro2009retinotopic}.
%
The response to objects was greater than the
response to scrambled images only in the posterior portion
\citep{arcaro2009retinotopic}.

% low-level features
OPA and pPPA have been shown to be closely related to the visual content of a
stimulus. Even low-level manipulations of spatial frequency [Rajimehr et al.,
2011; Kauffmann et al., 2015; Watson et al., 2016] or rectilinearity [Nasr et
al., 2014] can drive responses in these regions \citep{baldassano2016two}.

% baumann2016functionl
We observed that pPHC is more strongly modulated bilaterally by the visual
properties of the stimulus material (as evidenced by strong responses in the
picture-scene conditions and no significant response in the word-object
conditions)\citep{baumann2016functional}.
%
Our current study [and findings of Baldassano et al. (2013)] suggest that the
functional topographic segregation of the PHC is underpinned by corresponding
segregated connectivity patterns. Findings are consistent with a model of
parahippocampal function in which visual information is projected from occipital
and temporal areas to the pPHC [Libby et al., 2012], where encoding of
visuospatial information takes place (a process that is computationally more
demanding for complex stimuli such as scenes than for simple ones such as
objects; Chai et al., 2010)\citep{baumann2016functional}.



\paragraph{posterior network}

% baldassano2016two
The 1st (posterior) network [of two networks] includes
pPPA and OPA/TOS. pPPA (retinotopic maps PHC1 \& PHC2) and OPA/TOS contain
retinotopic maps and are not strongly coupled to the hippocampus at rest
\citep{baldassano2016two}.

% baldassano20160two: Neurosynth meta-analysis
\citep{baldassano2016two}'s ``meta-analysis'' using Neurosynth: we compare the
query ``scene'' (47 studies) with the query ``episodic memory, navigation, past
future'' (125 studie).

%
Results: voxels significant only in the scene meta-analysis were concentrated in
the posterior network (66\% in posterior network, 18\% in anterior, 16\% in
other) % baldassano2016two: meta-analysis
\citep{baldassano2016two} Voxels significant in both the scene and
memory/navigation meta-analyses tended to fall near the border between the two
networks and divided approximately equally among them (44\% posterior, 53\%
anterior, 4\% other).
%
\citep{baldassano2016two} Voxels significant only in the memory/navigation
meta-analysis were spread more widely across the cortex, but were concentrated
more in the anterior than the posterior network (16\% posterior, 42\% anterior,
42\% other).

% interpretation
Interpretation: visual scene activations tend to be posterior to the memory
activations along the parahippocampal gyrus. [The transition point corresponds
almost exactly to the division between our two networks. Dorsally, we also
observe a separation between the reverse inference maps, with scene and memory
activations falling into our two separate networks \citep{baldassano2016two}.]


\paragraph{anterior PPA (aPPA)}

The specific properties of aPPA have been less well studied, since it was not
recognized as a separate region within the PPA until recently
\citep{baldassano2016two}.

% anterior PPA
The anterior PPA is more [strongly] functionally connected to the RSC and
parietal cortices (caudal inferior parietal lobe; cIPL)[, medial PFC, and the
lateral surface of the anterior temporal lobe]
\citep{baldassano2013differential, baldassano2016two}.

% object sensitivity
The aPPA is less visually responsive to both scenes\todo{check their paradigm \&
result section} and objects, with notably low sensitivity to abstract object
stimuli \citep{baldassano2013differential}.
%
Results suggest that aPPA may be more concerned with these types of spatial and
non-object-based scene properties [= global scene properties such as spatial
expanse] than posterior PPA \citep{baldassano2013differential}\todo{check if
quoted correctly}.

\citep{baldassano2016two} The aPPA is
% category
a) driven more by high-level category information than by spatial frequency
content [Watson et al., 2016],
% locations
b) to represent real-world locations (even from perceptually distinct views)[
Marchette et al., 2015],
% associations
c) to encode object co-occurrences [Aminoff and Tarr, 2015], and
% scene size
d) to represent real-world physical scene size [Park et al., 2014]
\citep{baldassano2016two}.
% spaciousness
Its representation of scene spaciousness draws on prior knowledge about the
typical size of different scene categories, since it is affected by the presence
of diagnostic objects [Linsley and Macevoy, 2015]\citep{baldassano2016two}.


\paragraph{anterior network}

% baldassano2016two's networks
\citep{baldassano2016two} The anterior network (the 2nd of two networks)
consists of the caudal inferior parietal lobule (cIPL), RSC, and aPPA, which
connect to the hippocampus (especially anterior hippocampus). These regions are
implicated in both visual and nonvisual tasks, including episodic memory and
navigation] \citep{baldassano2016two}.
% baldassano2016connectivity
The anterior network [that includes aPPA] exhibits much higher resting-state
hippocampal coupling (especially to anterior hippocampus), suggesting that
memory- and navigation-related functions are primarily restricted to the
anterior network \citep{baldassano2016two}.

% The memory and navigational network
The network of parahippocampal, retrosplenial, and posterior parietal regions
that we identify has been emerged independently in many different fields of
neuroimaging, outside of scene perception.
%
Meta-analyses of internally directed tasks, such as theory of mind,
autobiographical memory, and prospection, have identified this as a core,
reoccurring network [Spreng et al., 2009; Kim,2010; Yeo et al., 2015 (component
C10 of ?)]. It comprises a subset of the broader default mode regions, but
functional and anatomical evidence suggests that it is a distinct, coherent
subnetwork [Andrews-Hanna et al., 2010, 2014; Yeo et al.,
2011]\citep{baldassano2016two}.
%
The broad set of tasks that recruit this network have been summarized in various
ways, such as ``scene construction'' (Hassabis and Maguire, 2007), ``mnemonic
scene construction'' [Andrews-Hanna et al., 2010], ``long-timescale
integration'' [Hasson et al., 2015], or ``relational processing'' [Eichenbaum
and Cohen, 2014].
%
A review of memory studies referred to this network as the posterior medial
memory system, and proposed that it is involved in any task requiring
``situation models'' relating entities, actions, and outcomes [Ranganath and
Ritchey, 2012]\citep{baldassano2016two}.

% hippocampus
The network has strong functional connections to the hippocampus, which has been
implicated in a broad set of cognitive tasks involving ``cognitive maps'' for
organizing declarative memories, spatial routes, and even social dimensions
[Eichenbaum and Cohen, 2014; Schiller et al., 2015]. During perception, the
hippocampus binds together visual elements of an image [Olsen et al., 2012;
Warren et al., 2012; Zeidman et al., 2015], which is especially important for
scene stimuli [Lee et al., 2005a,b; Graham et al., 2006; Hodgetts et al., 2016].
The hippocamppus then stores this representation into long-term memory [Ryan and
Cohen, 2004]\citep{baldassano2016two}.
%
Our results suggest that only the anterior scene regions interface directly with
the hippocampus, potentially enabling the construction of hippocampal
environmental representations, and retrieval of relevant memories and
navigational information for a presented or imagined
scene\citep{baldassano2016two}.



\paragraph{conclusion \& interpetation: subregions}

% conclusion by Baldassano (2013): PPA subregions
Data suggest that PPA might contain identifiable subregions, but subregions
should not be considered as completely independent modules.
%
Both subregions activate selectively to scenes, implying that these subregions
cooperate to build a complete representation of a scene
\citep{baldassano2013differential}.

We do not argue that the PHC is not sensitive to spaces, places, and spatial
information, but rather that it is sensitive to spatial stimuli because of the
associative information (space-related associations) that such stimuli evoke
\citep{aminoff2013role}.
% posterior PPA for visual visual associative information
Spatial contextual associations are a dominant property of places and scenes,
and thus scenes consistently activate this posterior subregion of the PHC (e.g.,
PPA; epstein1998ppa)\citep{aminoff2013role}.

% baldassano2013: functional contrast
The fact that aPPA had a lower sensitivity to our abstract object stimuli does
not necessarily imply that this region does not use object information
\citep{baldassano2013differential}.
% baldassano2013: connectivity
Their distinct connectivity properties, however, suggests that each may be
involved in specific aspects of visual and cognitive processing involved in the
overarching goal of scene understanding.
%
Previous work has shown that PPA responds to objects that have spatial
associations [Aminoff et al., 2007], are space-defining [Mullally and Maguire,
2011], and are navigationally-relevant [Janzen and Van Turennout, 2004]. These
types of responses require spatial memory and cannot be based purely on visual
features like object shape.
%
If anterior PPA is involved in processing spatial context, then space-defining
or navigationally-relevant objects could activate anterior PPA more strongly
than our abstract objects, which were unfamiliar and provided no sense of
context or orientation \citep{baldassano2013differential}. Further experiments
will be required to determine what type of object-related information is used in
this region \citep{baldassano2013differential}.

% baldassano2016two
\citep{baldassano2016two}: We propose that scene processing is fundamentally
divided into two collaborating but distinct networks
%
One is focused on the visual features of a scene image. The other is related to
contextual retrieval and navigation.
%
Under this framework, scene perception is less the function of a unified set of
distributed neural machinery and more of ``an ongoing dialogue between the
material and symbolic aspects of the past and the continuously unfolding
present'' (Baker, 2012, in The Word Hoard).
%
\citep{baldassano2016two} with respect to natural scene perception: one can
imagine at least two separable functions:
%
1) processing the specific visual features present in the current glance of a
scene,
%
2) and connecting that to the stable, high-level knowledge of where the place
exists in the world, what has happened here in the past, and what possible
actions we could take here in the future \citep{baldassano2016two}.
%
For most cognitive and physical tasks we undertake in real-world places, the
specific visual attributes we perceive are just a means to this end, of
recalling and updating information about the physical environment; ``the
essential feature of a landmark is not its design, but the place it holds in a
city’s memory'' [Muschamp, 2006, in New York Times]\citep{baldassano2016two}.


\subsubsection{contextual associations: Baumann 2016}

% visuospatial and mnemonic processes
The PHC plays a key role in
%
a) visuospatial analysis of scenes [Epstein et al., 1999, 2003; Park and Chun,
2009; Rajimehr et al., 2011] and
%
b) (contextual) mnemonic processing [Aminoff et al., 2013, Bar and Aminoff,
2003; Düzel et al., 2003; Janzen and van Turennout, 2004; Baumann et al., 2010;
Wegman and Janzen, 2011] \citep{baumann2016functional}

% associative processing
The PCH has been implicated in the integration and retrieval of contextual
associations in memory [Aminoff et al., 2013; Bar and Aminoff, 2003; Bar et al.,
2008a, 2008b]. According to this view, activity in PHC reflects
modality-independent retrieval of contextual information from memory, such as
which objects typically occur in a given scene and where they are likely to be
located relative to one another (contextual associations: relations between
objects and conditions that describe, represent, and bring meaning to an
environment [Aminoff et al., 2013]) \citep{baumann2016functional}.

%
We provide evidence for a dissociation between neural activation patterns
associated with visuospatial analysis of scenes and contextual mnemonic
processing along the parahippocampal longitudinal axis
\citep{baumann2016functional}.

%
Paradigm: Participants had to judge the contextual relatedness of scene and
object pairs (presented either as words or pictures)
\citep{baumann2016functional}.
%
Participants judged the contextual relatedness of scene and object pairs (“bike”
\& “bell” are a contextually congruent object– object pair, “sofa” \&
“microscope” are incongruent; “boat” \& “lake” are a contextually congruent
object–scene pair, “tractor” \& “hotel” are
incongruent)\citep{baumann2016functional}.
%
Each pair was presented either in printed-word format or in a combined
word–picture format. That allowed us to present pictorial objects and scenes in
isolation (Fig. 1)\citep{baumann2016functional}.

%
1) To identify visuospatial regions of the PHC, we tested for voxels with
greater activity for picture–scene trials than for picture–object trials over
and above activity differences elicited by the comparison of word–scene trials
and word– object trials\citep{baumann2016functional}.
%
2) To locate context-related regions of the PHC, we tested for voxels that were
selectively active during the contextual judgment task regardless of the type of
stimulus (scenes vs objects) or presentation modality (word vs
picture)\citep{baumann2016functional}.

%
Results: aPHC is more active during contextual judgments regardless of stimulus
category (scenes vs objects) or modality (word vs picture).
%
The left aPHC was active in all contextual judgment task conditions. This
suggests a functional segregation, with posterior sections of the PHC supporting
visuospatial processing of scenes and anterior sections of the left PHC involved
in contextual associative processing\citep{baumann2016functional}.
%
It is important to note that, whereas our study only examined retrieval-related
activity for object–object and object–scene relationships, findings from prior
studies suggest that the functions of the aPHC are not limited to these types of
material, but should also be involved in the retrieval of associations involving
other stimulus categories such as faces [Düzel et al., 2003; Kirwan and Stark,
2004]\citep{baumann2016functional}.

%
Interpretation: Activation maxima associated with visuospatial and mnemonic
processes were spatially segregated, providing support for the existence of
functionally distinct subregions along the PHC longitudinal axis and suggesting
that the PHC serves as a functional interface between perception and memory
systems\citep{baumann2016functional}.
%
This anatomical segregation of function [cf. their results] suggests a
predominantly perceptual role for the pPHC and a mnemonic role for the anterior
parahippocampal cortex (Note: the anterior parahippocampal cortex should not be
confused with the anterior portion of the parahippocampal gyrus, which includes
the perirhinal cortex; Ranganath and Ritchey,
2012.)\citep{baumann2016functional}.
%
We suggest that the aPHC encodes incoming information in an amodal format and
acts as an interface with other memory regions, including the hippocampal,
retrosplenial, and perirhinal memory systems (see Ranganath and Ritchey, 2012,
for an review)\citep{baumann2016functional}.
%
Linsley and McEvoy (2015) found (using an fMRI searchlight analysis) that
activity patterns in the aPHC were strongly modulated by the presence of objects
in scenes; patterns in the pPHC were not modulated. This again suggests that the
aPHC is more concerned with contextual than visuospatial analysis of scene
images\citep{baumann2016functional}.

%
Hypothesis: Moreover, it is conceivable that different categories of contextual
associations (e.g., spatial, temporal, affective, etc.) cluster in different
subregions within the aPHC.  High-resolution fMRI and analysis techniques such
as adaptation fMRI [Krekelberg et al., 2006] or multivoxel pattern
classification [Norman et al., 2006] could be used to determine any such
finegrained functional topography within the parahippocampal cortex]


\subsubsection{contextual associations: Aminoff 2013 (review) + 2015}


% memory
Models of recognition memory have previously identified parahippocampal cortex
as primarily encoding spatial context information (Eichenbaum et al., 2007)
\citep{baldassano2013differential}.

% contextual associations in anterior vs. posterior
\citep{aminoff2013role} proposed that the general function of the PHC can be
thought of as processing of contextual associations. Activity within the PHC is
distributed along an anterior–posterior axis:
% pPPA
Activity related to spatial contexts is focused in the posterior part of the PHC
=> posterior subregion of PHC is particularly optimized to process spatially
organized contextual associations.
% aPPA
Activity evoked by non-spatial contexts focused in the more anterior part of the
PHC (optimized to process contextual associations in other, non-spatial domains)
[bar2003cortical, aminoff2007parahippocampal] \citep{aminoff2013role}.

% Aminoff regarding contextual processing in the PHC and beyond
\citep{aminoff2013role} regarding method in [aminoff2007parahippocampal]: we
compared contextual processing of objects that are strongly related to a
specific place (oven is found in a kitchen) with contextual processing of
objects that are highly associated with a context but not a specific place
(birthday cake is found at a birthday party; but the party can take place
anywhere)\citep{aminoff2013role}.
%
Activity elicited by these two types of contextual objects was compared with the
activity elicited for objects that were only weakly associated with many
contexts (e.g., a lamp) \citep{aminoff2013role}.
% results
Differential activity was found in the PHC and the RSC, replicating the previous
findings [Bar \& Aminoff (2003) The parahippocampal\dots]
\citep{aminoff2013role}.


\citep{baumann2016functional} zu \citep{aminoff2015associative}:
% pPHC
pPHC showed increased activity when participants viewed (already familiar)
spatial configurations of meaningless shapes (regardless of their individual
identities);
% aPHC
aPHC was more active during the display of (already familiar) meaningless shapes
(regardless of their spatial configuration).
%
These findings can be interpreted as suggestive of pPHC visuospatial processes
and aPHC contextual processes \citep{baumann2016functional}.


\paragraph{Abstract}

\citep{aminoff2015associative} How are scenes represented in the human brain?
Along what visual and semantic dimensions are scenes encoded in memory?
%

Hypothesis:
%
a) global spatial properties provide a basis for categorizing the neural
response patterns arising from scenes
%
b) non-spatial properties, such as single objects, also account for variance in
neural responses

%
The list of critical scene dimensions has continued to grow encompassing
properties such as geometric layout, big/small, crowded/sparse, and
three-dimensionality.
%
These dimensions may be better understood within the more general framework of
associative properties \citep{aminoff2015associative}.
%
Across both the perceptual and semantic domains, features of scene
representations are related to one another through learned
associations\citep{aminoff2015associative}.

%
We investigated whether different types of relations, all falling under the
heading of associative processing, reliably regions of brain networks known to
be scene selective\citep{aminoff2015associative}.
%
Critically, the components of such associations are consistent with the
dimensions that are typically invoked to account for scene understanding and its
neural bases\citep{aminoff2015associative}.

Results: We show that non-scene stimuli displaying novel associations across
identities or locations recruit putatively scene-selective regions of the human
brain (the parahippocampal/lingual region, the retrosplenial complex, and the
transverse occipital sulcus/occipital place area)\citep{aminoff2015associative}.

%
Moreover, we find that the voxel-wise neural patterns arising from these
associations are significantly correlated with the neural patterns arising from
everyday scenes providing evidence whether the same encoding principals underlie
both types of processing\citep{aminoff2015associative}.

%
Interpretation: Results provide evidence that the neural representation of
scenes is better understood within the broader theoretical framework of
associative processing\citep{aminoff2015associative}.
%
Results also demonstrate a division of labor that arises across scene-selective
regions when processing associations and scenes providing better understanding
of the functional roles of each region within the cortical network that mediates
scene processing\citep{aminoff2015associative}.


\paragraph{Intro}

Scenes are complex stimuli containing rich, statistically regular information
about objects, background, context, semantics, and spatial layout at multiple
scales.
%
As a core component of scene understanding we extract and learn these
regularities (certain objects typically appear together
[1–3]).
%
Associated regularities are useful in defining scene as well as in predicting
which other objects and relations are likely to occur within a scene
[5].

% a-modality
Features of any kind of mental representation, irrespective as to whether that
representation is nominally visual, linguistic, etc., are related to one another
via learned associations.
%
These associations are not necessarily modality-specific (for example, visual
representations are likely to carry many semantic and affective associations not
directly present in the image).

%
With respect to scene understanding, we construe contextual associative
processing as a more semantically-driven process in which associations are not
arbitrary relations, but rather relations that emerge as a consequence of their
shared context within the larger scene.
%
Associations are not simply frequently co-occurring features pairs; instead they
are pairs that both co-occur and add meaning to scenes.
%
Furthermore, the mechanism by which scenes are perceived, recognized, and
understood, is through means of processing the associations elicited from
components of the scene.

%
At the core of neurally-based theories of scene perception are the issues of the
computations and representations instantiated in each of these brain regions.
The literature has focused on more the representations, characterizing the
nature of the information encoded about scenes.
%
It has been suggested that scenes are analyzed in terms of their spatial global
properties, such as the ‘spatial envelope’, which takes into account a scene’s
overall spatial structure and layout [20,21].
%
Empirically, it has been demonstrated that global properties are salient
cues for categorizing scenes in both behavior [22] and in patterns of neural
activity [23–26].

%
Compared to other factors that characterize scenes (i.e., content and depth),
the expanse of the scene (i.e. its spatial boundary) has been found to be the
most effective in accounting for variations in the neural responses within
scene-selective brain regions [23,24].
%
Generally, spatial information defining the geometric layout of a scene has
often been implicated in determining the neural responses arising from
scene-selective brain regions [27].

%
In contrast, non-spatial properties can account for variations in the neural
responses associated with scene processing and
representation.
%
For example, Harel et al. [11], found that the neural encoding of scenes
captures information about single objects (i.e., non-spatial) as well as
background spatial properties.
%
Supporting this point, strongly contextualized objects also elicit neural
activity within scene-selective cortex [28,29].

%
Finally, objects present as the contents of a scene can also modulate activity
in scene-selective regions.
%
For example, scenes where the most salient object carries strong contextual
associations (e.g., a scene with a parking meter) recruit scene-selective
regions more than do scenes with a salient object carrying weak contextual
associations (e.g., a scene with a squirrel) [30].

%
Overall, scene-selective brain regions are encoding a rich collection of
information, including global and local properties, spatial background
information, and individual object tokens.

%
The list of critical properties keeps growing (e.g., big/small, crowded/sparse,
three-dimensionality [31–33]), some seemingly contradicting others about which
properties are critical in scene understanding.
%
Scene understanding may be better understood within a more general framework in
which scenes are encoded with respect to their associative properties
[28,30,34,35].
%
Associative properties are not restricted to the spatial or the global domain,
and can account for why information regarding spatial layout and single objects
play an important role in scene understanding and concomitant neural
responses.

%
Associative processing offers an explanation for what particular kinds
of information are present in scene representation (e.g., objects and spatial
information) and a basis for explaining how and why this information is
important.
%
For instance, on average, spatial layout and global properties may carry more
associative information, and thus are typically prominent in accounts of the
mechanisms mediating scene recognition.

%
To test the idea that associative processing is inherent in scene understanding,
we explore whether the network of brain regions recruited in scene perception is
also recruited when processing non-scene-like stimuli that contain associative
information.

%
If these regions are active when processing the associations learned between
non-scene-like novel stimuli than these regions must not be involved in only
processing the visual properties of the scene, but rather, are processing what
is associated with the components of the scene as learned from previous
experience.

%
We focus on the associative processing of concurrent object identities (i.e.,
semantic associations) and spatial relations between objects (i.e., spatial
associations).

%
For both semantic and spatial associations, we predict that the three
typically-identified scene-selective brain areas, the PPA, RSC, and OPA, will
all exhibit selective activity for both scenes and for associations in general,
without necessarily having any obvious “scene-like”
qualities.

%
We are positing that scene processing is best understood as, at least in part,
arising from a more general mechanism–that of associative
processing.
%
The fact that this mechanism is more general does not make it vague. Our point
is that learning and encoding relations between tokens (“associations”) is a
fundamental process across many cognitive domains and, critically, this process
has explanatory validity for scene processing.
%
Reframing scene processing as involving a more general mechanism helps
articulate the means by which scene perception
occurs.

%
Participants were scanned while they engaged in processing either everyday
scenes or associations between meaningless shapes.
%
For processing of meaningless shapes, participants performed a task (prior to
scanning; unrelated to the associations) in which they implicitly learned
spatial or identity associations between configurations of novel shapes (Fig
1).

%
First, to compare brain activity from learning novel associations to the
processing of scenes, we correlated the distributed, voxel-by-voxel pattern of
activities arising from these two conditions, thereby testing whether similar
neural mechanisms mediate both general associative processing and scene
perception.

%
Second, consistent with previous work ([28,31,34,37]) we asked whether the
neural encoding of scenes shifts across association types–from non-spatial or
semantic scene properties to spatial scene properties–as one moves from the
anterior to the posterior PPA.
%
These analyses shed light on the hypothesis that a single underlying principle,
associative processing, accounts for how scenes are represented in the
brain.

%
Although prior studies have manipulated spatial and identity associations by
having participants learn meaningless patterns of shapes [34], ours is the first
study to directly compare such associative processing with the processing of
everyday scenes.

%
We include two new baselines to better understand whereby effects of
associations arise: weak contextual objects and scrambled images. Finally, we
are careful to collect an additional independent dataset to functionally define
scene-selective regions.

%
These new conditions are all advances (in terms of explicating the processing of
scene-selection regions) relative to our earlier study in which we used the same
shapes and trained participants over a two-week period [34].
%
We also include several methodological changes: a spatial-only associative
condition was included to isolate spatial processing independently of identity
associations; and the no-association condition provides a better control in that
it now shows the same number of shapes as the association condition (i.e., 3).

\paragraph{Method}

\paragraph{Results (just aPPA vs. pPPA)}

%
There is some evidence that within the parahippocampal region, anterior and
posterior regions may support different kinds of associations
[34,37,40,41].
%
a) aPPA processes non-spatial associations (as realized in the ID condition)
%
b) pPPA process spatial associations (as realized in the SP
condition.
%
We tested this hypothesis by examining two contrasts within scene-selective PHC
regions: SP versus NA and ID versus NA.
(we used the scenes versus baseline contrast to capture all scene-related
activity within parahippocampal cortex).

%
The pattern of average activity across subregions supports a division of labor
in which
%
a) anterior subregions showed the largest differences in the ID versus NA
contrast relative to the SP versus NA contrast, while
%
b) posterior subregions showed the largest differences in the SP versus NA
contrast relative to the ID versus NA contrast.
%
In the LH PPA this was significant in both in the interaction of Association by
Subregion (F(4,56) = 5.85, p < .001) and critically, in the linear interaction
contrast (F(1,14) = 8.05, p < .01).
%
The significance of this linear contrast indicates that the transition from
identity to spatial associative processing may be construed as a gradient from
anterior to posterior regions of parahippocampal cortex. However, the same
Association by Subregion interaction was not observed in the RH
PPA.

%
To examine whether identity and spatial associations also capture
functionally-relevant properties within scenes, we examined the similarity
between the voxel-wise pattern of BOLD responses elicited by identity
associations and real-world scenes, and by spatial associations and real-world
scenes.
%
We hypothesized that, to the degree that these two types of associations are
intrinsic to scene processing, similar patterns of activation across each of the
two association conditions and real-world scenes would identify those
(potentially separable) neural substrates supporting the representation of these
kinds of associations.

%
Unthresholded t-values for the ID versus NA and SP versus NA contrasts within
each of the five PPA subregions were extracted for each voxel and then
cross-correlated within each subregion with the real-world scenes versus
objects/scrambled contrast.
%
In LH PPA the interaction within an omnibus ANOVA [the two factors being
Subregion (moving from the most posterior to the most anteri- or) x Association
(identity or spatial)] was significant (F(4,56) = 2.69, p < .04) as well as the
critical linear x linear trend analysis (F(1,14) = 7.57, p < .016) (Fig
3A).
%
Consistent with our analysis of average activity within these subregions, this
interaction was not observed in the RH PPA (F(1,14) = 1.22, n.s.) (S1
Fig).
%
However, it is worth noting that within the right hemisphere, the correlation
between spatial associations and real-world scenes did exhibit the predicted
pattern whereby the strongest correlation was observed in posterior subregions
of PPA, with a progressively lower correlation being observed moving in a
posterior to anterior direction (S1 Fig).

%
=> Our results indicate that the organization of PPA with respect to associative
processing is a gradient in which posterior subregions of PPA are biased towards
spatial associations and anterior subregions of PPA are biased towards identity
associations.

%
Additional support for this claim is shown in Fig 3B, where instead of dividing
the PPA into five equal subregions, the correlation was run over progressively
more and more of the PPA, starting from the most posterior subregion and adding
subregions until the entire PPA was taken into account;
%
As illustrated, the most extreme ends of the PPA show the strongest biases
towards capturing the similarity of either spatial associations and scenes, or
identity associations and scenes.
%
whereas if the whole PPA were to be surveyed, these differences would be masked.
Of note, this gradient appears to be truly continuous across posterior and
anterior portions of the PPA–even considering only the posterior PPA, we observe
a gradient in which the strongest preference for spatial associations is found
in the most posterior portions of the posterior
PPA.
%
Similarly, considering only the anterior PPA, we observe a gradient in which the
strongest preference for identity associations is found in the most anterior
portions of the anterior PPA.


\paragraph{Discussion (just aPP vs pPPA)}

Results suggest some specialization (at least in preference) for the processing
of associative information across the PPA.
%
pPPA is preferentially recruited by spatial associations while anterior PPA is
preferentially recruited by semantic (i.e., non-spatial) associations.

%
A functional distinction between anterior and posterior PPA has been reported
previously. In particular, this distinction has been based on PPA responses when
processing objects associated with spatial contexts (i.e., contexts associated
with specific places, for example, an oven and a kitchen) and when processing
objects associated with non-spatial contexts (i.e., contexts not tied to
specific places, for example, champagne with New Year’s Eve) [28,34,37,40,41].

%
Here, we explicitly investigated this spatial/non-spatial distinction by
isolating the two association types in simple, novel stimuli and then relating
these controlled stimuli to the processing of real-world scenes containing
similar associations.
%
We predicted that this Association-Type x Scene correlation would reveal the
expected posterior to anterior progression with spatial properties within scenes
selectively recruiting posterior PPA and non-spatial, identity properties within
scenes selectively recruiting anterior PPA.
%
Moreover, we expected that this functional division would be continuous, with
the similarity in BOLD responses elicited by scenes and trained spatial
associations decreasing in a posterior to anterior direction.
%
In contrast, the similarity in BOLD responses elicited by scenes and trained
identity associations was predicted to decrease in an anterior to posterior
direction. These predictions were confirmed in our study.

%
This information processing gradient may lie within a broader, more general
organization of the PHG.
%
The gradient of processing identity or non-spatial associative information may
continue through the neighboring perirhinal cortex, which is thought to be
involved in object and person recognition particularly with regard to combining
different fea- tures of knowledge, such as a stop sign is red
[42].
%
In contrast, the pPPA border regions where mid-level visual information is
processed, which may be more sensitive to spatial
information.
%
We do note, however, that this interaction was only observed in the left
hemisphere, although the right hemisphere demonstrated a similar gradient of
specialization in spatial processing from posterior to
anterior.

%
However, the activity related to identity processing was less well organized.
This pattern of results is somewhat consistent with the fact that the left
hemisphere is often implicated in more semantically-oriented processing, for
example, scene categorization, whereas the right hemisphere has been implicated
in more spatial processing, for example, as in the visual details between
different exemplars of the same category [10].
%
These patterns hint that the right hemisphere PPA may play a larger role in the
processing of the spatial relations within scenes and a lessor role in the
processing of scene semantics.
%
In sum, our current functional view of the PPA is as a gradient in which
progressively different scene dimensions come to be
instantiated.


\paragraph{Discussion: division of labor in the PPA, RSC and OPA}

The PPA, RSC, and OPA were all strongly engaged in both associative processing
and in scene perception.

%
However, it is unlikely that these three regions play entirely redundant roles
in scene understanding–a conclusion supported by the differences we observed
between the SP, ID, and SPID conditions.
%
In particular, we suggest that the PPA involves the processing of multiple scene
dimensions that reflect different kinds of associative information (e.g.,
spatial and identity).

%
RSC and OPA were both more sensitive to identity associations, whether appearing
in isolation or in conjunction with spatial associations. Elsewhere we have
suggested that the RSC processes the prototypical representation of context,
termed a “context frame,” which contains information regarding both the key
objects and the spatial relations be- tween them
[28,43].
%
For example a context frame of bathroom would include information regarding a
shower, toilet, sink, toothbrush, mirror, as well as spatial information such as
the mirror is typically located above the sink.

%
Our present data support this suggestion, indicating that the RSC is involved in
both spatial and identity associative processing. Moreover, responses in the RSC
were the most predictive of learning: better learning elicited higher neural
activity in the RSC, specifically in the conjoined SPID
condition.
%
As such, the RSC, as the highest level of associative processing, may be
involved in the long-term encoding of contextual associations, that is, the
context frame, where both spatial and non-spatial associations are
processed.

%
In contrast, the OPA lies in close proximity to the IPS, a region that has
anatomical connections with both the ventral stream, potentially conveying
identity information, and the dorsal stream, potentially conveying spatial
information [44,45].
%
This provides the anatomical architecture to combine both identity and spatial
information within a unified contextual
representation.
%
Future studies are needed to explore the specific anatomical connections with
the OPA regions, examining whether it is similarly connected to both the dorsal
and ventral pathways.

%
Interestingly, our study also found the neural signal related to the processing
of spatial associations from the OPA was negatively correlated with
learning.
%
That is, the better the learning of spatial associations, the lower the response
of the OPA relative to the control condition. Because the OPA is more posterior
than the PPA and RSC, it may be involved in earlier stages of scene processing
[16–18].
%
Our results build on this, suggesting that the initial processing of spatial
associations occurs in the OPA, whereas more complex analysis of spatial
contextual associations, with increased learning, may occur in, progressively,
the PPA and RSC.


\paragraph{Conclusions}

%
We invoke associative processing as a computationally-definable construct for
predicting the spatial organization of scene-related activity across different
scene properties.
%
we suggest that the neural mediators of scene processing should not be construed
as encapsulated visual modules, but rather as manifestations of associations
that reflect the interaction between visual recognition processes and the
application of long-term memories arising from past experiences (Aminoff et al.,
2013).
%
Within this framework, scene processing is not a purely bottom-up visual
process, but rather is an interactive process in which we are constantly
considering–in the form of both spatial and non-spatial associations– our past
experiences to generate predictions, expectations, and constraints about our
physical environment.

%
We suggest that general associative processing mechanisms are sensitive to
frequently occurring or repeated relations within scenes, including object
identities that co-occur, spatial relations between the objects, and spatial
locations for individual objects, all of which facilitate the recognition and
categorization of visual scenes.
%
As such, the collection of associations that help to define a scene provides a
context for individual elements within that scene.
%
For example, a towel hanging on a rack near a shower would most likely be a
bathroom towel, whereas the same towel lying flat on the sand near an umbrella
would most likely be a beach towel.
%
We contend that the parsing of these sorts of contextual associations within
scenes is fundamental to scene understanding, and that scene understanding
construed as such, can then influence cognitive processing in myriad
ways.

%
Within the human brain, we argue that the neural bases for the application of
contextual knowledge is a functional gradient spanning different types of
associations instantiated across the PPA, and contextually-mediated
representations within the RSC and OPA.
%
Most saliently, our observations of selectivity for spatial and identity
associations provide compelling evidence that the neural representation of
visual scenes is best understood as a consequence of the asso- ciative
processing in both the spatial and non-spatial domains.



\subsubsection{RSC (in both stimuli)}

\todo[inline]{check location in primary contrast and overlap across contrasts}

\todo[inline]{report numbers of contrast in AO \& AV that show sign. clusters in
RSC}

% but RSC
Apart from the PPA, we find significantly increased activity consistently across
contrast of both contrasts in the retrosplenial cortex (RSC).

% location functionally defined
In context of a scene-responsive region, the RSC is functionally defined and not
necessarily identical to the anatomically defined retrosplenial cortex [26]
\citep{epstein2008parahippocampal}.
% location
The RSC is located in the retrosplenial cortex, and posterior cingulate region,
near to the point where the calcarine sulcus joins the parietal-occipital sulcus
\citep{epstein2008parahippocampal}.
% anatomy of real RSC
The Retrosplenial cortex (BA 29 and 30) adjoins and is partially encircled by
the posterior cingulate (BA 23 and 31) [51–56]
\citep{epstein2008parahippocampal}.
% function
RSC is strongly active during scene viewing, scene imagery [25] and mental
imagination of navigation through familiar environments [8]
\citep{epstein2008parahippocampal}.

% Aminoff 2013
Processing of strong, long-term, contextual associations elicits activity within
the PHC, as well as the RSC (which includes regions of the retrosplenial cortex,
extending into the posterior cingulate cortex, and the precuneus), the medial
prefrontal cortex (MPFC), and the transverse occipital sulcus (TOS) (Figure 2)
[2,3,59–67] \citep{aminoff2013role}.

% discussion of what the RSC does
\todo[inline]{interpretation: cf.\citep{vann2009what} and paper that reference
her}
% conclusion
\todo[inline]{conclusion?}


\citep{aminoff2015associative}: Three regions of the cortex that have bee
identified as responding selectively to visual scenes: the PHC/lingual region
(commonly referred to as the “parahippocampal place area”) [8–12]),
retrosplenial complex (including the retrosplenial cortex and portions of the
posterior cingulate and precuneus, RSC) [13–15]), and a lateral occipital region
termed the “occipital place area” (OPA; also referred to as the transverse
occipital sulcus, TOS) [16–18]).
%
All three of these brain regions are functionally defined by comparing the BOLD
signals–as measured by fMRI–arising from viewing scenes to those arising from
viewing objects or faces (e.g., [19])\citep{aminoff2015associative}.

% baldassano2016two
RSC: appears to be most directly involved in orienting the viewer to the
structure of the environment (both within and beyond the borders of the
presented image) for the purpose of navigational planning; it encodes both
absolute location and facing direction [Vass and Epstein, 2013; Epstein and
Vass, 2014; Marchette et al., 2014], integrates across views presented in a
panoramic sequence [Park and Chun, 2009], and shows strong familiarity effects
[Epstein et al., 2007a,b]\citep{baldassano2016two}.

% contrasting the two networks
Several previous studies have already shown differential effects within these
two networks. Contrasting the functional connectivity patterns of RSC versus OPA
or lateral occipital cortex (LOC; Nasr et al., 2013) or anterior versus
posterior PPA [Baldassano et al., 2013] show a division between the two
networks, consistent with our results. Contrasting scene-specific activity with
general (image or word) memory retrieval showed an anterior versus posterior
distinction in PPA and cIPL/OPA, with only more anterior regions (aPPA and cIPL,
along with RSC) responding to content-independent retrieval tasks [Johnson and
Rugg, 2007; Fairhall et al., 2014]\citep{baldassano2016two}.


\subsubsection{superior lateral occipital cortex (TOC / OPA?)}

\todo[inline]{check location in primary contrast and in overlap of contrasts}

\todo[inline]{numbers of contrast in AO \& AV that show sign. clusters in
LOC}

\todo[inline]{e.g.
Nakamura (2000). Functional delineation of the human occipito-temporal areas;
Bettencourt (2013). The Role of Transverse Occipital Sulcus in Scene Perception;
Dilks, Kanwisher (2013). The Occipital Place Area Is Causally and Selectively Involved in Scene Perception;
Grill-Spector (2003). The neural basis of object perception;
Nasr (2011). Scene-selective cortical regions in human and nonhuman primates;
Hasson (2003). Large-scale mirror-symmetry organization;
MacEvoy and Epstein (2007). Position selectivity in scene- and object-responsive\dots;
Dilks (2011). Mirror-image sensitivity and invariance in object \& scene processing}

%
\citep{baldassano2016two} functional difference pPPA vs. OPA: functional
distinction between pPPA and OPA is currently unclear. Previous work has
speculated about the purpose of the apparent ventral and dorsal ``duplication''
of regions sensitive to large landmarks, proposing that it may be related to
different output goals (e.g., action planning in OPA, object recognition in
pPPA)[Konkle and Caramazza, 2013], or to different input connections (e.g.,
lower visual field processing in OPA, upper visual field processing in
pPPA)[Kravitz et al., 2013; Silson et al., 2015]. OPA and pPPA may also use
information from different visual eccentricities: OPA processing less
peripheral, relatively high-resolution environmental features. pPPA processing
more peripheral, large-scale geometry, and context [Baldassano et al.,
2016a]\citep{baldassano2016two}.


\subsection{negative controls and cross-modal controls}

\todo[inline]{could be mentioned in a few sentences. But above topics a far too
much still; better embed somewhere above}


\subsection{individual analyses}

An examination of these results on an individual participant basis reveals that
this pattern remains highly reliable and, therefore, does not appear to be a
by-product of group averaging across slightly offset discrete functional
subregions[aminoff2015associative].


\subsubsection{AO stimulus}

\todo[inline]{``problem'': if discussion of anterior vs. posterior PPA above
turns out that anterior PPA is a submodul of the PPA we do not localize a
``visual area'' with an auditory stimulus}

\todo[inline]{how to discuss Bland-Altman-Plots? Let's talk about that
shortly please}

todo[inline]{in 11 subjects there are voxels of higher z-score (in the PPA ROI)
    in the AO contrast than in the contrast of the visual localizer -> should
    all be in the anterior part of the PPA ROI, i.e. the area that belongs to
    the significant cluster of the AO contrast}

\todo[inline]{shortcoming: no task to attend to spatial information}

\todo[inline]{different level of alertness during paradigms}

% intro
In order to test the audio-description's performance as a functional localizers,
we also compared the results of the audio-description's primary contrast to
results of the dedicated visualizer on the level of individual subjects.
% results of dedicated localizer
The dedicated localizer \citep{sengupta2016extension} yielded bilateral ROIs in
12 of 14 subjects and a unilateral right ROI in two subjects (sub-04, sub-20).
% in the current study
The audio-description's primary contrast yielded significant bilateral clusters
in 9 of 14 subjects (of which sub-04 shows just right-lateralized PPA ROI)  and
a unilateral significant cluster in one subject.
% conclusion
Results of our exploratory analyses suggest that a naturalistic auditory
stimulus is suitable to localize (the anterior part) of a higher-level visual
area in the majority of subjects.

% one threshold for all
Here we used a ``one threshold fits all approach`` whereas
\citep{sengupta2016extension} chose the best fitting $z$-threshold in a
subject's unthresholded $z$-maps by visual judgment and the ``best fitting''
threshold.
% one contrast for all
Moreover, we chose the same contrast to look at individual results of all
subjects.
% chose the best contrast in the future
Similarly to \citep{sengupta2016extension} that chose the ``best'' of three
alternative contrasts (strict, relaxed, or simple contrast), future studies that
aim for individual diagnostics / ROI localization should also test more than one
contrast to gain the ``best'' individual ROIs.\todo{``best'' sounds pretty
shitty}

% quote from Hanke (2014)
\citep{hanke2014audiomovie}: Compared to AV stimulus, the AO stimulus leaves a
much larger margin for inter-individual differences in imagining scenery, as
well as actors' character and personality traits, while still preserving the
time-locked presentation of information to a listener. At the same time, the AO
stimulus limits the effect of an attentional focus on the selection of a subset
of simultaneously occurring auditory events, in contrast to the selection of
different parts of the visual field.

\todo[inline]{discussion of individual differences? what do the results
suggest? Differences in alterness, attention to spatial information,
predisposition? ``capability'' to process auditory spatial information?}

\todo[inline]{individual ``preference'' to pay attention to
spatial infos (which contradicts statement above that attention is not a
prerequisite)}


\subsubsection{AV stimulus}

\todo[inline]{was not aim of the study; AV ROIs are in the plot of individual
    slices but not in the Bland-Altman-Plots}

\subsection{shortcomings in general}

\subsubsection{AO contrast}

\todo[inline]{cf. Methods: annotation \& Methods: AO copes}
% optimal stimulus type in vision
In the visual domain, landscapes and not pictures of landmarks or buildings are
considered to be the ``optimal'' stimulus type (cf. Epstein (1998, 1999, 2003,
2008).\todo{check references}
% cateogries in primary contrasts
In the current study's primary contrast, we compared verbal cues regarding
landmarks ((\texttt{geo}; often buildings or other single aspects of a whole
scene) and elements defining the geometry of a room/locale (\texttt{groom}) to
non-spatial cues.
% we did not choose se\_new and se\_old
We did not choose the categories that contained switches from one setting to
another (\texttt{se\_new} and \texttt{se\_old}) which one might assume to
contain the auditory equivalent to the ``optimal'' visual stimulus type.
% why not se\_new or se\_old
The categories \texttt{se\_new} and \texttt{se\_old} were actually
heterogeneous: they rarely contained whole and thus a vague verbal descriptions
of landscapes (e.g. ``in a football stadium'', ``Forrest is running through the
jungle'') but mostly landmarks or buildings, and also non-spatial
cues (e.g. ``Jenny as a teenager``).
% further studies
Given that humans can identify the gist of a rich visual scene within the
duration of a single fixation \citep{henderson2003human} further studies might
investigate if categories of holistic but vague verbal descriptions of
landscapes correlate with differential hemodynamic activity in the PPA compared
to more concrete spatial cues.

% future studies
Futures studies should chose a auditory narrative that better samples the
feature space correlating with the brain process to be investigated.
% future studies: stimulus material
Future studies should probably use a different narrative that models the feature
space of spatial information ``better'' (provides more events that are more
easily classified to belong to distinctive categories).

\todo[inline]{hypothesis of increased activation correlation with changes in the
    soundscape; cf. control contrasts (e.g. \texttt{vse\_new} >
    \texttt{pe\_old})}.
% foreshadowing of narrator by soundscape
Further, verbal cues are regularly forestalled by changes of the soundscape.
% arnott
\todo[inline]{e.g. arnott2008crinkling find increased activations of PPA to
sound made by objects}
% hypothesis
We hypothesize that not just semantic information but also the mere changes of a
soundscape that cues a listener about a setting's spatial layout of could
correlate with increased activity in the PPA.




\subsubsection{AV stimulus}

\todo[inline]{imo opening the pandora's box of the AV shortcomings especially at
the end of the paper should be kept to a minimum of text}

% focus of study
The focus of the current study was to investigate if signficantly increased
hemodynamic activity in the PPA correlates with spatial information (compared to
``non-spatial'' information) during a naturalistic auditory stimulus.
% shortcomings
Results of the AV stimulus that we ran as a conceptual control are tempered by
using different kind of movie cuts regardless of the visual content of the movie
frame after the respective cuts.
% which leads to the ``blurred'' results
Hence we got visual confounds / clusters extending in to early visual areas.
% further studies
Since the capability to isolate clusters or networks correlating with specific
perceptual (or cognitive) processes is influenced by the amount of annotated
stimulus features further studies focusing on visual perception should annotate
and model visual features of naturalistic stimuli more rigorously.
% solution
Future studies that aim to use a movie to localize different visual areas should
extensively annotate the content of frames (e.g. using the open-source solution
``Pliers'' for feature extraction from a visual naturalistic stimulus;
\citep{mcnamara2017developing}) and record the subject's visual attention via
eye-tracking.


\subsubsection{pros \& cons of natural stimuli (esp. narrative) as localizer}





\section{Conclusions}

\todo[inline]{template has no section ``conclusion'' -> use the last paragraph}

% natural stimulation
Intro: Natural stimuli like movies \citep{hasson2008neurocinematics,
sonkusare2019naturalistic} or narratives \citep{honey2012not,
lerner2011topographic, silbert2014coupled} offer an easy to implement,
(continuous,) complex, immersive, task-free paradigm that more closely resembles
our natural dynamic environment than traditional experimental designs.
% why naturalisti stimuli are so cool
(see also \citep{sonkusare2019naturalistic, eickhoff2020towards,hamilton2018revolution})
% re-use of data for independent research question
Due to their continuity, complexity, and length, they offer a vast amount of
data on a variety of brain functions like low-level perception, attention,
comprehension, memory, emotions, or social interactions. This suggests the data
can later be reused for variety of totally different, independent research
questions, possibly on an individual level; increased compliance in individual
diagnostics in ``special'' populations where compliance is more difficult to
archive (pediatric or psychiatric)

% we annotated a couple of features to build contrasts
Method: The feature ``auditory spatial information'' is just one example of a
stimulus feature that can be annotated and used to build a model.
% data-driven vs. model-driven
Why we used a traditional GLM...

% results
Results: Our results demonstrate...

% conclusion: AO and AV stability
In summary, our results across add further evidence that model-driven analyses
can be run on data from naturalistic stimuli to isolate brain activity
correlating with specific perceptual processes.
% but
Nevertheless isolation performance relies on how well the model captures
different aspects of the stimuli's feature space.

% conclusion auditory localizer
Conclusion: Thus, a complex but naturally engaging auditory stimulus like an
audiobook might in principle be used as a non-visual localizer for a variety of
brain functions in subjects who are not willing or able to follow task
instructions during brain scanning.


\subsection*{Code availability}
% wo kommt dieser Abschnitt hin?  how to write Code availability statement
% https://www.springernature.com/gp/authors/research-data-policy/data-availability-statements/12330880
\todo[inline]{provide supporting source code, and explaining how and where
others may access all data underlying the analysis} \emph{for all studies using
    custom code in the generation or processing of datasets, a statement must be
    included here, indicating whether and how the code can be accessed,
including any restrictions to access; include information on the versions of any
software used, if relevant, and any specific variables or parameters used to
generate, test, or process the current dataset. }

\section*{Acknowledgements}
% Text acknowledging non-author contributors. Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.
% Author contributions Please describe briefly the contributions of each author to this work on a separate line.
\emph{COH did this and that.
MH did this and that.
We are grateful to \href{www.florianschurz.de}{Florian Schurz} who initiated doing the annotation of the descriptive nouns, and performed the preliminary annotation of nouns.}

\section*{Competing financial interests}
\emph{A competing financial interests statement is required for all accepted
papers published in \emph{Scientific Data}. If none exist simply write,
``The author(s) declare no competing financial interests''.}

\section*{Figures Legends}
\emph{Figure should be referred to using a consistent numbering scheme through
the entire Data Descriptor. For initial submissions, authors may choose
to supply this document as a single PDF with embedded figures, but
separate figure image files must be provided for revisions and accepted
manuscripts. In most cases, a Data Descriptor should not contain more
than three figures, but more may be allowed when needed. We discourage
the inclusion of figures in the Supplementary Information \textendash{}
all key figures should be included here in the main Figure section.}

\emph{Figure legends begin with a brief title sentence for the whole figure
and continue with a short description of what is shown in each panel,
as well as explaining any symbols used. Legend must total no more
than 350 words, and may contain literature references.}

\section*{Tables}

\emph{Tables supporting the Data Descriptor. These can provide summary information
(sample numbers, demographics, etc.), but they should generally not
be used to present primary data (i.e. measurements). Tables containing
primary data should be submitted to an appropriate data repository.}

\emph{Tables may be provided within the \LaTeX{} document or as separate
files (tab-delimited text or Excel files). Legends, where needed,
should be included here. Generally, a Data Descriptor should have
fewer than ten Tables, but more may be allowed when needed. Tables
may be of any size, but only Tables which fit onto a single printed
page will be included in the PDF version of the article (up to a maximum
of three).}

{\small\bibliographystyle{unsrtnat}
\bibliography{references}}

%\begin{thebibliography}{1}
%\expandafter\ifx\csname url\endcsname\relax
%  \def\url#1{\texttt{#1}}\fi
%\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
%\providecommand{\bibinfo}[2]{#2}
%\providecommand{\eprint}[2][]{\url{#2}}
%
%\bibitem{cite1}
%\bibinfo{author}{Califano, A.}, \bibinfo{author}{Butte, A.~J.},
%  \bibinfo{author}{Friend, S.}, \bibinfo{author}{Ideker, T.} \&
%  \bibinfo{author}{Schadt, E.}
%\newblock \bibinfo{title}{{Leveraging models of cell regulation and GWAS data
%  in integrative network-based association studies}}.
%\newblock \emph{\bibinfo{journal}{Nature Genetics}}
%  \textbf{\bibinfo{volume}{44}}, \bibinfo{pages}{841--847}
%  (\bibinfo{year}{2012}).
%
%\bibitem{cite2}
%\bibinfo{author}{Wang, R.} \emph{et~al.}
%\newblock \bibinfo{title}{{PRIDE Inspector: a tool to visualize and validate MS
%  proteomics data.}}
%\newblock \emph{\bibinfo{journal}{Nature Biotechnology}}
%  \textbf{\bibinfo{volume}{30}}, \bibinfo{pages}{135--137}
%  (\bibinfo{year}{2012}).
%\end{thebibliography}

\section*{Data Citations}

Bibliographic information for the data records described in the manuscript.

1. Lastname1, Initial1., Lastname2, Initial2., ...\& LastnameN, InitialN. \emph{Repository name} Dataset accession number or DOI (YYYY).

\end{document}
\documentclass[english]{article}
