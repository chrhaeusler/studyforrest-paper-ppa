\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{svg}
\usepackage{caption, booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{todonotes}
\usepackage[
	colorlinks=true,
	urlcolor=blue,
	linkcolor=green
]{hyperref}
\newcommand{\scidatalogo}{\includegraphics[height=36pt]{SciData_logo.jpg}}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{40pt}
\lhead{\textsc{\scidatalogo}}
\usepackage{natbib}

\begin{document}

% Titel-Vorgabe < 110 chars incl. spaces
%  jetzt 109 Chars
\title{Hemodynamic responses of the Parahippocampal Place Area to spatial cues in a movie and it's audio-description}

\author{Christian~O.~Häusler\textsuperscript{1,2{*}}, Michael Hanke\textsuperscript{1,2}}
% https://www.nature.com/sdata/publish/for-authors#other-formats

\maketitle
\thispagestyle{fancy}

1. Psychoinformatics Lab, Institute of Neuroscience and Medicine, Brain \& Behaviour (INM-7), Research Centre Jülich, Jülich, Germany,
2. Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University, Düsseldorf, Germany
{*}corresponding author: Christian Olaf Häusler (der.haeusler@gmx.net)

\begin{abstract}
% < 170 words
% new analysis of existing data
% of interest to a broad section of our audience
% highlighting innovative examples of data reuse
% may be used to present compelling new findings & conclusions derived from published data.
Intro: PPA as classic example for an visual area
Methods: fMRI data, studyforrest data set, general linear model
Results: significant clusters in PPA, RSC, individual results in x of 15 subjects
Conclusions: PPA nicht exclusively
\end{abstract}
\todo[inline]{Vorgabe: Intro + Discussion = main text ~ 3,000 words}
\todo[inline]{include figure providing schematic overview of the study and assay(s) design -> is overkill: Steht in den entsprechenden Veröffentlichungen \& Paradigma ist mit "Film ab" wenig kompliziert}
\todo[inline]{our system cannot accept BibTeX bibliography files; authors who wish to use BibTeX to prepare their references should therefore copy the reference list from the .bbl file that BibTeX generates and paste it into the main manuscript .tex file (and delete the associated  \textbackslash{}bibliography and \textbackslash{}bibliographystyle commands); yeah, whatever. Arbeite erstmal mit der references.bib; und habe oben usepackage{natbib}' eingefügt}
\section{Introduction}
Cognitive neuroscientists use brain imaging methods like blood oxygenation level-dependent functional magnetic resonance imaging (BOLD fMRI) to map perceptual and cognitive brain processes (e.g. viewing of landscapes, \citep{epstein1998ppa}; or theory of mind \citep{spunt2014validating}) to functional brain areas or networks. Brain mapping studies have traditionally employed small sets of carefully chosen, simplified stimuli to strictly control experimental conditions and facilitate functional isolation of brain areas.
The disadvantage is that a limited number of simplified stimuli does not resemble our real-life experience and lacks external \citep{westfall2016fixing} as well as ecological validity \citep{hasson2004intersubject}.
Additionally, brain mapping studies have traditionally averaged / usually average results across at least 10-15 subjects to improve the signal-to-noise-ratio (SNR). The disadvantage here is that the averaging approach does not  characterize brain functions at an individual level, a prerequisite for the application of brain imaging methods in individual diagnostics (cf. \citep{dubois2016building})
\todo[inline]{Retrosplenial Cortex kommt in Einleitung (momentan) nicht vor; wird aufgrund der Ergebnisse AV und AO in Discussion diskutiert werden; ist in den geplanten Abbildungen / Slices aber nicht zu sehen}
A classic example of a functional, higher-level visual area is the ``parahippocampal place area'' (PPA) \citep{epstein1998ppa, epstein1999parahippocampal}.
The PPA is located in the posterior parahippocampal gyrus at the boundary to the anterior lingual gyrus and medial fusiform gyrus, both being part of the ventral visual stream \citep{mishkin1982contribution, goodale1992separate}.
Increased hemodynamic activity in the PPA correlates with the perception of static pictures of landscapes or landmarks compared to pictures of tools or faces.
According to the spatial layout hypothesis, the PPA is involved in processing the surface geometry of a visual stimulus and possibly in identifying scenes based on their spatial layout \citep{epstein2010reliable}.
% Überblick
At a group avergae level, results generalize to mental imagination of landscapes \citep{ocraven2000mental} and haptic exploration of scenes constructed from LEGO blocks \citep{wolbers2011modality}.
One study that used spoken, place-related sentences showed significantly decreased activation in the left and no modulation in the right-hemispheric PPA \citep{aziz2008modulation}.
\todo[inline]{Egal, was ich folgend in der Überblick \& später im Detail bzgl. Aziz (2008) schreibe, es wird wahrscheinlich nicht korrekt sein; so wie ich es verstanden habe, hat Aziz den Unterschied zwischen PPA und FFA abgeguckt, aber diesen Unterschied im Kontrast zu object-related Sentences; s. dortige Fig 3; im Grunde kommt da aber überall nur uneinheitliche Grütze raus}

% Im Einzelnen
In the study conducted by \citep{ocraven2000mental} participants viewed alternating blocks of pictures showing famous faces and familiar places during an initial experimental paradigm.
In a subsequent paradigm, participants were instructed to ``form a vivid mental image'' of the previously viewed pictures.
The PPA showed increased activation during imagination of places compared to faces but the imagination tasks showed a smaller activation level compared to the perceptual task.

% Folgend: Wolbers. Er hat noch ne Connectivitäts-Analyse mit dem Occipital Cortex gemacht. Zum visuellen Paradigma ist da dann keine keine Connectivity im Vergleich zum visuellen Paradigma. Seine Interpretation: Es kann keine mental imagery sein:
% "The scene-related increase in coupling with the PPA was significantly stronger in multiple clusters in occipital cortex under visual than haptic stimulation (Figure 2B; Table S2)"
In the block design study conducted by \citep{wolbers2011modality} the PPA of sighted participants showed increased activation during a delayed matching-to-sample task of haptically explored scenes constructed from LEGO bricks compared to haptically explored abstract geometric objects.

% Aziz hat auf eine zweite Mail geantwortet: Sie hat kein Block-Design, sondern modelt die HRF über den jeweils gesamten Satz von ca. 2.8 Sekunden.
% wie oben im Kommentar erwähnt: Ihre Method Section halte ich für etwas strange.Ich zeige dir ganz am Ende mal ihr Paper; hab die entsprechenden Stellen annotiert. Sonst wird hier im Detail Mist erzählt
To our knowledge only one study by \citep{aziz2008modulation} correlated hemodynamic activity of the PPA with spoken sentences. Sentences described generic or famous places (e.g. ``The Taj Mahal faces a long thin reflecting pool'') or faces  (e.g. ``Marylin Monroe has a large square jaw''). Participants were instructed to press a button whenever the sentence described an inaccurate or improbable fact. Surprisingly, activation in the left, but not right, PPA was significantly reduced when participants listened to place-related sentences compared to listening to face-related sentences. Moreover, this effect was only observed in sentences involving famous places.

\todo[inline]{Review von Aminoff (2013) kommt nur kurz als "competing hypothesis"; muss man dann natürlich in der Discussion diskutieren; was aber ggf. gar nicht notwendig ist, weil's darum gar bei uns gar nicht geht}
In summary, studies suggest that the PPA does not exclusively respond to visual(ly) spatial information. This is consistent with a competing hypotheses stating the PPA does not process the spatial layout of a scene specifically but contextual associations in general \citep{aminoff2006parahippocampal} for a review). Independent(ly) from competing theories (s. \citep{aminoff2013role} for a review), it is still unacknowledged/ uncertain/ indeterminate if auditory stimuli lead to inhibitory or excitatory responses, or no change at all in the PPA.
\todo[inline]{Coolnessfaktor von naturalistic stimuli ist beinahe komplett raus; Data-driven vs. theory-driven analyses ist komplett raus -> vgl. Speech-Anno-Paper; jedoch in Discussion bringen (s. dort); ggf. erwähnen, dass möglichst ähnliche und nicht fancy analysis gewählt wurde, um Replizierung der Ergebnisse, aber mit naturalistic stimuli zu gewährleisten.}
All revied studies used a just a small set of carefully designed, simplified stimuli to strictly control experimental conditions. They further relied on / implemented explicit judgment-based tasks to keep participants alert and paying attention. Thus, they cannot / could answer the question how the PPA might behave under life-like conditions.
Naturalistic stimuli like movies \citep{hasson2008neurocinematics, sonkusare2019naturalistic} or narratives \citep{honey2012not, lerner2011topographic, silbert2014coupled} offer an easy to implement, (continuous,) complex, immersive stimulus that better resembles our experience of a continuous dynamic environment.
Two early studies using naturalistic stimuli suggest functional specialization is preserved during free viewing of a movie when many areas respond simultaneously and interact [<- der Schlusssatz ist aus hasson und bartels plagiiert \citep{bartels2004mapping, hasson2004intersubject}

% Bartels (2004) und Hasson (2004)
\citep{bartels2004mapping} manually annotated time points that depict faces and human bodies in a 22 minute clip of the movie \textit{Tomorrow Never Dies}.
Employing a general linear model to model hemodynamic activity, these time points correlated with increased activity in the ``fusiform face area'' \citep{kanwisher1997ffa} and ``extrastriate body area'' \citep{downing2001bodyarea} respectively. Employing a reverse intersubject correlation, \citep{hasson2004intersubject} took used a 30 minute clip of the movie \textit{The Good, the Bad, the Ugly}.
fMRI brain volumes showing the highest hemodynamic activity in the PPA [across (five) participants] were matched to the (temporally) correlating movie frames.
Results revealed that frames depicted indoor and outdoor scenes including buildings and open fields.
Nevertheless, no study investigated how the PPA responds to spatial clues that are embedded in a complex, audio-only stimulus.

% Bzgl. Open Source der der Ergebnisse/Skripte: Dazu habe ich hier nichts geschrieben, weil ich nur weiß, das der Code veröffentlicht werden soll. Du hast die Vision diesbezüglich im Kopf und entsprechend auch kanonisch-prägnante Formulierungen und wo die am besten wie hinkommen

% Präteritum oder Präsens? Ist momentan gemischt; find's ok so.
% N=14 is inner join of AV & AO
\todo[inline]{The next paragraphs contain a bunch of methods already; reduce this to talk about the general approach:
    a) annotation of events,
    b) investigation of model quality,
    c) cross-analysis of datasets.
    d) Possibly cite https://psyarxiv.com/sdbqv/
    and declare that this study is a concrete realization of that promise
}
In this study, we analyze data of three publicly available datasets from the same set of 14 participants as part of the studyforrest data set (\href{http://www.studyforrest.org}{studyforrest.org}).
We exploratorily investigate how the parahippocampal gyrus [or more general ``human brain''?] responds to ``spatially relevant'' events embedded in a 2h audio-visual movie \citep{hanke2016simultaneous} and its audio-description of the same length \citep{hanke2014audiomovie}.
We compare these results to PPA localizations that were obtained from a dedicated visual localizer experiment \citep{sengupta2016extension}.
Both natural stimuli were originally designed to entertain an audience and to capture attention free from any perceptual or behavioral task.
We annotated events in the otherwise unknown temporal structure of both stimuli.
In/for the movie, we annotated different kinds of cinematographic cuts \citep{haeusler2016annotation} as presumably ``spatially relevant'' events.
Our rationale was that cuts re-orient the viewer in the depicted environment independent from the exact visual content in the movie frame following the cut.
In/for the audio-description, we annotated verbal clues about the missing visual content given by the audio-description's additional narrator [citep{haeusler2020speechanno}].
In conformity to the analyses performed in the studies using not-naturalistic stimuli, we employed a standard multi-level, voxel-wise general linear model (GLM) % in order to???
% Practical reasons = Es war viel einfacher nouns der audio-description als Frameinhalt des Films zu annotieren
For practical reasons, and encouraged by \citep{aziz2008modulation} and \citep{aminoff2013role}, we focus on the audio-description as a possible life-like stimulus free from any task to localize a ``visual area'' in individual study participants.


\section{Methods}
\todo[inline]{Specific data outputs should be explicitly referenced via data citation (see Data Records and Data Citations, below)}
\todo[inline]{MIH: we should consider making a statement how often these datasets have been analyzed be independent groups already -> Frage: ist die Liste auf psychoinformatics.de ist aktuell/erschöpfend? Ist dann schnell eingebaut}
\todo[inline]{entsprechend der neuen Infos hier, oben das Ende der Einleitung kürzen}
We used data from three publicly available datsets as part of the studyforrest dataset (\href{http://www.studyforrest.org}{studyforrest.org}) that has already been used by other research groups for independent research questions before. We used fMRI data from 14 subjects who took part in three different studies. Namely the same subjects were
a) listening to the audio-description (AO study) \citep{hanke2014audiomovie}) of the movie ``Forrest Gump'', a dataset already used by [XY].
b) watching the actual audio-visual movie (AV study) \citep{hanke2016simultaneous}, a dataset already used by [XY].
c) participating in a dedicated block-wise visual localizer (LOC study) \citep{sengupta2016extension}, a dataset aready used by [XY]. An exhaustive description of the participants, stimulus creation, procedure, stimulation setup, and fMRI acquisition can be found in the corresponding publications  \citep{hanke2014audiomovie, hanke2016simultaneous, sengupta2016extension}. Following is a brief summary of most important aspects.
% Fokus auf die Naturalistic Stimuli; LOC study ist effektiv nicht mit drin


\subsection{Participants}
% ich habe von den aligned data eine Intersection of 14 VPN von AO und AV
% in der Speech-Anno-Validation (AO only) habe ich 19 VPN
% es fehlt jeweils VP10
In the AO study, twenty [+++also, ich habe nur Daten von 19 benutzt; VP10 hat keine (aligned) Daten+++] participants (all right-handed, age 21–38 years, mean age 26.6 years, 12 male) listened to the German audio-description \citep{ForrestGumpGermanAD} of the movie ``Forrest Gump'' \citep{ForrestGumpMovie} as an additional audio track for visually impaired listeners on Swiss public television.
In the AV study, the same 15 participants [+++VP 10 wurde entpsrechend nicht genutzt+++] (21–39 years, mean age 29.4, six female) watched the audio-visual movie with dubbed German audio track \citep{ForrestGumpDVD}.
% LOC study einarbeiten
In the LOC study [].

All participants reported to have normal hearing, normal or corrected-to-normal vision, and no known history of neurological disorders.
% mit LOC study updaten
In both studies, participants received monetary compensation and gave written informed consent for their participation and for public sharing of all obtained data in anonymized form. The studies had prior approval by the Ethics Committee of Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli}
The movie's  \citep{ForrestGumpMovie} audio track and the audio-description \citep{ForrestGumpGermanAD} are largely identical.The plot is carried by an off-screen voice of the main character Forrest Gump.
The audio-description's male narrator additionally verbalizes essential aspects of the visual scenery when there is no dialog, off-screen voice or relevant sound in the audio track.
The audio-description was temporally aligned to the audio track of the German DVD release. A few scenes less relevant to the major plot were removed to create the ``research cut'' lasting $\approx$2h. Both stimuli were further processed (filtering, volume adjustments) to improve understandability / audibility during MRI scanning.
Finally, each stimulus was split into eight segments of approximately 15 minutes. Except for the first movie segment, each segment started with a snippet of at least six seconds immediately preceding the movie scene boundary used to split the segments (s. Figure 3a in \citep{hanke2014audiomovie}
% LOC study?


\subsection{Stimulus Annotation}
\todo[inline]{einleitendes Blabla zu haeusler2020speechanno}
\todo[inline]{allg. überarbeiten; deutlich machen, WARUM WAS WIE annotiert wurde}
% Florian hat 2 Regeln falsch verstanden, sonst war übereinstimmung 90-95%
% Kategorie ``++'' enthält ausnahmesweise auch adverbiale Best. der Zeit.
% ggf. angeben, dass die effektiv identisch waren
\todo[inline]{aus Speech-Anno Paper 'descriptive-statistics-anno.py' übernehmen \& anpassen, um Statistiken und runcommand-file zu erstellen}
A further annotation of the audio-description's narrator was done manually: Two
persons performed a categorization of nouns embedded in sentences spoken by the
narrator to describe the movie's missing visual content.
Categories focus on the cinematographic scene's environment, inherent objects, persons, and a person's appearance.
A categorizing of nouns that serve as auditory cues about the movie's visual content. Comprised categories focus on the cinematographic scene's environment (geo, geo-room; setting\_new, setting\_old), inherent persons (female, female name; male, male name, person), their appearance (face, head; body, bodypart), and objects (object, furniture).
A preliminary annotation according to the rules [entweder im laufenden Text oder in Tabelle], einer neuen Tabelle oder im laufenden Text was done by one person and corrected in several runs by the author.

% Tabelle der Descriptive Nouns;
\todo[inline]{wenn decriptive-statistics-anno.py angepasst und Datei mit newcommands geschrieben, Schrägstrich vor die jeweiligen Platzhalter setzen, damit commands daraus werden; Caption hübsch machen}
\begin{table*}[t]
    \caption{Narrator's descriptive nouns. All categories sorted alphapetically. Examples are given in English. Tabelle enthält bei den Counts Platzhalter für die commands, die ins .tex-file geschrieben werden.}
\label{tab:descr-nouns}
\begin{tabular}{llllllllllll}
\toprule
\textbf{category} & \textbf{rule} & \textbf{examples} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & trunk of the body; overlaid clothes & back, hip, shoulder; jacket, dress shirt & aDescrBodyAll & aDescrBodyI & aDescrBodyII & aDescrBodyIII & aDescrBodyIV & aDescrBodyV & aDescrBodyVI & aDescrBodyVII & aDescrBodyVIII \tabularnewline
bodypart & limbs and trousers & arm, finger, leg, toe & aDescrBodypartAll & aDescrBodypartI & aDescrBodypartII & aDescrBodypartIII & aDescrBodypartIV & aDescrBodypartV & aDescrBodypartVI & aDescrBodypartVII & aDescrBodypartVIII \tabularnewline
face & face or parts of it & face, ear, nose, mouth & aDescrFaceAll & aDescrFaceI & aDescrFaceII & aDescrFaceIII & aDescrFaceIV & aDescrFaceV & aDescrFaceVI & aDescrFaceVII & aDescrFaceVIII \tabularnewline
female & female person & nurse, mother, woman & aDescrFemaleAll & aDescrFemaleI & aDescrFemaleII & aDescrFemaleIII & aDescrFemaleIV & aDescrFemaleV & aDescrFemaleVI & aDescrFemaleVII & aDescrFemaleVIII \tabularnewline
females & female persons & women & aDescrFemalesAll & aDescrFemalesI & aDescrFemalesII & aDescrFemalesIII & aDescrFemalesIV & aDescrFemalesV & aDescrFemalesVI & aDescrFemalesVII & aDescrFemalesVIII \tabularnewline
fname & female name & Jenny & aDescrFnameAll & aDescrFnameI & aDescrFnameII & aDescrFnameIII & aDescrFnameIV & aDescrFnameV & aDescrFnameVI & aDescrFnameVII & aDescrFnameVIII \tabularnewline
furniture & movable objects insides/outsides & bench, bed, table, chair & aDescrFurnitureAll & aDescrFurnitureI & aDescrFurnitureII & aDescrFurnitureIII & aDescrFurnitureIV & aDescrFurnitureV & aDescrFurnitureVI & aDescrFurnitureVII & aDescrFurnitureVIII \tabularnewline
geo & immobile landmarks & building, tree, street, alley, meadow, cornfield, river & aDescrGeoAll & aDescrGeoI & aDescrGeoII & aDescrGeoIII & aDescrGeoIV & aDescrGeoV & aDescrGeoVI & aDescrGeoVII & aDescrGeoVIII \tabularnewline
geo-room & rooms / locales; elements defining a locale's spatial layout & living room; wall, door, window, floor, turf & aDescrGeoroomAll & aDescrGeoroomI & aDescrGeoroomII & aDescrGeoroomIII & aDescrGeoroomIV & aDescrGeoroomV & aDescrGeoroomVI & aDescrGeoroomVII & aDescrGeoroomVIII \tabularnewline
head & non-face parts of the head; worn headgear & head, hair, ear, neck, helmet & aDescrHeadAll & aDescrHeadI & aDescrHeadII & aDescrHeadIII & aDescrHeadIV & aDescrHeadV & aDescrHeadVI & aDescrHeadVII & aDescrHeadVIII \tabularnewline
male & male person & man, father, soldier & aDescrMaleAll & aDescrMaleI & aDescrMaleII & aDescrMaleIII & aDescrMaleIV & aDescrMaleV & aDescrMaleVI & aDescrMaleVII & aDescrMaleVIII \tabularnewline
males & male persons & boys, opponents & aDescrMalesAll & aDescrMalesI & aDescrMalesII & aDescrMalesIII & aDescrMalesIV & aDescrMalesV & aDescrMalesVI & aDescrMalesVII & aDescrMalesVIII \tabularnewline
mname & male name & Bubba, Kennedy & aDescrMnameAll & aDescrMnameI & aDescrMnameII & aDescrMnameIII & aDescrMnameIV & aDescrMnameV & aDescrMnameVI & aDescrMnameVII & aDescrMnameVIII \tabularnewline
object & countable entity with firm boundarie & telephone, car & aDescrObjectAll & aDescrObjectI & aDescrObjectII & aDescrObjectIII & aDescrObjectIV & aDescrObjectV & aDescrObjectVI & aDescrObjectVII & aDescrObjectVIII \tabularnewline
objects & countable entities with firm boundaries &  wheels, photos & aDescrObjectsAll & aDescrObjectsI & aDescrObjectsII & aDescrObjectsIII & aDescrObjectsIV & aDescrObjectsV & aDescrObjectsVI & aDescrObjectsVII & aDescrObjectsVIII \tabularnewline
persons & concrete persons of unknown sex / gender & hippies, patients & aDescrPersonsAll & aDescrPersonsI & aDescrPersonsII & aDescrPersonsIII & aDescrPersonsIV & aDescrPersonsV & aDescrPersonsVI & aDescrPersonsVII & aDescrPersonsVIII \tabularnewline
setting\_new & setting mentioned for the first time &  on a ``bridge'', on an ``alley'', on ``campus'' & aDescrSettingnewAll & aDescrSettingnewI & aDescrSettingnewII & aDescrSettingnewIII & aDescrSettingnewIV & aDescrSettingnewV & aDescrSettingnewVI & aDescrSettingnewVII & aDescrSettingnewVIII \tabularnewline
setting\_rec & recurring setting & at the ``bus stop'' & aDescrSettingrecAll & aDescrSettingrec & aDescrSettingrecII & aDescrSettingrecIII & aDescrSettingrecIV & aDescrSettingrecV & aDescrSettingrecVI & aDescrSettingrecVII & aDescrSettingrecVIII \tabularnewline
++ & nouns, adverbial adjectives and adverbs & in the ``evening'', it's ``daytime'', ``later'' &  aDescrAll & aDescrI & aDescrII & aDescrIII & aDescrIV & aDescrV & aDescrVI & aDescrVII & aDescrVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsection{Procedure}
In both studies, participants filled out a questionnaire on their basic demographic information and familiarity with the audio-description or the movie.
Participants were instructed to inhibit physical movements except for eye-movements, and otherwise to simply "enjoy the audiobook" or "enjoy the movie" respectively.
Audio-description and movie segments were presented in chronological order with four segments in two fMRI sessions respectively.
Between sessions, participants left the scanner for a break with a flexible duration. Structural images were obtained during the first study on a day different from the fMRI session.
% At the start (of each?) recording session, participants listened to music from the movie's closing credits segment to individually determine optimal stimulus volume.
% Participants were instructed to ``maximize the volume of the stimulus without it becoming unpleasantly loud or causing acoustic distortions by overdriving the loudspeaker hardware''.


\subsection{Stimulation setup}
\todo[inline]{irrelevantes kürzen}
In both studies, stimulation was implemented with PsychoPy v1.79 (with an early version of the MovieStim2 component later to be publicly released with \href{http://www.psychopy.org}{PsychoPy v1.81}; \citep{peirce2007psychopy}) running on a computer with the \href{http://neuro.debian.net}{(Neuro)Debian} operating system \citep{halchenko2012open}.
In the AO study, visual instructions were presented on a rear-projection screen inside scanner bore using an LCD projector (DLA-G150CL, JVC Ltd.).
During the functional scans, the projector presented a medium gray screen with the primary purpose to illuminate a participant’s visual field in order to prevent premature fatigue.
[The screen contained a solid black ``fixation'' dot that faded in and out at the beginning and end of a movie segment.]
In the AV study, visual instructions and stimuli were presented on a rear-projection screen using an LCD projector (JVC DLA RS66E, JVC Ltd., light transmission reduced to 13.7\% with a gray filter) connected to the stimulus computer [via a DVI extender system (Gefen EXT-DVI-142DLN with EXT-DVI-FM1000).
The movie was shown at a viewing distance of 63\,cm in 720\,p resolution at full width on a 1280 *1024\,pixel screen with 60\,Hz video refresh rate and screen dimension of 26.5 * 21.2\,cm, corresponding to 23.75*13.5 or 23.75*10.25 when considering only the movie content and excluding the horizontal gray bars].
In both studies [???] auditory stimulation was delivered through an MR confon mkII+ driving custom-built in-ear headphones (HP-M01, MR confon GmbH, Magdeburg, Germany; \citep{baumgart1998electrodynamic}) that reduced the scanner noise by at least 20–30 dB.
Headphones were  fed from an Aureon 7.1 USB (Terratec) sound card through an optical connection.


\subsection{fMRI data acquisition}
\todo[inline]{irrelevantes kürzen}
In both studies, a total of 3599 volumes were recorded for each participant (451, 441, 438, 488, 462, 439, 542 and 338 volumes for movie segment 1–8 respectively).
In AO study, a whole-body 7-Tesla Siemens MAGNETOM magnetic resonance scanner equipped with a local circularly polarized head transmit and a 32 channel brain receive coil (Nova Medical, Inc., Wilmington, MA, USA) acquired T2*-weighted echo-planar images (gradient-echo, 2\,s repetition time (TR), 22\,ms echo time, 0.78\,ms echo spacing, 1488\,Hz/Px bandwidth, generalized autocalibrating partially parallel acquisition, acceleration factor 3, 24 Hz/Px bandwidth in phase encoding direction) using 36 axial slices (thickness 1.4\,mm, 1.4 x 1.4\,mm in-plane resolution, 224\,mm field-of-view, anterior-to-posterior phase encoding direction) with a 10 percent inter-slice gap were recorded in ascending order.
Slices were oriented to include the ventral portions of frontal and occipital cortex while minimizing intersection with the eyeballs. The field-of-view was centred on the approximate location of Heschl's gyrus.
[EPI images were online-corrected for motion and geometric distortions [Oh, S. et al. 2012; In, M. \& Speck, O., 2012; Chung, J. et al., 2011+++]. Auxiliary scans for slice alignment and motion- and distortion-correction were performed at the beginning of the first fMRI recording session and also after the break at the start of the recording for the second half of the movie].
In the AV study, a whole-body 3 Tesla Philips Achieva dStream MRI scanner equipped with a 32 channel head coil acquired T2*-weighted echo-planar images (gradient-echo, 2\,s repetition time (TR), 30\,ms echo time, 90 DEGREE flip angle, 1943\,Hz/Px bandwidth, parallel acquisition with sensitivity encoding (SENSE) reduction factor 2).
35 axial slices (thickness 3.0\,mm, 10\% inter-slice gap) with 80*80 voxels (3.0*3.0 mm of in-plane resolution), 240\,mm mm field-of-view (FoV) and an anterior-to-posterior phase encoding direction were recorded in ascending order.
Philips' ``SmartExam'' was used to automatically position slices in AC-PC orientation such that the topmost slice was located at the superior edge of the brain.


\subsection{Preprocessing}
% DATA SOURCE
\todo[inline]{hier ggf. expliziter erwähnen, dass AO spatially downsampled ist}
% denoised movie data:
% https://github.com/OpenNeuroDatasets/ds001769/tree/master/sub-01/ses-movie/func
fMRI time series data were obtained from
\href{''https://github.com/psychoinformatics-de/studyforrest-data-aligned''}{GitHub}. Data are already aligned to a subject-specific \href{''https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms''}{3 Tesla BOLD template} by a rigid body transformation in MCFLIRT \citep{jenkinson2002registration} to correct for motion during scanning
\citep{sengupta2016extension}.

\todo[inline]{cite sengupta2016 where this was first done and described, ie same image/data was used here}

All further analysis steps in the current study were carried out using tools of FSL v5.0.9 (\href{''https://www.fmrib.ox.ac.uk/fsl''}{FMRIB's Software Library};\citep{smith2004fsl}; on a computer cluster running the \href{https://www.debian.org}{Debian} GNU/Linux operating system.
Software packages were obtained from repositories of \href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.

% ACTUAL PREPROCESSING
Preprocessing (and statistical analysis) was carried out using FEAT v6.00 (FMRI Expert Analysis Tool;\citep{woolrich2001autocorr}) as part of FSL.
High-pass temporal filtering was applied to every segment using a Gaussian-weighted least-squares straight line with a cutoff period of 150\,s (sigma=75.0\,s) to remove low-frequency confounds.
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
Data were spatially smoothed applying a Gaussian kernel with full width at half maximum (FWHM) of 4.0 mm.
A grand-mean intensity normalization of the entire 4D dataset was performed by a single multiplicative factor.
Correction for local autocorrelation in the time series (prewhitening) was applied using FILM (FMRIB’s Improved Linear Model).

\subsection{Statistical analysis}
A standard two-level general linear model (GLM) analysis was conducted in FEAT to create subject-specific results ($Z$-maps). A subsequent, third-level analysis averaged contrast estimates over subjects.

\subsubsection{first-level}
\paragraph{Regressors \todo{stimulus/statistical model?}}
To build the regressors for the GLMs, we took advantage of two publicly available annotations of the stimuli's temporal structure.
One annotation of speech occurring in the audio-description and movie [haeusler2020speechanno], and a second annotation of movie cuts and depicted locations \href{https://f1000research.com/articles/5-2273}{haeusler2016annotation}.
As a first step, the annotations were split via (\href{"https://github.com/chrhaeusler/studyforrest-data-annotations/blob/master/code/researchcut2segments.py"}{python script} into eight parts corresponding to the eight segments used in the scanner.
% wenn ich folgend den "null regressor" nehme, darf ich den ganzen Run - glaube ich - in der 2nd level Analyse gar nicht benutzen??
For the auditory stimulus, we used nouns embedded in sentences spoken by the narrator. The nouns are categorized by the clue they provide about a cinematographic scene's environment \todo{maybe skip the names and just cite the table} (geo, geo-room; setting\_new, setting\_rec), its inherent persons (female, female name; male, male name, person), a person's appearance (face, head; body, bodypart), and a scene's inherent objects (object, furniture).
% nope, kommt jetzt hier
Details on procedure to annotate the stimuli on a single-world level and details on the rules to manually categorize the nouns can be found in the \href{"https://www.overleaf.com/project/5d4ab1b759001b5db6ea2bc3"}{corresponding paper}.
We created 11 regressors from the XY (selected?) original categories by joining / merging some categories (female, females, fname; male, males, mname; face, head) into one regressor (sex\_f; sex\_m; fahead).
% over length of each noun
The regressors were modeled as boxcar functions at the end of each noun with a duration of 200 ms.
If a segment did not contain a single event for a specific regressor, a null regressor was chosen for that segment.
Five regressors related to the cinematographic cuts (s. below) were added to built contrasts of cross-modal negative control.
Two regressors reflecting auditory (low-level) features (left-right loudness volume / amplitude??? difference; root mean squared amplitude) aligned to the frame rate of (40\,ms) were modeled to reduce nuisance effects.
An overview and short explanation of the 11 regressors and 5 + 2 additional nuisance regressors can be found in Table xy.
% entgegen der Masterarbeit kommen hier für den Film keine Abbildungen zur Verdeutlichung, weniger Text; no cut erkläre ich nicht lang & breit
% die regressoren settinge_new & setting_old aus dem Hörspiel habe ich meiner meinung nach unnötigerweise im Film mit-modelliert; lasse mich da aber auch umstimmen
% -> noch mal reingucken; kamen im Grunde nur "auditorische Areale" als Cluster vor; umgekehrt gab's bei der visuellen Regressoren im Hörbuch keine sign. Cluster
For the audio-visual stimulus, we categorized the depicted location in the frame following one of the 869 cuts in the movie to built five regressors: 1) unfamiliar setting (US), 2) familiar setting (FS), 3) locale switch (LS), 4) unfamiliar perspective (UP), 5) familiar perspective (FP).
In order to build negative control contrasts, a sixth regressors was created by pseudo-randomly choosing frames present within a shot and apart from a cut. Regressors were modeled as boxcar function lasting 200\,ms after a cut.
An overview and short explanation of the six regressors and the additional nuisance regressors reflecting (auditory and) visual low-level features can be found in Table xy.
For both stimuli, regressors were convolved with a double gamma canonical hemodynamic response function (HRF).
Their temporal derivatives were added to compensate for small timing differences \citep{friston1998event}.
Six motion parameters were used as additional nuisance regressors. Finally, designs were filtered with the same high-pass filter (cutoff period of 150\,s) as the BOLD time series.

\begin{table*}
\caption{Regressors used in the analysis of the audio-description. \todo[inline]{make clear which stimulus each annotation is based on, and make clear(er) what defined an event. This could be a big table, because good defintions are important for judging the results}}
\begin{tabular}{lll}
\toprule
\textbf{name} & \textbf{count} & \textbf{description} \\
\midrule
body & 66 & trunk of the body or overlaid clothes \\
bpart & 69 & limbs and trousers \\
facehead & 83 & face(parts) or head(parts) \\
furniture & 50 & big inventory of rooms (insides/outsides) \\
geo & 125 & immobile landmarks \\
geo-room & 105 & elements defining a locale's spatial layout \\
setting\_new & 86 & first-time mentioned setting \\
setting\_old & 37 & recurring setting \\
object & 284 & inanimate entities with firm boundaries \\
sex\_f & 108 & female person(s), name \\
sex\_m & 403 & male person(s), name \\
vse\_new & 96 & control for movie cuts; s. table y \\
vse\_old & 90 & control for movie cuts; s. table y \\
vloc\_change & 89 & control for movie cuts; s. table y \\
vpe\_new & 386 & control for movie cuts; s. table y \\
vpe\_old & 208 & control for movie cuts; s. table y \\
fg\_ad\_lrdiff & xx &  left-right volume difference \\
fg\_ad\_rms & xx & root mean square amplitude \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}
\caption{Regressors used in the analysis of the movie.}
\begin{tabular}{lll}
\toprule
\textbf{name} & \textbf{count} & \textbf{description} \\
\midrule
vse\_new & 96 & change of the camera position to a setting not depicted before \\
vse\_old & 90 & change of the camera position to a recurring setting \\
vloc\_change & 89 & change of the camera position to another locale within the same setting \\
vpe\_new & 386 & change of the camera position within a locale not depicted before \\
vpe\_old & 208 & change of the camera position within a recurring locale \\
no\_cut & 148 & a continuous frame from the same camera position \\
se\_new & 148 & audio-description's narrator; s. table x; KANN RAUS? \\
se\_old & 148 & audio-description's narrator; s. table x; KANN RAUS? \\
fg\_av\_ger\_lr & xx & left right +++???+++ in movie frame  \\
fg\_av\_ger\_lrdiff & xx & left-right volume difference \\
fg\_av\_ger\_pd & xx & perceptual differences of consecutive frames \\
fg\_av\_ger\_rms & xx & root mean square amplitude \\
fg\_av\_ger\_ud & xx & +++up down???+++ \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{audio contrasts of parameter estimate}
% Überblick über die Kontraste geben, die tatsächlich am Ende im Paper berichtet werden -> am besten telefonisch klären
% HIER ERZÄHLEN, WARUM VIELE KONTRASTE GERCHNET WURDEN und WARUM DER EINE COPE THE ONE AND ONLY IST

\begin{table*}
\caption{Contrasts of parameter estimates for the analysis of the audio-description.}
% das sind die zusammengefassten Kurzversionen, d.h. nicht alle Regressoren sind explizit in dem COPEs ausformuliert
\begin{tabular}{lll}
\toprule
\textbf{COPE nr.} & \textbf{description} & \textbf{aim} \\
\midrule
1 & geo, rgeo, se\_new, se\_old > non\_geo & PPA \\
2 & geo, rgeo, se\_new > non\_geo & PPA \\
3 & geo, rgeo > non\_geo & PPA <<-- THE COOL \\
4 & geo > non\_geo & PPA \\
5 & se\_new > non\_geo & PPA \\
6 & se\_new, se\_old > non\_geo & PPA \\
7 & se\_new > se\_old & PPA \\
8 & body > all\_others (excl. bpart) & EBA \\
9 & body, bpart > all\_others & EBA \\
10 & body > non\_persons (excl. bpart) & EBA \\
11 & body, bpart > non\_persons & EBA \\
12 & body, bpart > non\_persons (excl. faHead) & EBA \\
13 & fahead > all\_others & FFA \\
14 & fahead > all\_others (excl. body/parts) & FFA \\
15 & fahead > non\_persons & FFA \\
16 & fahead > non\_persons (excl. body/parts) & FFA \\
17 & obj > all\_others & LOC \\
18 & obj > all\_others (excl. fahead) & LOC \\
19 & obj > all\_others (excl. fahead, rgeo) & LOC \\
20 & obj > persons & LOC \\
21 & se\_old > se\_new & DMN \\
22 & se\_old > non\_settings & DMN \\
23 & se\_old > non\_geo & DMN \\
24 & female > male & ??? \\
25 & male > female & ??? \\
26 & persons > non\_persons (excl. body/parts) & ??? \\
27 & non\_persons (excl. body/parts) > persons & ??? \\
28 & se\_new > se\_old & visual contr. \\
29 & pe\_new > pe\_old & visual contr. \\
30 & se\_new, pe\_new > se\_old, pe\_old & visual contr. \\
31 & se\_new > pe\_old & visual contr. \\
32 & se\_new > se\_old, pe\_old & visual contr. \\
33 & se\_new, pe\_new > pe\_old & visual contr. \\
34 & se\_old > se\_new & DMN  contr. \\
35 & pe\_old > pe\_new &  DMN contr. \\
36 & no\_cut > pe\_new, pe\_old & neg. control \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{video contrasts of parameter estimate}
% Überblick über die Kontraste geben, die tatsächlich am Ende im Paper berichtet werden
% ggf. irgendwo noch reinpressen, dass man damit fast alle schnitte in FG benutzt hat
We created a contrast presumably correlating with increased hemodynamic activity in the PPA by contrasting different kind of cuts. This was done without controlling for the specific visual content depicted in the frame after each cut. The cinematographic rationale was that - over the course of a movie - cuts shift from establishing the setting and the spatial relationships within a setting at the beginning of a movie to depicting persons and objects more relevant to the subsequently evolved narrative later in the movie {brown2012cinematography; mercado2011filmmakers}. The neurocientific rationale was that activity in the PPA is greater when participants view novel versus repeated scenes resp. view-points \citep{epstein1999parahippocampal}. Hence, we contrasted cuts to a setting occurring for the first time (vse\_new) and cuts within a locale depicted for the first time (vpe\_new) with shots to a recurring setting (vse\_old) and cuts within a recurring locale (vpe\_old).

\begin{table*}
\caption{COPEs for the movie.}
\begin{tabular}{lll}
\toprule
\textbf{COPE nr. } & \textbf{description} & \textbf{aim} \\
\midrule
1 & geo, rgeo, se\_new, se\_old > non\_geo & PPA \\
1 & se\_new > se\_old & PPA \\
2 & pe\_new > pe\_old & PPA \\
3 & se\_new, pe\_new > se\_old, pe\_old & PPA (the true and cool one) \\
4 & se\_new > pe\_old & PPA \\
5 & se\_new > se\_old, pe\_old & PPA \\
6 & se\_new, pe\_new > pe\_old & PPA \\
7 & pe\_new > se\_old, pe\_old & PPA \\
8 & se\_old > se\_new & DMN \\
9 & pe\_old > pe\_new & DMN \\
10 & no\_cut > se\_new, se\_old & neg. control \\
11 & no\_cut > se\_new & neg. control \\
12 & no\_cut > se\_old & neg. control \\
13 & no\_cut > pe\_new, pe\_old & neg. control \\
14 & ao: se\_new > se\_old & audio control \\
15 & ao: se\_old > se\_new & audio control \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Registration \& smoothing: individuals vs. group}
% hier wird's wie üblich bei Registrierung;
% bitte gucken, ob korrekt wiedergeben, was ihr gemacht habt
% wie besprochen sind hier die individuelle Results im subject space
% später warpe ich individuelle 2nd-lvl-results "for visualization purposes" doch noch nach MNI
The GLM analysis that fitted each voxel's time course separately for each subject was performed in functional space preserving the orientation of the EPI images. Correction for local autocorrelation in the time series (prewhitening) was applied using FILM (FMRIB's Improved Linear Model) to improve estimation efficiency \citep{woolrich2001autocorr}.
For subsequent subject-specific second level results, subject-specific BOLD T2* time series were anatomically aligned via linear transformation to a subject-specific, study-specific T2* template (s. \href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/sub-01/bold3Tp2/brain.nii.gz"}{example}) that served as standard space.
For subsequent the third level results averaging contrasts of interest across subjects, time series were anatomically aligned via non-linear transformation [+++wie genau?+++] to a \href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/templates/grpbold3Tp2/brain.nii.gz"}{study-specific T2* group template}.
To facilitate visualization and identification of cortical structures, this reference images was later co-registered to the MNI 152 template with an affine transformation (12 degrees of freedom)
\subsubsection{second level}
The second-level analyses which averaged contrast estimates over the eight stimulus segments were carried out using a fixed effects model, by forcing the random effects variance to zero in FLAME (FMRIB's Local Analysis of Mixed Effects) \citep{beckmann2003general, woolrich2004multilevel}. Z (Gaussianised T/F) statistic images were thresholded using clusters determined by Z>2.3 and a (corrected) cluster significance threshold of p=.05 \citep{worsley2001statistical}.
\subsubsection{third level}
Third level analyses which averaged contrast estimates over subjects were carried out using FLAME stage 1 with automatic outlier detection \citep{beckmann2003general, woolrich2004multilevel, woolrich2008robust}. Z (Gaussianised T/F) statistic images were thresholded using clusters determined by Z>3.4 and a (corrected) cluster significance threshold of p=.05 [Worsley, 2001].
% Brain regions associated with observed clusters were determined with the Juelich Histological Atlas Eickhoff et al. (2005); Eickhoff et al. (2007), the Harvard-Oxford Cortical Atlas (Desikan et al., 2006), and with queries on the NeuroSynth database (Yarkoni et al., 2011) provided by FSL.
% In order to determine if activated regions were spatially congruent with higher-visual functional areas (PPA, FFA, OFA, LOC and EBA), study-specific ROI masks generated by the mean of visual localizers in Sengupta et al. (2016) were used.

\subsection{Das Post-FSL-Scripting-Gedöns}
% Wahrscheinlicher ist, dass das Prozedere der Skripte nicht im Methodenteil berichtet werden muss
% als Erläuterung in den Bildunterschriften im Ergebnisteil sollten reichen
% -> die Skripte durchlaufen lassen und die Abbildungen in den Ergebnisteil samt Bildunterschriften einfügen
\subsubsection{plot-group-and-individuals.py}

\subsubsection{plot-bland-altman.py}

% generate-masks.py; DO NOT TOUCH!
% FOR GROUP:
% 0. In FSLeyes, manually created probabilistic brain lobe maps from \href{"https://doi.org/10.5281/zenodo.1470761"}{the MNI Structural Atlas} (./data/atlases/mni_prob_*.nii.gz)
% 1. these brain lobe maps were aligned (via FSL's flirt -applyxf with 12 dof) to the \href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/templates/grpbold3Tp2/brain.nii.gz"}{study-specific T2* group template} (./data/atlases/grpbold3Tp2_mni_prob_*.nii.gz)
% 2. from \href{"https://github.com/psychoinformatics-de/studyforrest-data-visualrois"} {sengupta2016extension}: UNIlateral probabilistic group ROI masks (data/masks/*_overlap.nii.gz) -> BIlateral probabilistic & binary grp ROI masks in grp space (.data/masks/bilat_???_prob.nii & .data/masks/bilat_???_binary.nii )
% FOR SUBJECTS:
% 1. bilateral grp ROI masks in grp space -> bilateral grp ROI masks in subject space (applywarp; sub-??/masks/grp_*_bin.nii.gz)
% 2. probabilistic MNI mask -> subject space (via FSL's applywarp)
% 3. merge probabilistic occipital & temporal mask in subject space
%	(sub-??/masks/mni_prob_occip_tempo.nii.gz)
% 4. created subject-specific FOV from 4d 7t audio drama time series
%	(./sub-??/masks/ao_fov_mask.nii.gz)
% 5. individual, unilateral ROIs from {sengupta2016extension} were joined and co-registered to MNI 152 (1mm) space, and binarized
% 6. individual (un)thresholded zmaps (from the COPEs) were co-regi<wstered with MNI 152 (1mm) template.
\section{Results (audio)}
% vgl. mit Ergebnis-Teilen anderer Localizer (in wievielen VPN funktioniert's etc.)
% https://en.wikipedia.org/wiki/Bland%E2%80%93Altman_plot

\section{Results (movie)}
lore ipsum

% MIH the discussion should primarily be anchored on the points the introduction concludes with. From my POV there is no need (and likely no space) to discuss results by stimulus type. Issue like different impact of attentional focus etc can be brought up in the joint paragraph that discussed how such a factor could have differentially impacted the results -- and if that would matter.
\section{Discussion (audio)}
% Hanke et al. (2014): Compared to an audio-visual movie, this is a stimulus that leaves a much larger margin for inter-individual differences in imagining scenery, as well as actors' character and personality traits, while still preserving the time-locked presentation of information to a listener. At the same time, an auditory stimulus limits the effect of an attentional focus on the selection of a subset of simultaneously occurring auditory events, in contrast to the selection of different parts of the visual field.
% mental imagery hypthesis (s. O'Craven \& Kanwisher, 2000)
% Lerner (2011), Honey (2012), Silbert (2014) finden keine ISC in PPA? vgl. DataPaper bzgl. Hörbuch??
% setting\_new ist häufig vermischt mit anderer Kategorie "ein Arzt", "ein alter Film", "auf einer Straße", "im Fernsehen", "die drei Jungs", "die zwei im Bett", "Jenny als Teenager", "Forrest vor einem Buffet", "Kennedy", "Jenny kommt aus dem Wohnwagen ihrer Großmutter"
% Soundscape macht es häufiger bereits vorher klar, dass Settingwechsel ist bzw. nicht erst das annotiere Substantiv
% object und (room)geo ist vermischt
% Retrospelinal Cortex ist in Probabilistic maps krasser als PPA.

\section{Discussion (movie)}
\begin{itemize}
\item lore ipsum
\end{itemize}


\section*{General Discussion}
% Data-driven vs. Analysis-Driven
% "Abfall" einer Film-Messung
% how do results (PPA-like response localization) differ from a standard visual/block-design localizer for:
% feasibility of (multi-dimensional) localizer development with naturalistic stimuli
% Story für parietale Aktivierung im Film -> räuml. Orientierung / Aufmerksamkeit
% CONFOUNDS als klassisches Gegenargument gg. Filme (aber er ändert sich nicht über die Zeit, vgl. resting state)
% was sind für Film und Audiobook die Confounds?
% Occipital Place Area (Nakamura, 2000), RSC
% Natural stimuli like movies \citep{hasson2008neurocinematics, sonkusare2019naturalistic} or narratives \citep{honey2012not, lerner2011topographic, silbert2014coupled} offer an easy to implement, (continuous,) complex, immersive, task-free paradigm that more closely resembles our natural dynamic environment than traditional experimental designs. Compared to specifically designed paradigms, the complexity of a natural stimulus goes hand in hand with an unknown temporal structure unless the stimulus is - often manually - annotated \citep{haeusler2016annotation, labs2015portrayed}.
% Due to their continuity, complexity, and length, they offer a vast amount of data on a variety of brain functions like low-level perception, attention, comprehension, memory, emotions, or social interactions. This suggests the data can later be reused for variety of totally different, independent research questions, possibly on an individual level.
% aminoffr2013; jedoch zu non-spatial stuff: Although the PHC is commonly thought of as responding to visual stimuli, it has also been found to elicit activity for auditory stimuli [25,26,30,31] and to respond to odor stimuli [32–34].

\subsection*{Code availability}
% wo kommt dieser Abschnitt hin?
% how to write Code availability statement
% https://www.springernature.com/gp/authors/research-data-policy/data-availability-statements/12330880
\todo[inline]{provide supporting source code, and explaining how and where others may access all data underlying the analysis}
\emph{for all studies using custom code in the generation or processing of datasets,
a statement must be included here, indicating whether and how the code can be
accessed, including any restrictions to access; include information on the versions
of any software used, if relevant, and any specific variables or parameters used to
generate, test, or process the current dataset. }

\section*{Acknowledgements}
% Text acknowledging non-author contributors. Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.
% Author contributions Please describe briefly the contributions of each author to this work on a separate line.
\emph{COH did this and that.
    MH did this and that.
    We are grateful to \href{www.florianschurz.de}{Florian Schurz} who initiated doing the annotation of the descriptive nouns, and performed the preliminary annotation of nouns.}

\section*{Competing financial interests}
\emph{A competing financial interests statement is required for all accepted
papers published in \emph{Scientific Data}. If none exist simply write,
``The author(s) declare no competing financial interests''.}

\section*{Figures Legends}
\emph{Figure should be referred to using a consistent numbering scheme through
the entire Data Descriptor. For initial submissions, authors may choose
to supply this document as a single PDF with embedded figures, but
separate figure image files must be provided for revisions and accepted
manuscripts. In most cases, a Data Descriptor should not contain more
than three figures, but more may be allowed when needed. We discourage
the inclusion of figures in the Supplementary Information \textendash{}
all key figures should be included here in the main Figure section.}

\emph{Figure legends begin with a brief title sentence for the whole figure
and continue with a short description of what is shown in each panel,
as well as explaining any symbols used. Legend must total no more
than 350 words, and may contain literature references.}

\section*{Tables}

\emph{Tables supporting the Data Descriptor. These can provide summary information
(sample numbers, demographics, etc.), but they should generally not
be used to present primary data (i.e. measurements). Tables containing
primary data should be submitted to an appropriate data repository.}

\emph{Tables may be provided within the \LaTeX{} document or as separate
files (tab-delimited text or Excel files). Legends, where needed,
should be included here. Generally, a Data Descriptor should have
fewer than ten Tables, but more may be allowed when needed. Tables
may be of any size, but only Tables which fit onto a single printed
page will be included in the PDF version of the article (up to a maximum
of three).}

{\small\bibliographystyle{unsrtnat}
\bibliography{references}}

%\begin{thebibliography}{1}
%\expandafter\ifx\csname url\endcsname\relax
%  \def\url#1{\texttt{#1}}\fi
%\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
%\providecommand{\bibinfo}[2]{#2}
%\providecommand{\eprint}[2][]{\url{#2}}
%
%\bibitem{cite1}
%\bibinfo{author}{Califano, A.}, \bibinfo{author}{Butte, A.~J.},
%  \bibinfo{author}{Friend, S.}, \bibinfo{author}{Ideker, T.} \&
%  \bibinfo{author}{Schadt, E.}
%\newblock \bibinfo{title}{{Leveraging models of cell regulation and GWAS data
%  in integrative network-based association studies}}.
%\newblock \emph{\bibinfo{journal}{Nature Genetics}}
%  \textbf{\bibinfo{volume}{44}}, \bibinfo{pages}{841--847}
%  (\bibinfo{year}{2012}).
%
%\bibitem{cite2}
%\bibinfo{author}{Wang, R.} \emph{et~al.}
%\newblock \bibinfo{title}{{PRIDE Inspector: a tool to visualize and validate MS
%  proteomics data.}}
%\newblock \emph{\bibinfo{journal}{Nature Biotechnology}}
%  \textbf{\bibinfo{volume}{30}}, \bibinfo{pages}{135--137}
%  (\bibinfo{year}{2012}).
%\end{thebibliography}

\section*{Data Citations}

Bibliographic information for the data records described in the manuscript.

1. Lastname1, Initial1., Lastname2, Initial2., ...\& LastnameN, InitialN. \emph{Repository name} Dataset accession number or DOI (YYYY).

\end{document}
