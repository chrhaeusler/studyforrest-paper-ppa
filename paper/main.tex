\documentclass[english]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{svg}
\usepackage{caption, booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{todonotes}
\usepackage{units}
\usepackage[
	colorlinks=true,
	urlcolor=blue,
	linkcolor=green
]{hyperref}
\newcommand{\scidatalogo}{\includegraphics[height=36pt]{SciData_logo.jpg}}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\setlength{\headheight}{40pt}
\lhead{\textsc{\scidatalogo}}
\usepackage{natbib}

\begin{document}
\input{descr-stats-ao-events.tex}
\input{descr-stats-av-events.tex}
\input{descr-stats-anno.tex}

% Titel-Vorgabe < 110 chars incl. spaces
%  jetzt 109 Chars
\title{Hemodynamic responses of the Parahippocampal Place Area to spatial cues in a movie and its audio-description}

\author{Christian~O.~Häusler\textsuperscript{1,2{*}}, Michael Hanke\textsuperscript{1,2}}
% https://www.nature.com/sdata/publish/for-authors#other-formats

\maketitle
\thispagestyle{fancy}

1. Psychoinformatics Lab, Institute of Neuroscience and Medicine, Brain \&
Behaviour (INM-7), Research Centre Jülich, Jülich, Germany,
2. Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University,  Düsseldorf, Germany
{*}corresponding author: Christian Olaf Häusler (der.haeusler@gmx.net)

\begin{abstract}
% < 170 words new analysis of existing data of interest to a broad section of
% our audience highlighting innovative examples of data reuse may be used to
% present compelling new findings & conclusions derived from published.
Intro: PPA as classic example for an visual area; Methods: fMRI data
studyforrest data set, general linear model; Results: significant clusters in
PPA, RSC, individual results in x of 15 subjects; Conclusions: PPA nicht
exclusively \end{abstract}

\todo[inline]{Vorgabe: Intro + Discussion = main text ca. 3,000 words; einige
offene einige Dinge sind folgend in Kommentaren und nicht als ToDo gemarkt}

\todo[inline]{include figure providing schematic overview of the study and assay
design -> is overkill: Steht in den entsprechenden Veröffentlichungen \&
Paradigma ist mit "Film ab" wenig kompliziert}

\todo[inline]{our system cannot accept BibTeX bibliography files; authors who
    wish to use BibTeX to prepare their references should therefore copy the
    reference list from the .bbl file that BibTeX generates and paste it into
    the main manuscript .tex file (and delete the associated
    \textbackslash{}bipliography and \textbackslash{}bibliographystyle
commands)}



\section{Introduction} Cognitive neuroscientists use brain imaging methods like
blood oxygenation level-dependent functional magnetic resonance imaging (BOLD
fMRI) to map perceptual and cognitive brain processes (e.g. viewing of
landscapes, \citep{epstein1998ppa}; or theory of mind
\citep{spunt2014validating}) to functional brain areas or networks. Brain
mapping studies have traditionally employed small sets of carefully chosen,
simplified stimuli to strictly control experimental conditions and facilitate
functional isolation of brain areas. The disadvantage is that a limited number
of simplified stimuli does not resemble our real-life experience and lacks
external validity \citep{westfall2016fixing} as well as ecological validity
\citep{hasson2004intersubject}. Additionally, brain mapping studies usually
average results across at least 10-15 subjects to improve the
signal-to-noise-ratio (SNR). The disadvantage here is that the averaging
approach does not characterize brain functions at an individual level, a
prerequisite for the application of brain imaging methods in individual
diagnostics (cf. \citep{dubois2016building, eickhoff2020towards})

\todo[inline]{Retrosplenial Cortex kommt in Einleitung bisher  nicht vor; wird
aufgrund der Ergebnisse in AV und AO in Discussion diskutiert werden; cluster
sind in den Abbildungen nicht zu sehen; kommen aber in (den Tabellen) der
Results Section }

% What is the PPA?
A classic example of a functional, higher-level visual area is the
``parahippocampal place area'' (PPA) \citep{epstein1998ppa,
epstein1999parahippocampal}. The PPA is located in the posterior parahippocampal
gyrus at the boundary to the anterior lingual gyrus and medial fusiform gyrus,
both being part of the ventral visual stream \citep{mishkin1982contribution,
goodale1992separate}. Increased hemodynamic activity in the PPA correlates with
the perception of static pictures of landscapes or landmarks compared to
pictures of tools or faces.  According to the spatial layout hypothesis, the PPA
is involved in processing the surface geometry of a visual stimulus and possibly
in identifying scenes based on their spatial layout \citep{epstein2010reliable}.

% Literature Review PPA: Intro
At a group average level, results generalize to mental imagination of landscapes
\citep{ocraven2000mental} and haptic exploration of scenes constructed from LEGO
blocks \citep{wolbers2011modality}. One study that used spoken, place-related
sentences showed significantly decreased activation in the left and no
modulation in the right-hemispheric PPA \citep{aziz2008modulation}.

% Exemplary studies
% O'Craven: Mental imagery
In the study conducted by \citep{ocraven2000mental} participants viewed
alternating blocks of pictures showing famous faces and familiar places during
an initial experimental paradigm. In a subsequent paradigm, participants were
instructed to ``form a vivid mental image'' of the previously viewed pictures.
The PPA showed increased activation during imagination of places compared to
faces but the imagination tasks showed a smaller activation level compared to
the perceptual task.

% Wolbers: haptic exploration
In the block design study conducted by \citep{wolbers2011modality} the PPA of
sighted participants showed increased activation during a delayed
matching-to-sample task of haptically explored scenes constructed from LEGO
bricks compared to haptically explored abstract geometric objects.

\todo[inline]{Er hat noch eine Connectivitäts-Analyse mit dem Occipital Cortex
gemacht. Im Vergleich zum visuellen Paradigma ist im haptischen Paradigma keine
Connectivität. Interpretation: Es kann keine mental imagery sein}

\todo[inline]{"The scene-related increase in coupling with the PPA was
significantly stronger in multiple clusters in occipital cortex under visual
than haptic stimulation (Figure 2B; Table S2)"}

% Aziz (2008): place related sentences
\todo[inline]{Aziz hat auf eine zweite Mail geantwortet: Sie modelt die HRF über
den jeweils gesamten Satz von ca. 2.8 Sekunden}

\todo[inline]{Ihre Method Section ist strange und es kommt bei ihr überall eh
    nur uneinheitliche Grütze heraus; Ich blicke nicht durch; was folgend kommt,
    wird nicht vollständig korrekt sein. So wie ich es verstanden habe, hat sie
    den Unterschied zwischen PPA und FFA angeguckt, aber diesen Unterschied im
Kontrast zu object-related Sentences (dortige Fig 3; lass uns kurz im Gespräch
drübergucken, ich habe die relvanten Abschnitte in der PDF markiert)}

To our knowledge only one study by \citep{aziz2008modulation} correlated
hemodynamic activity of the PPA with spoken sentences. Sentences described
generic or famous places (e.g. ``The Taj Mahal faces a long thin reflecting
pool''), faces  (e.g. ``Marilyn Monroe has a large square jaw''), or objects
(e.g. ``The television has a long antenna')'. Participants were instructed to
press a button whenever the sentence described an inaccurate or improbable fact.
Surprisingly, activation in the left, but not right, PPA was significantly
reduced when participants listened to place-related sentences compared to
listening to face-related sentences. Moreover, this effect was only observed in
sentences involving famous places.

\todo[inline]{Review von Aminoff (2013) kommt nur kurz als "competing hypothesis"; muss man dann natürlich in der Discussion diskutieren; was aber ggf. gar nicht notwendig ist, weil's darum gar bei uns gar nicht geht}

% Summary if literature review
In summary, studies suggest that the PPA does not exclusively respond to
visual(ly) spatial information. This is consistent with a competing hypotheses
stating that the PPA does not process the spatial layout of a scene specifically
but contextual associations in general (\citep{aminoff2006parahippocampal,
aminoff2013role}). It is still uncertain if auditory stimuli lead to inhibitory
or excitatory responses, or no change at all in the PPA.

\todo[inline]{aber Huth (2016) implizit; diskutiert PPA also solches jedoch
nicht und macht voxel-wise encoding, was - glaube ich - über
excitation/inhibition nichts aussagt -> Paper checken}

% Shortcomings of previous studies; lack of research
All reviewed studies used a just a small set of carefully designed, simplified
stimuli to strictly control experimental conditions. They further relied on
explicit judgment-based tasks to ensure that participants payed attention to
the stimuli. This raises the question how the PPA behaves under life-like conditions.

% Why naturalistic stimuli
Naturalistic stimuli like movies \citep{hasson2008neurocinematics,
sonkusare2019naturalistic} or narratives \citep{honey2012not,
lerner2011topographic, silbert2014coupled} offer complex, continuous stimuli
that better resemble our experience of a our dynamic environment.

% Literature review naturalistic stimuli: Einleitung
Two early studies using naturalistic stimuli suggest functional specialization
of brain areas is preserved during movie watching when many perceptual and
cognitive processes are triggered simultaneously \citep{bartels2004mapping, hasson2004intersubject}.

% Bartels (2004): Film + GLM (jedoch ohne PPA) und Hasson (2004)
\citep{bartels2004mapping} manually annotated time points that depict faces and
human bodies in a 22 minute clip of the movie \textit{Tomorrow Never Dies}
\citep{tomorrowneverdies}.
Employing a general linear model (GLM) to model hemodynamic activity, these time
points correlated with increased activity in the ``fusiform face area''
\citep{kanwisher1997ffa} and ``extrastriate body area''
\citep{downing2001bodyarea} respectively.

% Hasson (2004): Film + (reverse) ISC
Employing a reverse intersubject correlation, \citep{hasson2004intersubject}
used a 30 minute clip of the movie \textit{The Good, the Bad, and the Ugly}
\citep{goodbadugly} fMRI brain volumes showing the highest hemodynamic activity
in the PPA [across (five) participants] were matched to the temporally
corresponding movie frames. Results revealed that corresponding movie frames
depicted indoor and outdoor scenes including buildings and open fields.

% Summary naturalistic stimuli
Nevertheless, no study investigated how the PPA responds to spatial clues that are embedded in a complex, audio-only stimulus.
\todo[inline]{Huth (2016) checken}


\todo[inline]{The next paragraphs contain a bunch of methods already; reduce
this to talk about the general approach:

a) annotation of events

b) investigation of model quality,

c) cross-analysis of datasets

d) Possibly cite https://psyarxiv.com/sdbqv/ (DuPre (2019). Nature abhors a
paywall) and declare that this study is a concrete realization of that promise}

% what we did data source
In this study, we analyze functional data of the same 14 participants provided
by the studyforrest dataset \href{http://www.studyforrest.org}{studyforrest.org}; \citep{hanke2014audiomovie, hanke2016simultaneous}).
% PPA + movie + audio-description
We annotated cuts in the movie ``Forrest Gump'', and words spoken by an
additional narrator in the movie's audio-description (i.e. the movie's
audio-only variant) to exploratorily investigate how the parahippocampal gyrus
behaves during lifelike stimulation.
% model-driven analysis
In conformity to the analyses performed in the reviewed studies,  we also
employed a standard multi-level, voxel-wise general linear model
(GLM).\todo{why? in order to\dots}
% focus on audio-description
Encouraged by \citep{aziz2008modulation} and \citep{aminoff2013role}, we focus
on the audio-description as a naturalistic, auditory-only paradigm free from any task to localize a ``visual area'' in individual study participants.
% comparison to dedicated localizer
Finally, we compare our results to results of a dedicated visual localizer
experiment done with the same 14 participants \citep{sengupta2016extension}.

\todo[inline]{Bzgl. Open Source der Ergebnisse/Skripte: Dazu habe ich hier nichts geschrieben, weil ich nur weiß, das der Code veröffentlicht werden soll. Du hast die Vision diesbezüglich im Kopf und entsprechend auch kanonisch-prägnante Formulierungen und wo die am besten wie hinkommen}


\section{Methods}

\todo[inline]{Specific data outputs should be explicitly referenced via data
citation (see Data Records and Data Citations, below)}
\todo[inline]{order of subsections possibly wrong}
\todo[inline]{parts from original studies still detailed}

% intro
We used data from three publicly available datasets
(\href{http://www.studyforrest.org}{studyforrest.org}) which have already been
used by other research groups for independent research questions.
% used studies
\todo{this is just loveless listing so far but it's 'complete'} The same
subjects were
% AO
a) listening to the audio-description (AO study; \citep{hanke2014audiomovie}) of
the movie ``Forrest Gump'', a dataset already used by \citep{hu2017decoding,
nguyen2016integration},
% AV
b) watching the actual audio-visual movie (AV study;
\citep{hanke2016simultaneous}, a dataset already used by
\citep{ben2018hippocampal},
% VIS
c) participating in a dedicated six-category block-design visual localizer (VIS
study) \citep{sengupta2016extension}, a dataset already used by
\citep{jiahui2019predicting}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications \citep{hanke2014audiomovie, hanke2016simultaneous,
sengupta2016extension}. Following is a summary of most important aspects.


\subsection{Participants}
% AO study
In the AO study \citep{hanke2014audiomovie}, 20 German native speakers (all
right-handed, age 21–38 years, mean age 26.6 years, 12 male) listened to the
German audio-description \citep{ForrestGumpGermanAD} of the movie ``Forrest
Gump'' \citep{ForrestGumpMovie} as an additional audio track for visually
impaired listeners on Swiss public television.
% AV study
In the AV study \citep{hanke2016simultaneous}, 15 participants (21–39 years,
mean age 29.4, six female) of the prior AO study watched the audio-visual movie
with dubbed German audio track \citep{ForrestGumpDVD}.
% VIS study
In the VIS study \citep{sengupta2016extension}, the same 15 participants took
part in a six-category block-design visual localizer.

% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli}
% AO & AV stimulus name & references
We used the German DVD release \citep{ForrestGumpDVD} of the audio-visual movie
``Forrest Gump'' \citep{ForrestGumpMovie} and its temporally aligned
audio-description (i.e. the audio-only variant of the movie) that was broadcast
as an additional audio track for visually impaired listeners on Swiss public
television \citep{ForrestGumpGermanAD}.
% AO vs. AV: differences
The plot of the movie is already carried by an off-screen voice-over of the main
character Forrest Gump. In the largely identical audio-description, an
additional male narrator describes essential aspects of the visual scenery when
there is no off-screen voice, dialog, or other relevant auditory content.

% AO & AV: stimulus creation
The audio-description was temporally aligned to the audio track of the German
DVD release. A few scenes less relevant to the major plot were removed to create
the ``research cut'' lasting $\approx$2h \citep{hanke2014audiomovie,
hanke2016simultaneous}.
% further processing
Both stimuli were further processed (filtering, volume adjustments) to improve
audibility during MRI scanning. Black horizontal bars at the top and bottom of
the movie were replaced with medium-gray bars of the same size in order to
increase background illumination for a more pleasant experience.
% splitting
Each stimulus was split into eight segments of approximately 15 minutes. Except
for the first movie segment, each segment started with a snippet of at least six
seconds immediately preceding the movie scene boundary used to split the
segments (see Figure 3a in \citep{hanke2014audiomovie}.

% VIS study picture categories
All stimuli for the VIS study were used in a previous study
\citep{haxby2011common}. There were 24 unique greyscale images (matched in
luminance, size of 400 $\times$\unit[400]{px}) for each of six stimulus
categories: human faces, human bodies without heads, small objects, houses and
outdoor scenes comprising of nature and street scenes, and phase scrambled
images. Mirrored views of these 24 $\times$ 6 images were also used as stimuli.

\subsection{Naturalistic stimuli annotation}

Both natural stimuli were originally designed to entertain an audience and to capture attention free from any perceptual or behavioral task.
We annotated events in the otherwise unknown temporal structure of both stimuli.
In/for the movie, we annotated different kinds of cinematographic cuts \citep{haeusler2016cutanno} as presumably ``spatially relevant'' events.
Our rationale was that cuts re-orient the viewer in the depicted environment
independent from the exact visual content in the movie frame following the cut.
In/for the audio-description, we annotated verbal clues about the missing visual content given by the audio-description's additional narrator \citep{haeusler2020speechanno}.

% AV anno
For the analysis of the AV data, we took advantage of a publicly available
annotation of movie cuts and depicted location \citep{haeusler2016cutanno}.
% AO anno
For the analysis of the AO data, we extended another publicly available
annotation of speech \citep{haeusler2020speechanno}. We manually annotated words
spoken by the audio-description's narrator.
% annotation procedure
Two persons performed a categorization of nouns embedded in sentences spoken by
the narrator to describe the movie's missing visual content. Some categories
were created in such a way to reflect the six stimulus categories used in the
visual localizer experiment. Some additional categories were created to
semantically cluster nouns into categories ``that were obvious to make
sense''.\todo{yeah, well, sounds shit but that's basically it}.
% categories Kategorie ``++'' enthält ausnahmesweise auch adverbiale Best. der
% Zeit.
More specifically, nouns were categorized by the clue they provide about the
cinematographic scene's environment (geo, geo-room; setting\_new, setting\_old),
its inherent persons (female, female name; male, male name, person), a person's
appearance (face, head; body, bodypart), and a scene's inherent objects (object,
furniture).
% reference to table
A complete overview of all categories, the rule for a word to belong to a
category, and examples can be seen in Table \ref{tab:descr-nouns-rules}.
% procedure
A preliminary annotation was performed by one person according to the rules (see
Table. Minor corrections due to incorrectly applied rules were by the author
(see Table \ref{tab:descr-nouns-counts} for the counts in the whole stimulus and
the eight stimulus segments used during fMRI scanning).

\todo[inline]{so ist die Tabelle zu breit; schrift kleiner oder Text kürzen?
Könnte durchaus auch detaillierteren Text schreiben}

% rules Tabelle für descriptiven nouns
\begin{table*}[t]
    \caption{Descriptive nouns spoken by the audio-description's narrator:
    cateogries, annotation rules and examples (given in English). The categorie
    \texttt{++} also contains adverbial adjectives and adverbs}
\label{tab:descr-nouns-rules}
\begin{tabular}{lll}
\toprule
\textbf{category} & \textbf{rule} & \textbf{examples} \\
\midrule
body & trunk of the body; overlaid clothes & back, hip, shoulder; jacket, dress
shirt \tabularnewline
bodypart & limbs and trousers & arm, finger, leg, toe \tabularnewline
face & face or parts of it & face, ear, nose, mouth \tabularnewline
female & female person & nurse, mother, woman \tabularnewline
females & female persons & women \tabularnewline
fname & female name & Jenny \tabularnewline
furniture & moveable furniture insides \& outsides & bench, bed, table, chair
\tabularnewline
geo & immobile landmarks & building, tree, street, alley, meadow, cornfield \tabularnewline
geo-room & rooms (insides) or  locales (outsides) & living room; wall, door, window, floor, turf \tabularnewline
head & non-face parts of the head; worn headgear & head, hair, ear, neck,
helmet \tabularnewline
male & male person & man, father, soldier \tabularnewline
males & male persons & boys, opponents \tabularnewline
mname & male name & Bubba, Kennedy \tabularnewline
object & countable entity with firm boundaries & telephone, car \tabularnewline
objects & countable entities & wheels, plants \tabularnewline
persons & concrete persons of unknown sex & hippies, patients \tabularnewline
setting\_new & noun cueing a new setting &  on a ``bridge'', on an ``alley'', on ``campus'' \tabularnewline
setting\_rec & noun cueing a recurring setting & at the ``bus stop'' \tabularnewline
++ & cue regarding time & in the ``evening'', it's ``daytime'', ``later'' \tabularnewline

\bottomrule
\end{tabular}
\end{table*}

% table of descriptive nouns counts
\begin{table*}[t]
    \caption{Descriptive nouns spoken by the audio-descriptions narrator:
        counts for the whole audio-only stimulus and its eight segments used
        during fMRI scanning.}
\label{tab:descr-nouns-counts}
\begin{tabular}{llllllllll}
\toprule
\textbf{category} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & \anBodyAll & \anBodyI & \anBodyII & \anBodyIII & \anBodyIV & \anBodyV & \anBodyVI & \anBodyVII & \anBodyVIII \tabularnewline
bodypart &  \anBodypartAll & \anBodypartI & \anBodypartII & \anBodypartIII & \anBodypartIV & \anBodypartV & \anBodypartVI & \anBodypartVII & \anBodypartVIII \tabularnewline
face & \anFaceAll & \anFaceI & \anFaceII & \anFaceIII & \anFaceIV & \anFaceV & \anFaceVI & \anFaceVII & \anFaceVIII \tabularnewline
female & \anFemaleAll & \anFemaleI & \anFemaleII & \anFemaleIII & \anFemaleIV & \anFemaleV & \anFemaleVI & \anFemaleVII & \anFemaleVIII \tabularnewline
females & \anFemalesAll & \anFemalesI & \anFemalesII & \anFemalesIII & \anFemalesIV & \anFemalesV & \anFemalesVI & \anFemalesVII & \anFemalesVIII \tabularnewline
fname & \anFnameAll & \anFnameI & \anFnameII & \anFnameIII & \anFnameIV & \anFnameV & \anFnameVI & \anFnameVII & \anFnameVIII \tabularnewline
furniture & \anFurnitureAll & \anFurnitureI & \anFurnitureII & \anFurnitureIII & \anFurnitureIV & \anFurnitureV & \anFurnitureVI & \anFurnitureVII & \anFurnitureVIII \tabularnewline
geo & \anGeoAll & \anGeoI & \anGeoII & \anGeoIII & \anGeoIV & \anGeoV & \anGeoVI & \anGeoVII & \anGeoVIII \tabularnewline
geo-room & \anGeoroomAll & \anGeoroomI & \anGeoroomII & \anGeoroomIII & \anGeoroomIV & \anGeoroomV & \anGeoroomVI & \anGeoroomVII & \anGeoroomVIII \tabularnewline
head & \anHeadAll & \anHeadI & \anHeadII & \anHeadIII & \anHeadIV & \anHeadV & \anHeadVI & \anHeadVII & \anHeadVIII \tabularnewline
male & \anMaleAll & \anMaleI & \anMaleII & \anMaleIII & \anMaleIV & \anMaleV & \anMaleVI & \anMaleVII & \anMaleVIII \tabularnewline
males & \anMalesAll & \anMalesI & \anMalesII & \anMalesIII & \anMalesIV & \anMalesV & \anMalesVI & \anMalesVII & \anMalesVIII \tabularnewline
mname & \anMnameAll & \anMnameI & \anMnameII & \anMnameIII & \anMnameIV & \anMnameV & \anMnameVI & \anMnameVII & \anMnameVIII \tabularnewline
object & \anObjectAll & \anObjectI & \anObjectII & \anObjectIII & \anObjectIV & \anObjectV & \anObjectVI & \anObjectVII & \anObjectVIII \tabularnewline
objects & \anObjectsAll & \anObjectsI & \anObjectsII & \anObjectsIII & \anObjectsIV & \anObjectsV & \anObjectsVI & \anObjectsVII & \anObjectsVIII \tabularnewline
persons & \anPersonsAll & \anPersonsI & \anPersonsII & \anPersonsIII & \anPersonsIV & \anPersonsV & \anPersonsVI & \anPersonsVII & \anPersonsVIII \tabularnewline

setting\_new & \anSettingnewAll & \anSettingnewI & \anSettingnewII & \anSettingnewIII & \anSettingnewIV & \anSettingnewV & \anSettingnewVI & \anSettingnewVII & \anSettingnewVIII \tabularnewline

setting\_rec & \anSettingrecAll &
\anSettingrecI & \anSettingrecII & \anSettingrecIII & \anSettingrecIV & \anSettingrecV & \anSettingrecVI & \anSettingrecVII & \anSettingrecVIII \tabularnewline
++ & \anAll & \anI & \anII & \anIII & \anIV & \anV & \anVI & \anVII & \anVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsection{Procedure}
% questionnaire
In all three studies, participants filled out a questionnaire on their basic
demographic information and familiarity with the movie).
% AO & AV: instructions
Participants were instructed to inhibit physical movements except for
eye-movements, and otherwise to simply ``enjoy the audiobook'' or ``enjoy the
movie'' respectively.
% AO & AV: presentation & instructions
Audio-description and movie segments were presented in chronological order with
four segments in two fMRI sessions. Between sessions, participants left the
scanner for a break with a flexible duration. Structural images were obtained
during the first study on a day different from the fMRI session.

% VIS: presentation & instructions
In the VIS study, participants were presented with four block-design runs, with
two \unit[16]{s} blocks per stimulus category in each run, while they also
performed a one-back matching task to keep them attentive.

\subsection{Stimulation setup}

% paradigm implementation
In all three studies, stimulation was implemented with \href{http://www.psychopy.org}{PsychoPy} \citep{peirce2007psychopy} running on a computer with the \href{http://neuro.debian.net}{(Neuro)Debian} operating system \citep{halchenko2012open}.

% AO
In the AO study, visual instructions were presented on a rear-projection screen
inside scanner bore using an LCD projector (DLA-G150CL, JVC Ltd.). During the
functional scans, the projector presented a medium gray screen with the primary
purpose to illuminate a participant's visual field in order to prevent premature
fatigue. [The screen contained a solid black ``fixation'' dot that faded in and
out at the beginning and end of a movie segment.]

% AV & VIS
In the AV and VIS study, visual instructions and stimuli were presented on a
rear-projection screen using an LCD projector (JVC DLA RS66E, JVC Ltd., light
transmission reduced to \unit[13.7]{\%} with a gray filter) connected to the
stimulus computer [via a DVI extender system (Gefen EXT-DVI-142DLN with
EXT-DVI-FM1000)].
% screen size
The movie was shown at a viewing distance of \unit[63]{cm} in \unit[720]{p}
resolution at full width on a \unit[1280 $\times$ 1024]{pixel} screen with
\unit[60]{Hz} video refresh rate and screen dimension of \unit[26.5 $\times$
21.2]{cm}.
% angle of view: AV
In the AV study, this corresponded to \unit[23.75 $\times$ 13.5] or \unit[23.75
$\times$ 10.25]{cm} when considering only the movie content and excluding the
horizontal gray bars.
% angle of view: VIS
In the VIS study, stimulus images were displayed at a size of approximately
\unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.

\todo[inline]{auditory stimulation was the same in AO and AV?}

% AO & AV: auditory stimulation
In the AO and AV study, auditory stimulation was delivered through an MR confon
mkII+ driving custom-built in-ear headphones (HP-M01, MR confon GmbH, Magdeburg,
Germany; \citep{baumgart1998electrodynamic}) that reduced the scanner noise by
at least \unit[20–30]{dB}. Headphones were fed from an Aureon 7.1 USB
(Terratec) sound card through an optical connection.


\subsection{fMRI data acquisition}

% AO
In the AO study, a whole-body \unit[7]{Tesla} Siemens MAGNETOM magnetic
resonance scanner equipped with a local circularly polarized head transmit and a
32 channel brain receive coil (Nova Medical, Inc., Wilmington, MA, USA) acquired
T2*-weighted echo-planar images (gradient-echo, \unit[2]{s} repetition time
(TR), \unit[22]{ms} echo time, \unit[0.78]{ms} echo spacing, \unit[1488]{Hz/Px}
bandwidth, generalized autocalibrating partially parallel acquisition,
acceleration factor 3, \unit[2]{Hz/Px} bandwidth in phase encoding direction).
% slices & FOV
36 axial slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) with a 10 percent inter-slice gap were recorded in ascending order.
Slices were oriented to include the ventral the ventral portions of frontal and
occipital cortex while minimizing intersection with the eyeballs. The field of
view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions [Oh, S. et
al. 2012; In, M. \& Speck, O., 2012; Chung, J. et al., 2011]. Auxiliary scans
for slice alignment and motion- and distortion-correction were performed at the
beginning of the first fMRI recording session and also after the break at the
start of the recording for the second half of the movie.

% AV & VIS
In the AV and VIS study, a whole-body \unit[3]{Tesla} Philips Achieva dStream
MRI scanner equipped with a 32 channel head coil acquired T2*-weighted
echo-planar images (gradient-echo, \unit[2]{s} repetition time (TR),
\unit[30]{ms} echo time, \unit[90]{$^{\circ}$} flip angle, \unit[1943]{Hz/Px}
bandwidth, parallel acquisition with sensitivity encoding (SENSE) reduction
factor 2).
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction were recorded in ascending order. Philips' ``SmartExam'' was
used to automatically position slices in AC-PC orientation such that the topmost
slice was located at the superior edge of the brain. This automatic slice
positioning procedure was identical in the AV and VIS study, and yielded a
congruent geometry across paradigms.

% no. of volumes
A total of 3599 volumes were recorded for each participant in each of the
naturalistic stimulus paradigms (451, 441, 438, 488, 462, 439, 542, and 338 volumes for segment 1–8).


\subsection{Preprocessing}

\todo[inline]{I revised part of motion correction/co-registration but as usually
I am pretty unsure about correctness}
\todo[inline]{Satz zum Downsampling der 7T Daten?}

% data sources
% https://github.com/psychoinformatics-de/studyforrest-data-aligned/tree/master/code
% https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms}
fMRI time series of those 15 participants in the studyforrest dataset that took
part in all three experiments were obtained from
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{GitHub}.
Data were already corrected for motion and aligned by non-linear warping to a
particpant-specific BOLD template image \citep{sengupta2016extension}.
% exclusion of VP 10
Data of one participant were dropped to due to invalid distortion correction
during scanning of the AO stimulus.

% preprocessing intro
All further analysis steps of the current study were carried out using tools of
FSL v5.0.9 (\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software
Library}; \citep{smith2004fsl}) on a computer cluster running the
\href{https://www.debian.org}{Debian} GNU/Linux operating system. Software
packages were obtained from repositories of
\href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.

% actual preprocessing
Preprocessing was carried out using FEAT v6.00 (FMRI Expert Analysis Tool;
\citep{woolrich2001autocorr}) as part of FSL.
% temporal filtering
High-pass temporal filtering was applied to every segment using a
Gaussian-weighted least-squares straight line with a cutoff period of
\unit[150]{s} (sigma=\unit[75.0]{s}) to remove low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model; \citep{woolrich2001autocorr})
to improve estimation efficiency.

\subsection{Statistical analysis}

For both naturalistic stimuli, we conducted a standard two-level general linear
model (GLM) analysis in FEAT to create subject-specific results ($Z$-maps)
across the 8 segments for every subject. A subsequent, third-level analysis averaged contrast estimates over subjects.


\subsubsection{first-level}

\paragraph{Regressors}

\todo[inline]{better speak of conditions than categories?}

% AO: events
For the analysis of the AO stimulus, we created regressors correlating with the
occurrence of descriptive nouns spoken by the narrator that were added to the
original annotation of speech \citep{haeusler2020speechanno}.
% AV events
For the AV stimulus, we created regressors correlating with movie cuts provided
by a previously published annotation \citep{haeusler2016cutanno}.
% https://github.com/psychoinformatics-de/studyforrest-data-annotations/blob/master/code/researchcut2segments.py}

% procedure
First, both annotations were split into eight parts corresponding to the eight
stimulus segments.
% AO events
To create regressors for the AV analysis, we used all of the original 19
categories of descriptive nouns except \texttt{++}.
% pooling
Some categories that were similar but offered only a small
amount of counts were pooled resulting in eleven new categories (see Table
\ref{tab:ao-events}). Events that fell into one of the two setting-related
category but also into another category were treated as belonging to the
setting-related category (\texttt{se\_new} or \texttt{se\_old}).
% modeling as boxcar function
Events were were modeled as boxcar function from onset to offset of each word.
% null regressors
If a segment did not contain an event of a category, a null regressor was chosen
for that segment.  \todo{wenn ich "null regressor" nehme, darf ich den ganzen
Run, glaube ich, in der 2nd level Analyse gar nicht benutzen? Hatte, glaube ich,
Jeanette Mumford in einem ihren Videos mal gesagt}
% categories taken from movie cut annotation
Five additional categories were created from the annotation of movie cuts and
modeled as impulse events to capture variance of a possibly confounding change
of the soundscape after a cut, and to create contrasts of cross-modal negative
control.
% nuissance regressors
Lastly, we used continuous bins of information about two auditory features
(left-right difference in volume and root mean square energy) that was averaged
across the length of every movie frame (\unit[40]{ms}) to capture variance
correlating with low-level perceptual processes.
% Reference to table
An explanation and counts of the eleven speech-related event categories, the
five movie cut-related categories, and the two low-level confounds can be found
in Table \ref{tab:ao-events}.


\begin{table*}[t]
\caption{Overview of events to build the 18 regressors for the analysis of the
audio-only (AO) stimulus. Some of the original categories (female, females,
fname; male, males, mname; face, head; object, objects; see Table
\ref{tab:descr-nouns-rules}) were pooled into 11 new categories (sex\_f; sex\_m; fahead; object).
\texttt{fg\_ad\_lrdiff} (left-right volume difference) and
\texttt{fg\_ad\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
For a description of the five control conditions for movie cuts see Table
\ref{tab:av-events}.}
\label{tab:ao-events}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
body & trunk of the body or overlaid clothes & \aoBodyAll & \aoBodyI & \aoBodyII & \aoBodyIII & \aoBodyIV & \aoBodyV & \aoBodyVI & \aoBodyVII & \aoBodyVIII \tabularnewline
bpart & limbs and trousers & \aoBpartAll & \aoBpartI & \aoBpartII & \aoBpartIII & \aoBpartIV & \aoBpartV & \aoBpartVI & \aoBpartVII & \aoBpartVIII \tabularnewline
fahead & face(parts) or head(parts) & \aoFaheadAll & \aoFaheadI & \aoFaheadII & \aoFaheadIII & \aoFaheadIV & \aoFaheadV & \aoFaheadVI & \aoFaheadVII & \aoFaheadVIII \tabularnewline
furn & moveable furniture insides \& outsides & \aoFurnAll & \aoFurnI & \aoFurnII & \aoFurnIII & \aoFurnIV & \aoFurnV & \aoFurnVI & \aoFurnVII & \aoFurnVIII \tabularnewline
geo & immobile landmarks & \aoGeoAll & \aoGeoI & \aoGeoII & \aoGeoIII & \aoGeoIV & \aoGeoV & \aoGeoVI & \aoGeoVII & \aoGeoVIII \tabularnewline
groom & elements defining a locale's spatial layout & \aoGroomAll & \aoGroomI & \aoGroomII & \aoGroomIII & \aoGroomIV & \aoGroomV & \aoGroomVI & \aoGroomVII & \aoGroomVIII \tabularnewline
object & inanimate entities with firm boundaries & \aoObjAll & \aoObjI & \aoObjII & \aoObjIII & \aoObjIV & \aoObjV & \aoObjVI & \aoObjVII & \aoObjVIII \tabularnewline
se\_new & first-time mentioned setting & \aoSenewAll & \aoSenewI & \aoSenewII & \aoSenewIII & \aoSenewIV & \aoSenewV & \aoSenewVI & \aoSenewVII & \aoSenewVIII \tabularnewline
se\_old & recurring setting & \aoSeoldAll & \aoSeoldI & \aoSeoldII & \aoSeoldIII & \aoSeoldIV & \aoSeoldV & \aoSeoldVI & \aoSeoldVII & \aoSeoldVIII \tabularnewline
sex\_f & female person(s), name & \aoSexfAll & \aoSexfI & \aoSexfII & \aoSexfIII & \aoSexfIV & \aoSexfV & \aoSexfVI & \aoSexfVII & \aoSexfVIII \tabularnewline
sex\_m & male person(s), name & \aoSexmAll & \aoSexmI & \aoSexmII & \aoSexmIII & \aoSexmIV & \aoSexmV & \aoSexmVI & \aoSexmVII & \aoSexmVIII \tabularnewline
% vlo_ch has no events in segment 5. So indices are shifted
vse\_new & control for movie cut & \aoVsenewAll & \aoVsenewI & \aoVsenewII & \aoVsenewIII & \aoVsenewIV & \aoVsenewV & \aoVsenewVI & \aoVsenewVII & \aoVsenewVIII \tabularnewline
vse\_old & control for movie cut & \aoVseoldAll & \aoVseoldI & \aoVseoldII & \aoVseoldIII & \aoVseoldIV & \aoVseoldV & \aoVseoldVI & \aoVseoldVII & \aoVseoldVIII \tabularnewline
vlo\_ch & control for movie cut & \aoVlochAll & \aoVlochI & \aoVlochII & \aoVlochIII & \aoVlochIV & 0 & \aoVlochV & \aoVlochVI & \aoVlochVII \tabularnewline
vpe\_new & control for movie cut & \aoVpenewAll & \aoVpenewI & \aoVpenewII & \aoVpenewIII & \aoVpenewIV & \aoVpenewV & \aoVpenewVI & \aoVpenewVII & \aoVpenewVIII \tabularnewline
% vpe_old has no events in 3. So the indices are shifted
vpe\_old & control for movie cut & \aoVpeoldAll & \aoVpeoldI & \aoVpeoldII & 0 & \aoVpeoldIII & \aoVpeoldIV & \aoVpeoldV & \aoVpeoldVI & \aoVpeoldVII \tabularnewline
fg\_ad\_lrdiff & left-right volume difference & \aoFgadlrdiffAll & \aoFgadlrdiffI & \aoFgadlrdiffII & \aoFgadlrdiffIII & \aoFgadlrdiffIV & \aoFgadlrdiffV & \aoFgadlrdiffVI & \aoFgadlrdiffVII & \aoFgadlrdiffVIII \tabularnewline
fg\_ad\_rms & root mean square (loudness) & \aoFgadrmsAll & \aoFgadrmsI & \aoFgadrmsII & \aoFgadrmsIII & \aoFgadrmsIV & \aoFgadrmsV & \aoFgadrmsVI & \aoFgadrmsVII & \aoFgadrmsVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% AV events
To create regressors for the AV analysis, we categorized the depicted location
in the frame following one of the 869 cuts into five categories (see Table
\ref{tab:av-events}): 1) a cut into an unfamiliar setting (\texttt{vse\_new}),
2) a cut into a familiar setting (\texttt{vse\_old}), 3) a switch of locale
within a setting (\texttt{vlo\_ch}), 4) a cut to an unfamiliar perspective
(\texttt{vpe\_new}), and 5) a cut to familiar perspective (\texttt{vpe\_old}).
% null regressor
Here again, a null regressor was chosen for segment if it did not contain an
event of specific category.
% no cut condition: intro
A sixth regressors (\texttt{no\_cut}) serving a control condition and to create
negative control contrasts. Frames within movie shots lasting at least
\unit[20]{s} were pseudo-randomly chosen by the following rationale:
% how they were fitted
\texttt{no\_cut} event had to have a minimum distance of \unit[10]{s} to a movie
cut and to another \texttt{no\_cut} event.
% reference to table
A short explanation and counts of these six categories modeled as impulse
events, and six additional event categories for low-level auditory and visual
features of the movie (here again continuous bins lasting \unit[40]{ms}) can be
found in Table \ref{tab:av-events}.

\begin{table*}[t]
    \caption{Overview of events to build the 14 regressors for the analysis of the audio-only (AV) stimulus.
\texttt{fg\_av\_ger\_lr} (left-right difference in [+++???+++] of
consecutive movie frames).
\texttt{fg\_av\_ger\_pd} (perceptual differences of consecutive frames).
\texttt{fg\_av\_ger\_ud} (up-down difference in [+++???+++] of consecutive movie frames).
\texttt{fg\_av\_ger\_lrdiff} (left-right volume difference) and \texttt{fg\_av\_ger\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
For description of the two events categories of the audio-description's
narrator (\texttt{se\_new} and \texttt{se\_old}) to build contrast
of negative control see Table \ref{tab:ao-events}}
\label{tab:av-events}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
vse\_new &  change of the camera position to a setting not depicted before & \aoVsenewAll & \aoVsenewI & \aoVsenewII & \aoVsenewIII & \aoVsenewIV & \aoVsenewV & \aoVsenewVI & \aoVsenewVII & \aoVsenewVIII \tabularnewline
vse\_old & change of the camera position to a recurring setting & \aoVseoldAll & \aoVseoldI & \aoVseoldII & \aoVseoldIII & \aoVseoldIV & \aoVseoldV & \aoVseoldVI & \aoVseoldVII & \aoVseoldVIII \tabularnewline
vlo\_ch & change of the camera position to another locale within the same setting & \aoVlochAll & \aoVlochI & \aoVlochII & \aoVlochIII & \aoVlochIV & 0 & \aoVlochV & \aoVlochVI & \aoVlochVII \tabularnewline
vpe\_new & change of the camera position within a locale not depicted before & \aoVpenewAll & \aoVpenewI & \aoVpenewII & \aoVpenewIII & \aoVpenewIV & \aoVpenewV & \aoVpenewVI & \aoVpenewVII & \aoVpenewVIII \tabularnewline
% vpe_old has no events in 3. So the indices are shifted
vpe\_old & change of the camera position within a recurring locale & \aoVpeoldAll & \aoVpeoldI & \aoVpeoldII & 0 & \aoVpeoldIII & \aoVpeoldIV & \aoVpeoldV & \aoVpeoldVI & \aoVpeoldVII \tabularnewline
vno\_cut & a continuous frame from the same camera position & \avVnocutAll & \avVnocutI & \avVnocutII & 0 & \avVnocutIII & \avVnocutIV & \avVnocutV & \avVnocutVI & \avVnocutVII \tabularnewline
se\_new & control for AO narrator & \aoSenewAll & \aoSenewI & \aoSenewII & \aoSenewIII & \aoSenewIV & \aoSenewV & \aoSenewVI & \aoSenewVII & \aoSenewVIII \tabularnewline
se\_old & control for AO narrator & \aoSeoldAll & \aoSeoldI & \aoSeoldII & \aoSeoldIII & \aoSeoldIV & \aoSeoldV & \aoSeoldVI & \aoSeoldVII & \aoSeoldVIII \tabularnewline
fg\_av\_ger\_lr & XYZ & \avFgavgerlrAll & \avFgavgerlrI & \avFgavgerlrII & \avFgavgerlrIII & \avFgavgerlrIV & \avFgavgerlrV & \avFgavgerlrVI & \avFgavgerlrVII & \avFgavgerlrVIII \tabularnewline
fg\_av\_ger\_lrdiff & left-right volume difference & \avFgavgerlrdiffAll & \avFgavgerlrdiffI & \avFgavgerlrdiffII & \avFgavgerlrdiffIII & \avFgavgerlrdiffIV & \avFgavgerlrdiffV & \avFgavgerlrdiffVI & \avFgavgerlrdiffVII & \avFgavgerlrdiffVIII \tabularnewline
fg\_av\_ger\_ml & +++???+++ & \avFgavgermlAll & \avFgavgermlI & \avFgavgermlII & \avFgavgermlIII & \avFgavgermlIV & \avFgavgermlV & \avFgavgermlVI & \avFgavgermlVII & \avFgavgermlVIII \tabularnewline
fg\_av\_ger\_pd & +++???+++ & \avFgavgerpdAll & \avFgavgerpdI & \avFgavgerpdII & \avFgavgerpdIII & \avFgavgerpdIV & \avFgavgerpdV & \avFgavgerpdVI & \avFgavgerpdVII & \avFgavgerpdVIII \tabularnewline
fg\_av\_ger\_rms & root mean square (loudness) & \avFgavgerrmsAll & \avFgavgerrmsI & \avFgavgerrmsII & \avFgavgerrmsIII & \avFgavgerrmsIV & \avFgavgerrmsV & \avFgavgerrmsVI & \avFgavgerrmsVII & \avFgavgerrmsVIII \tabularnewline
fg\_av\_ger\_ud & +++???+++ & \avFgavgerudAll & \avFgavgerudI & \avFgavgerudII & \avFgavgerudIII & \avFgavgerudIV & \avFgavgerudV & \avFgavgerudVI & \avFgavgerudVII & \avFgavgerudVIII \tabularnewline
\end{tabular}
\end{table*}


% modeling of regressors from events
Events of both stimuli were convolved with a double gamma hemodynamic response
function (HRF).
% reference to figure ``correlation of regressors''
The Pearson correlation coefficients of the regressors of the AO and AV stimulus
and across the time course of all stimulus segments can be seen in Figure
\ref{fig:reg-corr}.
% temporal derivatives
Temporal derivatives were included in the design matrix to compensate for
regional differences between modeled and actual HRF \citep{friston1998event}.
% motion parameters
Six motion parameters were also used as additional nuisance regressors.
% high-pass filtering
Finally, designs were filtered with the same high-pass filter (cutoff period of
\unit[150]{s}) as the BOLD time series.

\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/regressor-corr} \caption{Pearson
        correlation coefficients of the regressors used in the analysis of the
        audio-description (blue; see Table \ref{tab:ao-events} for a
        description) and audio-visual movie (red; see Table
        \ref{tab:av-events}).
    \label{fig:reg-corr}
\end{figure*}

\paragraph{AO: COPEs}

\todo[inline]{Why done this way? Why is no. 1 the one? Event count,
se\_new and se\_old only vaguely describe the scene; narrator is often
lacking behind dialogue and change in soundscape}

\todo[inline]{contrasts are ordered by ``how well they make sense'' according to
theory, ``quality of auditory cue'', and amount of events/data. Hence, the were
assumed to result in more and more shitty results; at the moment all 8 contrasts
are used in the result (figure)}

% intro
For the AO stimulus, we created eight $t$-contrasts to isolate presumably
correlating with increased hemodynamic activity in the PPA by contrasting
different kind descriptive nouns. % furniture: blurred border between groom and
object Furniture was not included in one of the contrast because it can be
considered as an object or geometry-defining structure of a room.
% negative control contrasts
Five $t$-contrasts (see Table \ref{tab:ao-contrasts} we created for negative
control purposes to test if increased activation in the PPA was correlated with
moment of cuts and therefore correlated with a possible change in the
soundscape.


\begin{table*}[t]
    \caption{Contrasts of parameter estimates for the analysis of
        the audio-only stimulus (AO).
\texttt{all non-geo} = body, bodypart, fahead, object, sex\_f, sex\_m).
Contrast 1-8 aim to isolate the PPA.
Contrasts 9-13 serve as negative control at the moments of movie cuts
with possible changes of the audio tracks soundscape (see Table
\ref{tab:av-contrasts}).}

\label{tab:ao-contrasts}
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{nr.} &  \textbf{contrast} & \textbf{purpose} \\
\midrule
1 & geo, groom > all non-geo & PPA \tabularnewline
2 & geo, groom, se\_new > all non-geo & PPA \tabularnewline
3 & gro
om, se\_new, se\_old > all non-geo & PPA \tabularnewline
4 & geo > all non-geo & PPA \tabularnewline
5 & groom > all non-geo & PPA \tabularnewline
6 & se\_new > all non-geo & PPA \tabularnewline
7 & se\_new, se\_old > all non-geo & PPA \tabularnewline
8 & se\_new > se\_old & PPA \tabularnewline
9 & vse\_new > vpe\_old & control at moments of cuts \tabularnewline
10 & vse\_new, vpe\_new > vse\_old, vpe\_old & control at moments of cuts \tabularnewline
11 & vse\_new > vse\_old & control at moments of cuts \tabularnewline
12 & vse\_new > vse\_old, vpe\_old & control at moments of cuts \tabularnewline
13 & vse\_new, vpe\_new > vpe\_old & control at moments of cuts \tabularnewline
\end{tabular}
\end{table*}


\paragraph{AV: COPEs}

\todo[inline]{Why done this way? Why is no. 1 the one?}
% intro
For the AV stimulus, we created five $t$-contrasts presumably correlating with
increased hemodynamic activity in the PPA by contrasting different kind of cuts.
% no control for frames content
Notably, this was done without an annotation of the visual content depicted in
the frame after each cut and thus without statistically controlling confounding
variables.
% cinematographic rationale
% cut to new scenes: establishing shots
The cinematographic rationale was that movie tend to establish the setting
(\texttt{vse\_new}) and the spatial relationships within a setting
(\texttt{vpe\_new}) at the beginning of a movie.
% cut to recurrent scene
Later in the movie, the field sizes of movie shots tend to decrease when
switching back to already established settings (\texttt{vse\_old}).
% similarly for shots within a scene
Shots within a recurring settings tend to shift to depicting persons and objects
(\texttt{vpe\_old}) more relevant to the evolved narrative \citep{brown2012cinematography, mercado2011filmmakers}.
% neuroscientific rationale
The neuroscientific rationale was that simply that activity in the PPA is
greater when participants view novel versus repeated scenes or view-points.
\citep{epstein1999parahippocampal}.
% im Ergebnis
Hence, we created varying contrasts comparing a setting occurring for the first
time and cuts within a locale depicted for the first time to cuts into recurring
settings and cuts within a recurring locale.
% negative control
Five additional $t$-contrasts we created for negative control purposes (see
Table \ref{tab:av-contrasts}.

\begin{table*}[t]
    \caption{Contrasts of parameter estimates for the analysis of the audio-visual movie (AV).}
\label{tab:av-contrasts}
\footnotesize
\begin{tabular}{lll}
\toprule
\textbf{nr.} &  \textbf{contrast} & \textbf{purpose} \\
\midrule
1 & vse\_new > vpe\_old & PPA \tabularnewline
2 & vse\_new, vpe\_new > vse\_old, vpe\_old & PPA \tabularnewline
3 & vse\_new > vse\_old & PPA \tabularnewline
4 & vse\_new > vse\_old, vpe\_old & PPA \tabularnewline
5 & vse\_new, vpe\_new > vpe\_old & PPA \tabularnewline
6 & vno\_cut > vse\_new & negative control \tabularnewline
7 & vno\_cut > vse\_old & negative control \tabularnewline
8 & vno\_cut > vse\_new, vse\_old & negative control \tabularnewline
9 & vno\_cut > vpe\_new, vpe\_old & negative control \tabularnewline
10 & se\_new > se\_old & control for narrator \tabularnewline
\end{tabular}
\end{table*}

\paragraph{alignment etc.}
% single subjects (for Bland-Altman-Plots) 1st level
The GLM analysis that fitted each voxel's time course separately for each
subject was performed in functional space preserving the orientation of the EPI
images.
% two ways of co-registration
For the subsequent second level results, we co-registered the subjects in two
ways.
% subject template (for Bland-Altman-Plot of unthresholded maps)
% \href{"https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/sub-01/bold3Tp2/brain.nii.gz"}{example})
On the one hand, subject-specific BOLD T1* time series were anatomically aligned
via linear transformation to a subject-specific, study-specific T1* template.
% group template (for group and individual brain slices)
% https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms/blob/master/templates/grpbold3Tp2/brain.nii.gz"
On the other hand, time series were aligned via non-linear transformation to a
study-specific T2* group template.
% MNI152
This reference images was later co-registered to the MNI 152 template with an
affine transformation (12 degrees of freedom) to facilitate visualization and
identification of cortical structures,

% second level
% model
The second level analyses which averaged contrast estimates over the eight
stimulus segments were carried out using a fixed effects model by forcing the
random effects variance to zero in FLAME (FMRIB's Local Analysis of Mixed
Effects; \citep{beckmann2003general, woolrich2004multilevel}.
% thresholding (Z>2.3 in subject-space; Z>3.4 in group space)
(Gaussianised T/F) statistic images were thresholded using clusters determined
by Z>3.4 and a (corrected) cluster significance threshold of p=.05
\citep{worsley2001statistical}.

% third level
% model
The third level analysis which averaged contrast estimates over subjects was
carried out using FLAME stage 1 with automatic outlier detection
\citep{beckmann2003general, woolrich2004multilevel, woolrich2008robust}.
% thresholding
Here again, Z (Gaussianised T/F) statistic images were thresholded using
clusters determined by Z>3.4 and a (corrected) cluster significance threshold of
p=.05 \citep{worsley2001statistical}.
% brain region identification
Brain regions associated with observed clusters were determined with the Jülich
Histological Atlas \citep{eickhoff2005toolbox, eickhoff2007assignment} and the
Harvard-Oxford Cortical Atlas \citep{desikan2006automated} provided by FSL.
% PPA masks
PPA ROI masks for individual subjects and a PPA group mask of individual PPA
overlaps were created from data provided by \citep{sengupta2016extension}.


\section{Results}

% HOW TO WRITE METHODS SEXTION
% first paragraph (cause most readers don't read methods beforehand
% summarizes the overall approach to the problem outlined in the intro
% key innovative methods that were developed
% -> gist of the methods that were used

% subsequent paragraphs start with a sentence or two
% that set up the question that the paragraph answers
% e.g. ``To verify that there are no artifacts..''
% What is the test-retest reliability of our measure?''
% “We next tested whether Ca2+ flux through Ca2+ channels was involved.''
% Middle of the paragraph presents data and logic that pertain to the question,
% paragraph ends with a sentence that answers the question
% For example, it may conclude that none of potential artifacts were detected.
% Each paragraph convinces the reader of answer given in its last sentence.
% The result of each paragraph is a logical statement


\subsection{primary contrasts}
% AO
The primary $t$-contrast for the audio-description comparing geometry-related
nouns to non geometry-related nouns spoken by the audio-description's narrator
(\texttt{geo, groom > all non-geo} yielded six significant clusters (see Table
\ref{tab:res-ao-group1}, Figure \ref{fig:group-slices}).
% precuneus
Two homologous clusters in the ventral precuneus extending into the
retrosplenial cortex.
% PPA
Two homologous clusters in the lingual gyrus, occipital and temporal fusiform
gyrus, and posterior parahippocampal cortex \todo{the clusters are small but
that's what the Harvard-Oxford Atlas suggests, probably because at borders of
the areas}.
% LOC
And finally, two homologous clusters in the superior lateral occipital cortex.

% AV
The primary $t$-contrast for the movie comparing cuts to a new setting with cuts
to a familiar perspective (\texttt{vse\_new > vpe\_old}) yielded three
significant clusters (see Table \ref{tab:res-av-group1}, Figure
\ref{fig:group-slices}).
% LOC
Two clusters in the superior lateral occipital cortex (left bigger than right).
% PPA + others.
And one one cluster spanning across the midline and comprising intracalcarine
and cuneal cortex, lingual gyrus and retrosplenial cortex,  occipital fusiform
gyrus and temporal fusiform gyrus, and parahippocampal cortex bilaterally.


\begin{table*}[t]
    \caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected)
        of the primary $t$-contrast for the audio-description comparing
        geometry-related nouns to non geometry-related nouns spoken by the
        audio-description's narrator (\texttt{geo, groom > all non-geo}).
        Clusters sorted by voxel size.
    The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
    \label{tab:res-ao-group1}
\begin{tabular}{rrrrrrrrrp{3cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
188 & 5.96E-08 & 4.48 & -17.5 & -65.5 & 25.5 & -14.7 & -59.1 & 15.2 & l. precuneus \\
164 & 2.38E-07 & 4.47 & 17.5 & -58 & 23 & 15.6 & -55.6 & 16 & r. precuneus; \\
83 & 0.000128 & 4.48 & 27.5 & -43 & -17 & 27.2 & -41.1 & -14 & r. temporal (occipital) fusiform c.; posterior parahippocampal g. \\
73 & 0.000318 & 3.93 & -22.5 & -43 & -12 & -23.9 & -43.6 & -11.2 & l. lingual g.; temporal (occipital) fusiform g., posterior parahippocampal c. \\
63 & 0.000824 & 4.1 & 40 & -75.5 & 30.5 & 40.9 & -76.3 & 28.6 & r. superior lateral occipital c. \\
37 & 0.0129 & 4.24 & -37.5 & -78 & 33 & -38.4 & -79.5 & 28.9 & l. superior lateral occipital c. \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
    \caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected)
        of the primary $t$-contrast for the movie comparing cuts to a new
        setting with cuts to a familiar perspective (\texttt{vse\_new >
        vpe\_old}).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:res-av-group1}
\begin{tabular}{rrrrrrrrrp{3cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
3003 & 0 & 5.31 & 22.5 & -45.5 & -12 & 4.53 & -63.3 & -3.72 & r. lingual g.; r
cuneal c., intracalcarine c., bilaterally occipital fusiform g., temporal fusiform c., posterior parahippocampal c.  \\
154 & 6.56E-07 & 4.46 & -35 & -83 & 28 & -32.8 & -86.2 & 21.4 & l. superior lateral occipital c. \\
121 & 7.69E-06 & 4.65 & 25 & -80.5 & 25.5 & 23.7 & -83.8 & 25.4 & r. superior lateral occipital cortex \\
\bottomrule
\end{tabular}
\end{table*}


\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/group-slices}
    \caption{Mixed-effects group-level (N=14) GLM results. Significant clusters
        (Z>3.4, p<0.05 cluster-corrected) are overlaid on the MNI152 T1-weighted
        head template (grey).
        Light grey: The audio-description's field-of-view
        (cf. \citep{hanke2014audiomovie}).
        Blue: the primary $t$-contrast for the audio-description comparing
        geometry-related nouns to non geometry-related nouns spoken by the
        audio-description's narrator (\texttt{geo, groom > all non-geo}).
        Red: the primary $t$-contrast for the movie comparing cuts to a new
        setting with cuts to a familiar perspective (\texttt{vse\_new >
        vpe\_old)}.
        Black: outline of overlapping individual PPA ROIs.
        \citep{sengupta2016extension}.}
    \label{fig:group-slices}
\end{figure*}


\subsection{individual results}
See Figure \ref{fig:subs-thresh-ppa}; AO / AV to localizer ratios

\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/subs-thresh-ppa}
    \caption{Fixed-effects individual-level GLM results (Z>3.4, p<0.05
        cluster-corrected). The brains of the 14 subjects are aligned via
        non-linear transformation to a study-specific T2* group template that is
    co-registered to the MNI152 template with an affine transformation (12
degrees of freedom) to facilitate comparisons across subjects.
        Light grey: The audio-description's field-of-view
        (cf. \citep{hanke2014audiomovie}).
        Blue: the primary $t$-contrast for the audio-description comparing
        geometry-related nouns to non geometry-related nouns spoken by the
        audio-description's narrator (\texttt{geo, groom > all non-geo}).
        Red: the primary $t$-contrast for the movie comparing cuts to a new
        setting with cuts to a familiar perspective (\texttt{vse\_new >
        vpe\_old)}.
        Black: outline of subject-specific PPA ROI.}

\label{fig:subs-thresh-ppa}
\end{figure*}


\subsection{Bland-Altman-Plots}
See Figure \ref{fig:bland-altman}


\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/subjs_bland-altman.png}
    \caption{Bland-Altman-Plots for individual subjects}
    \label{fig:bland-altman}
\end{figure*}


\todo[inline]{I consider tables of significant clusters for every contrast as
overkill. One contrast, the ``primary contrast'', is appropriate.}

\subsection{stability contrasts}

% why we did different contrasts
We tested a variety of contrasts to ensure\dots ???

% results
The overlap of significant clusters of the 7 (well actually 8) contrast for the
audio-description, and 5 contrasts computed for the movie can be found in Figure
\ref{fig:stability-slices}.

% AO
\todo{alle noch einmal im einzelnen angucken; was ist mit COPE 8, der ist in der
Abb. nicht drin; zeigt (noch) einer keine sign. cluster?}
% PPA
Six of seven AO contrasts show significant bilateral clusters in anterior
regions of the group PPA overlap (which is temporal occipital fusiform gyrus
regarding to co-registered Harvard-Oxford Cortical Structural Atlas).
% Precuneus = retrosplenial cortex ?
Six of seven contrasts show significant clusters in the ventral precuneus /
retrosplenial cortex (RSC).

% AV
% PPA
All five AV contrasts yielded significant bilateral clusters in (temporal)
occipital, fusiform cortex showing congruency with the group overlap of
individual PPA ROIs.
% Lingual Gyrus
Homologous clusters of three contrasts extend further into lingual gyrus.
% RSC
Three contrasts show significant bilateral clusters in RSC.
% LOC
Four contrasts show significant bilateral clusters in lateral occipital cortex


\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/stability-slices}
    \caption{Stability of contrasts. Overlap of significant clusters (Z>3.4;
        p<.05, cluster corrected) of contrasts created to isolate the PPA
        overlaid on the MNI152 T1-weighted head template (grey).
        Light grey: The audio-description's field-of-view
        (cf. \citep{hanke2014audiomovie}).
        Blue: contrasts 1-7 for the audio-description (see Table \ref{tab:ao-contrasts}.
        Red: contrasts 1-5 for the audio-visual movie (see Table \ref{tab:av-contrasts}.
        Black: outline of overlapping individual PPA ROIs
        \citep{sengupta2016extension}.}
    \label{fig:stability-slices}
\end{figure*}


\section{Discussion}

% Discussion explains how results fill gap identified in intro
% provides caveats to the interpretation
% describes how paper advances the field by providing new opportunities.

% typically done by recapitulating the results, discussing limitations,
% and then revealing how the central contribution may catalyze future progress.

% first discussion paragraph is special in that it generally summarizes the
% important findings from the results section -> give gist of results

% following paragraphs in the discussion section starts by describing an area of weakness or strength of the paper.
% It then evaluates the strength or weakness by linking it to the
% relevant literature.

% Discussion paragraphs often conclude by describing a clever, informal way of perceiving the contribution or by discussing future directions that can extend the contribution.

% For example, the first paragraph may summarize the results, focusing on their meaning.

% The second through fourth paragraphs may deal with potential weaknesses and with how the literature alleviates concerns or how future experiments can deal with these weaknesses.

% The fifth paragraph may then culminate in a description of how the paper moves
% the field forward.

%Step by step, the reader thus learns to put the paper’s conclusions into the right context.

\todo[inline]{MIH: discussion should primarily be anchored on the points the
    introduction concludes with. From my POV there is no need (and likely no
    space) to discuss results by stimulus type. Issue like different impact of
    attentional focus etc can be brought up in the joint paragraph that
    discussed how such a factor could have differentially impacted the results
    and if that would matter.}

\paragraph{From Poster}: 1. PPA shows increased activation during non-visual,
complex auditory stimulation that provides incidental spatial information.

2. only minor temporal correlation of regressors of the audio drama \& movie that were not designed for experimental research.

3. but still, alternative contrasts show significant clusters in auditory
cortices.

4. inter-individual differences: for two subjects, an alternative audio-contrast
shows bilaterally significant clusters whereas the primary contrast shows no
significant clusters.

5. audio drama shows bilaterally increased activation in subject 4 in which the
visual localizer shows an unilateral cluster.

6. nevertheless, a  complex auditory stimulus might allow individual
localization of a functionally defined brain region.

7. results of other studies suggest7 the PPA does not exclusively process the
``spatial layout''" but ``contextual associations''.

8. an audio drama might be suitable as ecologically more valid ``localizer'' for
a variety of brain functions.


\subsection{AO Discussion}

Hanke et al. (2014): Compared to an audio-visual movie, this is a stimulus that
leaves a much larger margin for inter-individual differences in imagining
scenery, as well as actors' character and personality traits, while still
preserving the time-locked presentation of information to a listener. At the
same time, an auditory stimulus limits the effect of an attentional focus on the
selection of a subset of simultaneously occurring auditory events, in contrast
to the selection of different parts of the visual field.

aminoff2013: jedoch zu non-spatial stuff: Although the PHC is commonly thought
of as responding to visual stimuli, it has also been found to elicit activity
for auditory stimuli [25,26,30,31] and to respond to odor stimuli [32–34].

Unterschiedliche Qualität der Kontrast: Warum wurde Kontrast 1 als Kontrast 1
gewählt; se\_new se\_old sind recht vague in der Beschreibung der Szene
verglichen mit Bild; Anzahl der Events ergibt auch noch Sinn

setting\_new ist häufig vermischt mit anderer Kategorie "ein Arzt", "ein alter
Film", "auf einer Straße", "im Fernsehen", "die drei Jungs", "die zwei im Bett",
"Jenny als Teenager", "Forrest vor einem Buffet", "Kennedy", "Jenny kommt aus
dem Wohnwagen ihrer Großmutter" Soundscape macht es häufiger bereits vorher
klar, dass Settingwechsel ist bzw. nicht erst das annotiere Substantiv object
und (room)geo ist vermischt Retrospelinal Cortex ist in Probabilistic maps
krasser als PPA.

Mental Imagery Hypthesis (s. O'Craven \& Kanwisher, 2000); Lerner (2011), Honey
(2012), Silbert (2014) finden keine ISC in PPA? vgl. auch DataPaper bzgl.
Hörbuch??

Contrasts 6-8: words belonging to the categories \texttt{se\_new} and
\texttt{se\_old} were very vague in describing a scene compared to the richness
of a picture. Hence, we assumed smaller probability of successfully isolating
the PPA.


\subsection{AV discussion}
lore ipsum

\section*{General Discussion}

\subsection{Data-driven vs. Analysis-Driven}
"Abfall" einer Film-Messung

\subsection{naturalistic vs. dedicated} how do results (PPA-like response
localization) differ from a standard visual/block-design localizer for:

\subsection{anterior (AO) vs. posterior (AV) PPA}

\subsection{parietal and RSC activations} Story für parietale Aktivierung im
Film -> räuml. Orientierung / Aufmerksamkeit

\subsection{model-driven vs. data-driven analyses} Due to their continuity,
complexity, and length, they offer a vast amount of data on a variety of brain
functions like low-level perception, attention, comprehension, memory, emotions,
or social interactions. This suggests the data can later be reused for variety
of totally different, independent research questions, possibly on an individual
level.

\subsection{confounds in AO and AV}

\subsection{multi-dimensional localizer???}
feasibility of (multi-dimensional) localizer development with naturalistic
stimuli.

Natural stimuli like movies \citep{hasson2008neurocinematics,
sonkusare2019naturalistic} or narratives \citep{honey2012not,
lerner2011topographic, silbert2014coupled} offer an easy to implement,
(continuous,) complex, immersive, task-free paradigm that more closely resembles
our natural dynamic environment than traditional experimental designs.

why naturalistic stimuli are so cool: sonkusare2019, eickhoff2020, hamilton


\subsection*{Code availability}
% wo kommt dieser Abschnitt hin?
% how to write Code availability statement
% https://www.springernature.com/gp/authors/research-data-policy/data-availability-statements/12330880
\todo[inline]{provide supporting source code, and explaining how and where others may access all data underlying the analysis}
\emph{for all studies using custom code in the generation or processing of datasets,
a statement must be included here, indicating whether and how the code can be
accessed, including any restrictions to access; include information on the versions
of any software used, if relevant, and any specific variables or parameters used to
generate, test, or process the current dataset. }

\section*{Acknowledgements}
% Text acknowledging non-author contributors. Acknowledgements should be brief, and should not include thanks to anonymous referees and editors, or effusive comments. Grant or contribution numbers may be acknowledged.
% Author contributions Please describe briefly the contributions of each author to this work on a separate line.
\emph{COH did this and that.
MH did this and that.
We are grateful to \href{www.florianschurz.de}{Florian Schurz} who initiated doing the annotation of the descriptive nouns, and performed the preliminary annotation of nouns.}

\section*{Competing financial interests}
\emph{A competing financial interests statement is required for all accepted
papers published in \emph{Scientific Data}. If none exist simply write,
``The author(s) declare no competing financial interests''.}

\section*{Figures Legends}
\emph{Figure should be referred to using a consistent numbering scheme through
the entire Data Descriptor. For initial submissions, authors may choose
to supply this document as a single PDF with embedded figures, but
separate figure image files must be provided for revisions and accepted
manuscripts. In most cases, a Data Descriptor should not contain more
than three figures, but more may be allowed when needed. We discourage
the inclusion of figures in the Supplementary Information \textendash{}
all key figures should be included here in the main Figure section.}

\emph{Figure legends begin with a brief title sentence for the whole figure
and continue with a short description of what is shown in each panel,
as well as explaining any symbols used. Legend must total no more
than 350 words, and may contain literature references.}

\section*{Tables}

\emph{Tables supporting the Data Descriptor. These can provide summary information
(sample numbers, demographics, etc.), but they should generally not
be used to present primary data (i.e. measurements). Tables containing
primary data should be submitted to an appropriate data repository.}

\emph{Tables may be provided within the \LaTeX{} document or as separate
files (tab-delimited text or Excel files). Legends, where needed,
should be included here. Generally, a Data Descriptor should have
fewer than ten Tables, but more may be allowed when needed. Tables
may be of any size, but only Tables which fit onto a single printed
page will be included in the PDF version of the article (up to a maximum
of three).}

{\small\bibliographystyle{unsrtnat}
\bibliography{references}}

%\begin{thebibliography}{1}
%\expandafter\ifx\csname url\endcsname\relax
%  \def\url#1{\texttt{#1}}\fi
%\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
%\providecommand{\bibinfo}[2]{#2}
%\providecommand{\eprint}[2][]{\url{#2}}
%
%\bibitem{cite1}
%\bibinfo{author}{Califano, A.}, \bibinfo{author}{Butte, A.~J.},
%  \bibinfo{author}{Friend, S.}, \bibinfo{author}{Ideker, T.} \&
%  \bibinfo{author}{Schadt, E.}
%\newblock \bibinfo{title}{{Leveraging models of cell regulation and GWAS data
%  in integrative network-based association studies}}.
%\newblock \emph{\bibinfo{journal}{Nature Genetics}}
%  \textbf{\bibinfo{volume}{44}}, \bibinfo{pages}{841--847}
%  (\bibinfo{year}{2012}).
%
%\bibitem{cite2}
%\bibinfo{author}{Wang, R.} \emph{et~al.}
%\newblock \bibinfo{title}{{PRIDE Inspector: a tool to visualize and validate MS
%  proteomics data.}}
%\newblock \emph{\bibinfo{journal}{Nature Biotechnology}}
%  \textbf{\bibinfo{volume}{30}}, \bibinfo{pages}{135--137}
%  (\bibinfo{year}{2012}).
%\end{thebibliography}

\section*{Data Citations}

Bibliographic information for the data records described in the manuscript.

1. Lastname1, Initial1., Lastname2, Initial2., ...\& LastnameN, InitialN. \emph{Repository name} Dataset accession number or DOI (YYYY).

\end{document}
\documentclass[english]{article}
